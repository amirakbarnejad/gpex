{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "list_pathstoadd = [\"../../../\", \"../../../../PyDmed/\", \"../../../../TGP/\",\\\n",
    "                   \"../../../../uda_and_microscopyimaging_repo2/Src/BoVWPipeline/\"]\n",
    "for path in list_pathstoadd:\n",
    "    if(path not in sys.path):\n",
    "        sys.path.append(path)\n",
    "#import generalGPmodule\n",
    "import skimage\n",
    "from skimage import io\n",
    "import torchofgp.tgp as tgp\n",
    "import relatedwork\n",
    "from relatedwork.utils.generativemodels import ResidualEncoder\n",
    "import projutils\n",
    "from projutils.kernelmodules import Resnet18List,\\\n",
    "        Resnet50List, SqueezeNetList,\\\n",
    "        MultiResnet18ListAndOneLayer,\\\n",
    "        MultiResnet50ListAndOneLayer, TinyResNet18List,\\\n",
    "        Resnet18BackboneKernel, Resnet50BackboneKernel,\\\n",
    "        Resnet34BackboneKernel,\\\n",
    "        Resnet18BackboneKernelDivideAvgPool,\\\n",
    "        Resnet34BackboneKernelDivideAvgPool,\\\n",
    "        Resnet50BackboneKernelDivideAvgPool,\\\n",
    "        Resnet101BackboneKernelDivideAvgPool,\\\n",
    "        Resnet152BackboneKernelDivideAvgPool\n",
    "\n",
    "import akresnetforcifar\n",
    "from akresnetforcifar import *\n",
    "import loadcifar\n",
    "from loadcifar import *\n",
    "import relatedwork.utils.transforms\n",
    "tfm_denormalize = relatedwork.utils.transforms.ImgnetDenormalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings ====\n",
    "idx_trainingbatch = 1\n",
    "flag_enabledataaugmentation = True\n",
    "fname_gpmodel = \"TrainingHistory/Phase2/Nov17_Explainattention_onBeluga/\"+\\\n",
    "                \"output_explainattention_version4_afterepoch2.pt\"\n",
    "int_mode_modulekernel = 16\n",
    "flag_train_memefficient, memefficeint_heads_in_compgraph = False, None\n",
    "du_per_class = 20\n",
    "int_exposedclass = None\n",
    "idx_split = 0\n",
    "dim_wideoutput = 1024\n",
    "dim_before_wideoutput_attention = 200\n",
    "num_classes = 10\n",
    "batchsize = 10\n",
    "flag_efficient = True\n",
    "flag_detachcovpvn = True\n",
    "flag_controlvariate = True\n",
    "flag_setcovtoOne = False\n",
    "int_mode_controlvariate = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make datasets ====\n",
    "ds_train = Cifar10Dataset(\n",
    "    rootdir = \"./cifar-10-batches-py\",\n",
    "    fname_batchfile = \"data_batch_{}\".format(idx_trainingbatch),\n",
    "    str_trainoreval = \"train\",\n",
    "    flag_enabledataaugmentation = flag_enabledataaugmentation\n",
    ")\n",
    "ds_recurring = Cifar10Dataset(\n",
    "    rootdir = \"./cifar-10-batches-py\",\n",
    "    fname_batchfile = \"data_batch_{}\".format(idx_trainingbatch),\n",
    "    str_trainoreval = \"eval\"\n",
    ")\n",
    "ds_test = Cifar10Dataset(\n",
    "    rootdir = \"./cifar-10-batches-py\",\n",
    "    fname_batchfile = \"test_batch\",\n",
    "    str_trainoreval = \"eval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=batchsize,\n",
    "                      shuffle=True, num_workers=0)\n",
    "dl_recurring = DataLoader(ds_recurring, batch_size=batchsize,\n",
    "                          shuffle=True, num_workers=0)\n",
    "dl_test = DataLoader(ds_test, batch_size=batchsize,\n",
    "                     shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClampAndTanh(torch.nn.Module):\n",
    "    def __init_(self, minval=-1.0, maxval=1.0):\n",
    "        self.minval, self.maxval = minval, maxval\n",
    "        super(ClampAndTanh, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = torch.nn.functional.tanh(\n",
    "                        torch.clamp(\n",
    "                            x, -1.0, 1.0\n",
    "                        )\n",
    "                    )\n",
    "        return output\n",
    "    \n",
    "def initweights_to_zero(m):\n",
    "    if type(m) in {nn.Linear, nn.Conv2d, nn.Linear}:\n",
    "        torch.nn.init.zeros_(m.weight)\n",
    "        m.bias.data.fill_(np.random.randn()*0.1) #TODO:check\n",
    "    \n",
    "class ModuleF1(torch.nn.Module):\n",
    "    def __init__(self, module_caller):\n",
    "        super(ModuleF1, self).__init__()\n",
    "        #make internals ===\n",
    "        if(int_mode_modulekernel == 1):\n",
    "            #a list of resnet18s\n",
    "            self.module = TinyResNet18List(\n",
    "                scale_macrokernel = 1,\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 2):\n",
    "            #a list of resnet50s\n",
    "            self.module = Resnet50List(\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 3):\n",
    "            #a list of resnet18s eroded by 2\n",
    "            self.module = TinyResNet18List(\n",
    "                scale_macrokernel = 2,\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 4):\n",
    "            #a list of squeezenets\n",
    "            self.module = SqueezeNetList(\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 5):\n",
    "            #a list of resnet18s eroded by 4\n",
    "            self.module = TinyResNet18List(\n",
    "                scale_macrokernel = 4,\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 6):\n",
    "            #10 instances of MultiResnet18ListAndOneLayer ====\n",
    "            self.module = MultiResnet18ListAndOneLayer(\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class,\n",
    "                num_backbones = num_backbones\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 7):\n",
    "            self.module = MultiResnet50ListAndOneLayer(\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class,\n",
    "                num_backbones = num_backbones\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 9):\n",
    "            self.module = PrimaryNetwork(\n",
    "                    num_GPs=num_classes,\n",
    "                    du_per_class = du_per_class\n",
    "                )\n",
    "        elif(int_mode_modulekernel == 12):\n",
    "            self.module = Resnet18BackboneKernel(\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 13):\n",
    "            self.module = Resnet50BackboneKernel(\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 14):\n",
    "            self.module = Resnet34BackboneKernel(\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 15):\n",
    "            self.module = Resnet18BackboneKernelDivideAvgPool(\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 16):\n",
    "            self.module = Resnet50BackboneKernelDivideAvgPool(\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 17):\n",
    "            self.module = Resnet34BackboneKernelDivideAvgPool(\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 18):\n",
    "            self.module = Resnet101BackboneKernelDivideAvgPool(\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 19):\n",
    "            self.module = Resnet152BackboneKernelDivideAvgPool(\n",
    "                num_classes = num_classes,\n",
    "                du_per_class = du_per_class\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 21):\n",
    "            self.module = Resnet50List(\n",
    "                    num_classes = num_classes,\n",
    "                    du_per_class = du_per_class,\n",
    "                    scale_macrokernel = 2.0\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 22):\n",
    "            self.module = Resnet34List(\n",
    "                    num_classes = num_classes,\n",
    "                    du_per_class = du_per_class,\n",
    "                    scale_macrokernel = 2.0\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 23):\n",
    "            self.module = Resnet101List(\n",
    "                    num_classes = num_classes,\n",
    "                    du_per_class = du_per_class,\n",
    "                    scale_macrokernel = 2.0\n",
    "            )\n",
    "        elif(int_mode_modulekernel == 24):\n",
    "            self.module = Resnet152List(\n",
    "                    num_classes = num_classes,\n",
    "                    du_per_class = du_per_class,\n",
    "                    scale_macrokernel = 2.0\n",
    "            )\n",
    "        else:\n",
    "            print(\"Unknown mode_modulekernel {}.\".format(int_mode_modulekernel))\n",
    "        print(\"<><><><><><><><><> finisehd creating module_tail <><><><><><><><>.\")\n",
    "    \n",
    "    def set_rng_outputheads(self, rng_outputhead):\n",
    "        self.module.set_rng_outputheads(rng_outputhead)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        toret = self.module(x)\n",
    "        toret = toret.unsqueeze(-1).unsqueeze(-1)\n",
    "        return toret\n",
    "            \n",
    "class MainModule(nn.Module):\n",
    "    def __init__(self, num_classes, device, dl_recurring, dl_nonrecurring, dl_test, batchsize, dim_wideoutput):\n",
    "        '''\n",
    "        Inputs:\n",
    "            - size_input: size of the input, e.g., [32 x 2000 x 7 x 7].\n",
    "            - device: the device on which the GP fields are going to be created.\n",
    "            - num_outputheads: an integer, number of output heads.\n",
    "        '''\n",
    "        super(MainModule, self).__init__()\n",
    "        #grab args ===\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.dl_recurring = dl_recurring\n",
    "        self.dl_nonrecurring = dl_nonrecurring\n",
    "        self.dl_test = dl_test\n",
    "        self.batchsize = batchsize\n",
    "        #self.iter_dl_recurring = iter(self.dl_recurring)\n",
    "        #make internal module_tobecomeGP ===\n",
    "        self.module_classifier = akresnetforcifar.ResnetClassifierWithAttention(\n",
    "                num_classes = 10,\n",
    "                block_classifier = akresnetforcifar.BasicBlock,\n",
    "                num_blocks_classifier = [2, 2, 2, 2],\n",
    "                block_attention = akresnetforcifar.BasicBlock,\n",
    "                num_blocks_attention = [2, 2, 2, 2],\n",
    "                dim_before_wideoutput_attention = dim_before_wideoutput_attention,\n",
    "                dim_wideoutput_attention = dim_wideoutput\n",
    "        )\n",
    "        #make internals ===\n",
    "        self.dic_dlname_to_iter = {\n",
    "            \"dl_recurring\":iter(self.dl_recurring),\n",
    "            \"dl_nonrecurring\":iter(self.dl_nonrecurring),\n",
    "            \"dl_test\":iter(self.dl_test)\n",
    "        }\n",
    "        #make module f1 ===\n",
    "        self.module_f1 = ModuleF1(self) #nn.Sequential(\n",
    "        self._lastidx_recurring = []\n",
    "        #internal field to subsample when feeding minbatch ====\n",
    "        self.n_subsampleminibatch = None\n",
    "            \n",
    "    def forward(self, x, y, n):\n",
    "         return self.module_classifier(x, y, n)[0], y, n\n",
    "        \n",
    "    \n",
    "    def func_get_modulef1(self):\n",
    "        return self.module_f1\n",
    "    \n",
    "    def func_mainmodule_to_moduletobecomeGP(self, module_input):\n",
    "        return module_input.module_classifier.module_attention\n",
    "    \n",
    "    \n",
    "    def _func_feed_minibatch(self, dl_input, str_dlname, flag_addnoisetoX = False):\n",
    "        #print(\"reached here 1\")\n",
    "        if(False):#iter_dl is None):\n",
    "            pass #x, y, n = next(iter(dl_input))\n",
    "        else:\n",
    "            try:\n",
    "                x, y, n = next(self.dic_dlname_to_iter[str_dlname])\n",
    "            except (StopIteration):\n",
    "                self.dic_dlname_to_iter[str_dlname] = iter(dl_input)\n",
    "                x, y, n = next(self.dic_dlname_to_iter[str_dlname])\n",
    "                \n",
    "        \n",
    "        if(flag_addnoisetoX == True):\n",
    "            idx_permutex = np.random.permutation([u for u in range(list(x.size())[0])]).tolist()\n",
    "            idx_permutex = torch.LongTensor(idx_permutex)\n",
    "            x_perumted = x[idx_permutex, :, :, :]\n",
    "            \n",
    "            rand_w = torch.rand((list(x.size())[0])).float().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) #[Nx1x1x1]\n",
    "            rand_w = rand_w*(rng_randw[1]-rng_randw[0]) + rng_randw[0] #in [rng[0] , rng[1]]\n",
    "            rand_w = rand_w.detach()\n",
    "            #print(\"rand_w.shape = {}\".format(rand_w.shape))\n",
    "            x = rand_w*x + (1.0-rand_w)*x_perumted  #[N x 3 x 224 x 224]\n",
    "            x = x + 0.1*torch.randn_like(x).float()\n",
    "            \n",
    "        \n",
    "        if(self.n_subsampleminibatch is None):\n",
    "            pass\n",
    "        else:\n",
    "            x = x[0:self.n_subsampleminibatch, :,:,:]\n",
    "            y = y[0:self.n_subsampleminibatch]\n",
    "            n = n[0:self.n_subsampleminibatch]\n",
    "        \n",
    "        if(dl_input.dataset == ds_recurring):\n",
    "            self._lastidx_recurring = n\n",
    "            if(len(n) != list(x.size())[0]):\n",
    "                assert False\n",
    "        \n",
    "        output, _, _ = self.forward(x.to(self.device), y, n)\n",
    "        \n",
    "        #print(\"reached here 4\")\n",
    "        return output, y, n\n",
    "    \n",
    "    def func_feed_recurring_minibatch(self):\n",
    "        output, y, n = self._func_feed_minibatch(\n",
    "                            self.dl_recurring,\n",
    "                            str_dlname = \"dl_recurring\"\n",
    "                         )\n",
    "        return output, y, n\n",
    "    \n",
    "    def func_feed_noise_minibatch(self):\n",
    "        output, y, n = self._func_feed_minibatch(\n",
    "                        self.dl_nonrecurring,\n",
    "                        str_dlname = \"dl_nonrecurring\",\n",
    "                        flag_addnoisetoX=True\n",
    "                      )\n",
    "        return output, y, n\n",
    "    \n",
    "    def func_feed_nonrecurring_minibatch(self):\n",
    "        output, y, n = self._func_feed_minibatch(\n",
    "                                self.dl_nonrecurring,\n",
    "                                str_dlname = \"dl_nonrecurring\",\n",
    "                                flag_addnoisetoX=False\n",
    "                              )\n",
    "        return output, y, n\n",
    "    \n",
    "    def func_feed_test_minibatch(self):\n",
    "        output, y, n = self._func_feed_minibatch(\n",
    "                                    self.dl_test,\n",
    "                                    str_dlname = \"dl_test\"\n",
    "                                )\n",
    "        self._last_idx_test = n.tolist()\n",
    "        return output, y, n\n",
    "    \n",
    "    def func_get_indices_lastrecurringinstances(self):\n",
    "        return self._lastidx_recurring.cpu().numpy().tolist()\n",
    "    \n",
    "    def func_get_idxlastfed_testdl(self):\n",
    "        return self._last_idx_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MainModule(\n",
    "    dim_wideoutput=dim_wideoutput,\n",
    "    num_classes = num_classes,\n",
    "    device = device,\n",
    "    dl_recurring = dl_recurring,\n",
    "    dl_nonrecurring = dl_train,\n",
    "    dl_test = dl_test,\n",
    "    batchsize = batchsize\n",
    "  )\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpmodel = tgp.TGPModule(\n",
    "    module_rawmodule = model,\n",
    "    size_recurringdataset = len(ds_recurring),\n",
    "    device = device,\n",
    "    func_mainmodule_to_moduletobecomeGP = model.func_mainmodule_to_moduletobecomeGP, \n",
    "    func_feed_noise_minibatch = model.func_feed_noise_minibatch,\n",
    "    func_feed_recurring_minibatch = model.func_feed_recurring_minibatch,\n",
    "    func_feed_nonrecurring_minibatch = model.func_feed_nonrecurring_minibatch,\n",
    "    func_feed_test_minibatch = model.func_feed_test_minibatch,\n",
    "    func_get_indices_lastrecurringinstances = model.func_get_indices_lastrecurringinstances,\n",
    "    func_get_modulef1 = model.func_get_modulef1,\n",
    "    flag_efficient = flag_efficient,\n",
    "    flag_detachcovpvn = flag_detachcovpvn,\n",
    "    flag_controlvariate = flag_controlvariate,\n",
    "    flag_setcovtoOne = flag_setcovtoOne,\n",
    "    int_mode_controlvariate = int_mode_controlvariate,\n",
    "    flag_train_memefficient = flag_train_memefficient,\n",
    "    memefficeint_heads_in_compgraph = memefficeint_heads_in_compgraph\n",
    "  )\n",
    "#model.n_subsampleminibatch = 50\n",
    "gpmodel.sigma2_GP = 1.0 #TODO:check\n",
    "gpmodel.train()\n",
    "gpmodel.to(device)\n",
    "print(\"gpmodel was created on {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpmodel.load_state_dict(\n",
    "    torch.load(\n",
    "        fname_gpmodel\n",
    "     ),\n",
    "    strict = True\n",
    ")\n",
    "gpmodel.train()\n",
    "gpmodel.to(device)\n",
    "gpmodel.renew_precomputed_XTX()\n",
    "print(\"gpmodel was loaded from checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_kernel(model_input, ds_input, input_device, idx_begin, idx_end, tbegin=0.0, tend=1.0):\n",
    "    model_input.eval()\n",
    "    t_stepsize = 0.01\n",
    "    x_begin, _, _ = ds_input[idx_begin]\n",
    "    x_end, _, _ = ds_input[idx_end]\n",
    "    interval_toinspect = [\n",
    "        x_begin+((tend-tbegin)*t+tbegin)*(x_end-x_begin)\\\n",
    "        for t in np.arange(0.0, 1.0, t_stepsize)\n",
    "    ]\n",
    "    #compute the t's for x_begin and x_end ====\n",
    "    alpha, beta = tbegin, tend\n",
    "    t1 = (-alpha)/(beta-alpha)\n",
    "    t2 = (1.0-alpha)/(beta-alpha)\n",
    "    interval_t = [t for t in np.arange(0.0, 1.0, t_stepsize)]\n",
    "    idx_beforet1 = np.where(np.array(interval_t)>t1)[0][0]-1\n",
    "    idx_t1 = idx_beforet1 +\\\n",
    "            (interval_t[idx_beforet1+1]-t1)/(interval_t[1]-interval_t[0])\n",
    "    idx_beforet2 = np.where(np.array(interval_t)>t2)[0][0]-1\n",
    "    idx_t2 = idx_beforet2 +\\\n",
    "            (interval_t[idx_beforet2+1]-t2)/(interval_t[1]-interval_t[0])\n",
    "    \n",
    "    output_gp, output_nn = [], []\n",
    "    output_uncertainty = []\n",
    "    for idx_x, x in enumerate(interval_toinspect):\n",
    "        print(\" instance {} from {}\".format(idx_x, len(interval_toinspect)), end='\\r')\n",
    "        #feed to GP ======\n",
    "        output, uncertainty, output_similarities = \\\n",
    "                model_input.testingtime_forward(x.unsqueeze(0).to(input_device), 0, 0)\n",
    "        output = output[0]\n",
    "        output_gp.append(output.squeeze().detach().cpu().numpy())\n",
    "        output_uncertainty.append(uncertainty.flatten())\n",
    "        #feed to NN ===\n",
    "        netout, _, _ = model_input.module_rawmodule(x.unsqueeze(0).to(input_device), 0, 0)\n",
    "        output_nn.append(netout.squeeze().detach().cpu().numpy())\n",
    "    print(\"\\n\")\n",
    "    output_gp = np.array(output_gp)\n",
    "    output_nn = np.array(output_nn)\n",
    "    output_uncertainty = np.array(output_uncertainty)\n",
    "    model_input.train()\n",
    "    return output_gp, output_nn, output_uncertainty, idx_t1, idx_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpmodel.module_f1.module.set_rng_outputheads(rng_outputhead = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gpmodel.module_f1.module._rng_outputheads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gp, output_nn, output_uncertainty, t1, t2 = inspect_kernel(\n",
    "        model_input = gpmodel,\n",
    "        ds_input = ds_test,\n",
    "        input_device = device,\n",
    "        idx_begin = 10,\n",
    "        idx_end = 1100,\n",
    "        tbegin = -5.0,\n",
    "        tend = 5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_inspectkernel(output_gp, output_nn, output_uncertainty, t1, t2):\n",
    "    if(int_exposedclass is None):\n",
    "        num_classes = output_gp.shape[1]\n",
    "        for c in range(num_classes):\n",
    "            fig, ax1 = plt.subplots()\n",
    "            ln1 = ax1.plot(range(output_gp.shape[0]), output_gp[:,c], label=\"GP-mean\", c='r')\n",
    "            ln2 = ax1.plot(range(output_gp.shape[0]), output_nn[:,c], label=\"NN-mean\", c='b')\n",
    "\n",
    "            ax2 = ax1.twinx()\n",
    "            ln3 = ax2.plot(range(output_gp.shape[0]), 1.0/output_uncertainty[:,c],\\\n",
    "                           label=\"1.0/GP-uncertainty\", c='g')\n",
    "            ax2.axvline(x=t1, color='k', linestyle='--')\n",
    "            ax2.axvline(x=t2, color='k', linestyle='--')\n",
    "            lns = ln1+ln2+ln3\n",
    "            plt.legend(lns, [u.get_label() for u in lns], loc=0)\n",
    "            plt.title(\"output {}\".format(c))\n",
    "            plt.show()\n",
    "    else:\n",
    "        #the output index is not None ===\n",
    "        print(output_nn.shape)\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ln1 = ax1.plot(range(output_gp.shape[0]), output_gp, label=\"GP-mean\", c='r')\n",
    "        ln2 = ax1.plot(range(output_gp.shape[0]), output_nn, label=\"NN-mean\", c='b')\n",
    "        ax2 = ax1.twinx()\n",
    "        ln3 = ax2.plot(range(output_gp.shape[0]), 1.0/output_uncertainty,\\\n",
    "                       label=\"1.0/GP-uncertainty\", c='g')\n",
    "        ax2.axvline(x=t1, color='k', linestyle='--')\n",
    "        ax2.axvline(x=t2, color='k', linestyle='--')\n",
    "        lns = ln1+ln2+ln3\n",
    "        plt.legend(lns, [u.get_label() for u in lns], loc=0)\n",
    "        plt.title(\"output {}\".format(int_exposedclass))\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(range(output_gp.shape[0]), output_gp, label=\"GP-mean\", c='r')\n",
    "        plt.axvline(x=t1, color='k', linestyle='--')\n",
    "        plt.axvline(x=t2, color='k', linestyle='--')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_inspectkernel(output_gp, output_nn, output_uncertainty, t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_input, ds_input, input_device):\n",
    "    model_input.eval()\n",
    "    with torch.no_grad():\n",
    "        toret = []\n",
    "        list_gty = []\n",
    "        for n in range(len(ds_input)):\n",
    "            if(True):#try:\n",
    "                print(\" instance {} from {}\".format(n, len(ds_input)), end='\\r')\n",
    "                x, y, _ = ds_input[n]\n",
    "                output, _, _ = model_input.testingtime_forward(\n",
    "                                  x.unsqueeze(0).to(input_device), y, n\n",
    "                               )\n",
    "                #TODO:check output = output.clamp(min=-clampval_netout, max=clampval_netout)\n",
    "                toret.append(output[0].detach().cpu().numpy())\n",
    "                list_gty.append(y)\n",
    "            #except:\n",
    "            #    print(\"An exception occured for instnace {}\".format(n))\n",
    "        print(\"\\n\")\n",
    "        toret = np.array(toret)\n",
    "        toret = toret[:,0,:]\n",
    "    model_input.train()\n",
    "    return toret, list_gty\n",
    "\n",
    "def evaluate_g(model_input, ds_input, input_device):\n",
    "    model_input.eval()\n",
    "    with torch.no_grad():\n",
    "        toret = []\n",
    "        list_gty = []\n",
    "        for n in range(len(ds_input)):\n",
    "            print(\" instance {} from {}\".format(n, len(ds_input)), end='\\r')\n",
    "            x, y, n = ds_input[n]\n",
    "            output, _, _ = model_input(x.unsqueeze(0).to(input_device), y, n)\n",
    "            toret.append(output[0,:].detach().cpu().numpy())\n",
    "            list_gty.append(y)\n",
    "        print(\"\\n\")\n",
    "        toret = np.array(toret) #[N x 10]\n",
    "        #toret = toret[:,0,:]\n",
    "    model_input.train()\n",
    "    return toret, list_gty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate g(.) i.e. the bypass network ======\n",
    "#model.to(device)\n",
    "predy, list_gty = evaluate_g(gpmodel.module_rawmodule, ds_test, device)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np_confmatrix = confusion_matrix(np.argmax(predy,1), list_gty)\n",
    "print(np_confmatrix)\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"accuracy of g(.) = {}\".format(np.sum(np_confmatrix*np.eye(9))/np.sum(np_confmatrix) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the GP path ===\n",
    "gpmodel.renew_precomputed_XTX()\n",
    "predy, list_gty = evaluate_model(gpmodel, ds_test, device)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np_confmatrix = confusion_matrix(np.argmax(predy[:,:,0,0],1), list_gty)\n",
    "print(np_confmatrix)\n",
    "print(\"\\n\\n\\n\") \n",
    "print(\"accuracy = {}\".format(np.sum(np_confmatrix*np.eye(9))/np.sum(np_confmatrix) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check if g(.) and GP path match on test instances ====\n",
    "FLAG_RELU = False\n",
    "gpmodel.renew_precomputed_XTX()\n",
    "list_outputgpoutputg = gpmodel.checkequal_f1path_gpath_ontest(10)\n",
    "for n in range(len(list_outputgpoutputg)):\n",
    "    a = list_outputgpoutputg[n][0]\n",
    "    b = list_outputgpoutputg[n][1]\n",
    "    if(FLAG_RELU == True):\n",
    "        a = np_relu(a); b = np_relu(b)\n",
    "    \n",
    "    min_ab = min(np.min(a), np.min(b))\n",
    "    max_ab = max(np.max(a), np.max(b))\n",
    "    print(\"a-b in range [{} , {}]\".format(np.min(a-b), np.max(a-b)))\n",
    "    #print(np.round(a[0:5, 0:5], 2))\n",
    "    #print(np.round(b[0:5, 0:5], 2))\n",
    "    \n",
    "    #compute the class activations ====\n",
    "    gpmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            classactivation_a = gpmodel.module_rawmodule.linear(torch.tensor(a).float().to(device))\n",
    "            classactivation_b = gpmodel.module_rawmodule.linear(torch.tensor(b).float().to(device))\n",
    "            classactivation_a = classactivation_a.detach().cpu().numpy()\n",
    "            classactivation_b = classactivation_b.detach().cpu().numpy()\n",
    "            onehot_a = np.zeros((batchsize, 16))\n",
    "            onehot_a[list(range(batchsize)), np.argmax(classactivation_a, 1).tolist()] = 1\n",
    "            onehot_b = np.zeros((batchsize, 16))\n",
    "            onehot_b[list(range(batchsize)), np.argmax(classactivation_b, 1).tolist()] = 1\n",
    "\n",
    "            min_classactivations = min(np.min(classactivation_a), np.min(classactivation_b))\n",
    "            max_classactivations = max(np.max(classactivation_a), np.max(classactivation_b))\n",
    "        except:\n",
    "            onehot_a = np.zeros((batchsize, 16))\n",
    "            onehot_a[list(range(batchsize)), np.argmax(a, 1).tolist()] = 1\n",
    "            onehot_b = np.zeros((batchsize, 16))\n",
    "            onehot_b[list(range(batchsize)), np.argmax(b, 1).tolist()] = 1\n",
    "    gpmodel.train()\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(np.round(a, 2), vmin=min_ab, vmax=max_ab, aspect=\"auto\"); plt.colorbar()\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(np.round(b, 2), vmin=min_ab, vmax=max_ab, aspect=\"auto\"); plt.colorbar()\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(np.round(a-b, 2), cmap=\"seismic\", vmin=-np.max(np.abs(a-b)),\\\n",
    "               vmax=np.max(np.abs(a-b)), aspect=\"auto\"); plt.colorbar()\n",
    "    plt.colorbar()\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToPublish Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if g(.) and GP path match on test instances ====\n",
    "FLAG_RELU = False\n",
    "gpmodel.renew_precomputed_XTX()\n",
    "list_outputgpoutputg = gpmodel.checkequal_f1path_gpath_ontest(10)\n",
    "for n in range(len(list_outputgpoutputg)):\n",
    "    a = list_outputgpoutputg[n][0]\n",
    "    b = list_outputgpoutputg[n][1]\n",
    "    if(FLAG_RELU == True):\n",
    "        a = np_relu(a); b = np_relu(b)\n",
    "    \n",
    "    min_ab = min(np.min(a), np.min(b))\n",
    "    max_ab = max(np.max(a), np.max(b))\n",
    "    print(\"a-b in range [{} , {}]\".format(np.min(a-b), np.max(a-b)))\n",
    "    #print(np.round(a[0:5, 0:5], 2))\n",
    "    #print(np.round(b[0:5, 0:5], 2))\n",
    "    \n",
    "    #compute the class activations ====\n",
    "    gpmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            classactivation_a = gpmodel.module_rawmodule.linear(torch.tensor(a).float().to(device))\n",
    "            classactivation_b = gpmodel.module_rawmodule.linear(torch.tensor(b).float().to(device))\n",
    "            classactivation_a = classactivation_a.detach().cpu().numpy()\n",
    "            classactivation_b = classactivation_b.detach().cpu().numpy()\n",
    "            onehot_a = np.zeros((batchsize, 16))\n",
    "            onehot_a[list(range(batchsize)), np.argmax(classactivation_a, 1).tolist()] = 1\n",
    "            onehot_b = np.zeros((batchsize, 16))\n",
    "            onehot_b[list(range(batchsize)), np.argmax(classactivation_b, 1).tolist()] = 1\n",
    "\n",
    "            min_classactivations = min(np.min(classactivation_a), np.min(classactivation_b))\n",
    "            max_classactivations = max(np.max(classactivation_a), np.max(classactivation_b))\n",
    "        except:\n",
    "            onehot_a = np.zeros((batchsize, 16))\n",
    "            onehot_a[list(range(batchsize)), np.argmax(a, 1).tolist()] = 1\n",
    "            onehot_b = np.zeros((batchsize, 16))\n",
    "            onehot_b[list(range(batchsize)), np.argmax(b, 1).tolist()] = 1\n",
    "    gpmodel.train()\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(np.round(a, 2), vmin=min_ab, vmax=max_ab, aspect=\"auto\"); plt.colorbar()\n",
    "    plt.title(\"GPs output \\n (batchsize x D)\", font = 'Formata', fontsize = 22)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(np.round(b, 2), vmin=min_ab, vmax=max_ab, aspect=\"auto\"); plt.colorbar()\n",
    "    plt.title(\"ANN output \\n (batchsize x D)\", font = 'Formata', fontsize = 22)\n",
    "    plt.axis('off')\n",
    "    #plt.show()\n",
    "    plt.savefig(\n",
    "        \"InterpGP/ToPublish/Tables/Cifar10_Attention/{}.png\".format(time.time()),\n",
    "        dpi=100, bbox_inches='tight', pad_inches=0, Q=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute GP diff ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_idx_to_gpout, dict_idx_to_annout = gpmodel.check_GP_match_ANN_on_aDataloader(\n",
    "    func_feed_dlinstances = model.func_feed_test_minibatch,\n",
    "    func_get_lastidx_fedinstances = model.func_get_idxlastfed_testdl,\n",
    "    list_allidx = [n for n in range(len(ds_test))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the dicts to np_arrays ====\n",
    "list_gpout = []\n",
    "list_annout = []\n",
    "for n in range(len(ds_test)):\n",
    "    list_gpout.append(dict_idx_to_gpout[n])\n",
    "    list_annout.append(dict_idx_to_annout[n])\n",
    "np_gpout = np.array(list_gpout)\n",
    "np_annout = np.array(list_annout)\n",
    "print(np_gpout.shape)\n",
    "print(np_annout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import projutils.evaluation\n",
    "from projutils.evaluation import GPdiffANN_ontorchdl\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy import stats\n",
    "\n",
    "list_correlation = []\n",
    "for c in range(np_gpout.shape[1]):\n",
    "    list_correlation.append(\n",
    "        stats.pearsonr(np_gpout[:, c], np_annout[:, c])[0]\n",
    "    )\n",
    "print(\"list correl = {}\".format(list_correlation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depict the Correlation Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "np_gp = np_gpout #np_gpann[:,0:num_classes]\n",
    "np_ann = np_annout #np_gpann[:,num_classes::]\n",
    "np_gp_softmax = softmax(np_gp, 1)\n",
    "np_ann_softmax = softmax(np_ann, 1)\n",
    "\n",
    "list_disaggrement = (np.argmax(np_gp, 1) != np.argmax(np_ann, 1)).tolist()\n",
    "list_c = [[0,0,0,0.05] if(True) else [0,0,0,0.05] for u in list_disaggrement]\n",
    "count_plotted = 0\n",
    "for c in range(num_classes):\n",
    "    plt.ioff()\n",
    "    plt.figure()\n",
    "    plt.scatter(np_gp[:,c], np_ann[:,c], c=np.array(list_c), marker='o', facecolors='none')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.xlabel(\"GP output (head {})\".format(c+1), fontsize=22, font = 'Formata')\n",
    "    plt.ylabel(\"ANN output (head {})\".format(c+1), fontsize=22, font = 'Formata')\n",
    "    plt.savefig(\n",
    "        \"InterpGP/Correlation_Scatters/Attention/{}.png\".format(count_plotted),\n",
    "        dpi=100, bbox_inches='tight', pad_inches=0, Q=100\n",
    "    )\n",
    "    count_plotted += 1\n",
    "    plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_kernel(model_input, ds_input, input_device):\n",
    "    model_input.eval()\n",
    "    toret = []\n",
    "    list_gty = []\n",
    "    list_uncertainty = []\n",
    "    list_similarities = []\n",
    "    list_x, list_y = [], []\n",
    "    list_output_g = []\n",
    "    for n in range(len(ds_input)):\n",
    "        print(\" instance {} from {}\".format(n, len(ds_input)), end='\\r')\n",
    "        x, y, _ = ds_input[n]\n",
    "        output, uncertainty, output_similarities = \\\n",
    "                model_input.testingtime_forward(x.unsqueeze(0).to(input_device), y, n)\n",
    "        #print(\"output_similaritites.shape = {}\".format(output_similarities.shape))\n",
    "        output = output[0]\n",
    "        toret.append(output.detach().cpu().numpy())\n",
    "        list_gty.append(y)\n",
    "        list_uncertainty.append(uncertainty)\n",
    "        list_similarities.append(output_similarities.detach().cpu().numpy())\n",
    "        list_x.append(x); list_y.append(y)\n",
    "        #feed the model to g(.) ====\n",
    "        output_g, _, _ = \\\n",
    "                model_input.module_rawmodule(x.unsqueeze(0).to(input_device), y, n)\n",
    "        list_output_g.append(output_g.detach().cpu().numpy().flatten().tolist())\n",
    "\n",
    "    print(\"\\n\")\n",
    "    toret = np.array(toret)\n",
    "    toret = toret[:,0,:]\n",
    "    output_g = np.array(list_output_g)\n",
    "    print(output_g.shape)\n",
    "    model_input.train()\n",
    "    return toret, list_gty, list_uncertainty, list_similarities, list_x, list_y, output_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_retval_inspectmodel = [inspect_kernel(gpmodel, ds_test, device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "m = 10\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "for n in range(len(ds_test)):\n",
    "    print(\"n = {} ==========================\".format(n))\n",
    "#     fname_n, _ = ds_test._ntoimage(n)\n",
    "#     fname_n = os.path.relpath(fname_n, ds_rootdir)\n",
    "#     if(fname_n not in ds_split[\"fname_hoskys\"]):\n",
    "#         continue\n",
    "    #fields common between all models\n",
    "    _, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "    x, y = list_x[n], list_y[n]\n",
    "    \n",
    "    plt.figure(figsize=((m+2)*10, 1*10))\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    count_subplot = 1\n",
    "    str_subfolder = \"\"\n",
    "    idx_model = 0 #for idx_model in range(len(list_retval_inspectmodel)):\n",
    "    list_predy, list_gty, list_uncertainty, list_similarities, _, _, output_g = \\\n",
    "                        list_retval_inspectmodel[idx_model]\n",
    "    np_argmax_list_predyn = np.argmax(output_g[n])\n",
    "    if(np_argmax_list_predyn == list_gty[n]):\n",
    "        str_subfolder = str_subfolder + \"True\"\n",
    "    else:\n",
    "        str_subfolder = str_subfolder + \"False\"\n",
    "    kn = list_similarities[n][np_argmax_list_predyn, :].flatten()\n",
    "    idx_similars = np.argsort(-kn).tolist()[0:m]\n",
    "    plt.ioff()\n",
    "    plt.subplot(1, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "    plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "    plt.axis('off')\n",
    "    print(\"===== showed.\")\n",
    "    plt.title(\"gt-label = {}\\n predicted = {}\\n instance {}\".format(\n",
    "                            ds_recurring.label_names[y],\n",
    "                            ds_recurring.label_names[np_argmax_list_predyn],\n",
    "                            n\n",
    "                ), fontsize=100\n",
    "             )\n",
    "\n",
    "    list_relevantinstances =[]\n",
    "    for count_similars in range(len(idx_similars)):\n",
    "        plt.subplot(1, 1*(m+2), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(\n",
    "          tfm_denormalize(\n",
    "                  ds_recurring[idx_similars[count_similars]][0]\n",
    "              ).cpu().numpy().transpose(1,2,0),\n",
    "        )\n",
    "        plt.axis('off')\n",
    "        plt.title(\"instnace {}\".format(idx_similars[count_similars]), fontsize=100)\n",
    "        list_relevantinstances.append(idx_similars[count_similars])\n",
    "            \n",
    "        \n",
    "                \n",
    "        \n",
    "    if(True):#os.path.isfile(\"InterpGP/{}/{}.png\".format(str_subfolder, n)) == False):\n",
    "        plt.savefig(\n",
    "                \"InterpGP/{}/{}.jpg\".format(str_subfolder, n),\n",
    "                dpi=50, bbox_inches='tight', pad_inches=0, Q=80\n",
    "            )\n",
    "        print(\"================= saved.\")\n",
    "    plt.close()\n",
    "    #assert False\n",
    "#enable_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain Similarities CAM-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchofgp\n",
    "import torchofgp.kernel_explainers\n",
    "from torchofgp.kernel_explainers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_retval_inspectmodel = [inspect_kernel(gpmodel, ds_test, device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#explain the similarity itself =====\n",
    "m = 10\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "for n in range(len(ds_test)):\n",
    "    \n",
    "    #fields common between all models\n",
    "    _, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "    x, y = list_x[n], list_y[n]\n",
    "    \n",
    "    plt.figure(figsize=((m+1)*10, 3*10))\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    count_subplot = 1\n",
    "    str_subfolder = \"\"\n",
    "    for idx_model in range(len(list_retval_inspectmodel)):\n",
    "        list_predy, list_gty, list_uncertainty, list_similarities, _, _, output_g = \\\n",
    "                            list_retval_inspectmodel[idx_model]\n",
    "        np_argmax_list_predyn = np.argmax(output_g[n])\n",
    "        if(np_argmax_list_predyn == list_gty[n]):\n",
    "            str_subfolder = str_subfolder + \"True\"\n",
    "        else:\n",
    "            str_subfolder = str_subfolder + \"False\"\n",
    "        kn = list_similarities[n][np_argmax_list_predyn, :].flatten()\n",
    "        idx_similars = np.argsort(-kn).tolist()[0:m]\n",
    "        plt.ioff()\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        plt.title(\"gt-label = {}\\n predicted = {}\\n instance {}\".format(\n",
    "                                ds_recurring.label_names[y],\n",
    "                                ds_recurring.label_names[np_argmax_list_predyn],\n",
    "                                n\n",
    "                    ), fontsize=100\n",
    "                 )\n",
    "        plt.axis('off')\n",
    "        \n",
    "        list_relevantinstances =[]\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(\n",
    "              tfm_denormalize(ds_recurring[idx_similars[count_similars]][0]).cpu().numpy().transpose(1,2,0),\n",
    "            )\n",
    "            plt.axis('off')\n",
    "            plt.title(\"instnace {}\".format(idx_similars[count_similars]), fontsize=100)\n",
    "            list_relevantinstances.append(idx_similars[count_similars])\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        #plot rows 2 (explanations for x2)\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            x2 = ds_recurring[idx_similars[count_similars]][0].unsqueeze(0).to(device)\n",
    "            x1 = x.unsqueeze(0).to(device)\n",
    "            explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "              gpmodel = gpmodel,\n",
    "              func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "              x1 = x.to(device),\n",
    "              x2 = ds_recurring[idx_similars[count_similars]][0].to(device),\n",
    "              idx_ann_outputhead = y,\n",
    "              du_per_gp = du_per_class,\n",
    "              scale_resizemaps = 1.0\n",
    "            )\n",
    "            \n",
    "            toret1 = np.sum(np.sum(explanationx1x2, axis=-1), axis=-1)\n",
    "            toret2 = np.sum(np.sum(explanationx1x2, axis=0), axis=0)\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(toret2, cmap=\"seismic\", vmin=np.min(toret2), vmax=np.max(toret2))\n",
    "            plt.axis('off')\n",
    "        #plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        #plt.hist(kn.flatten(), bins=200)\n",
    "        \n",
    "        #plot rows 3 (explanations for the instance itself)\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            x2 = ds_recurring[idx_similars[count_similars]][0].unsqueeze(0).to(device)\n",
    "            x1 = x.unsqueeze(0).to(device)\n",
    "            explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "              gpmodel = gpmodel,\n",
    "              func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "              x1 = x.to(device),\n",
    "              x2 = ds_recurring[idx_similars[count_similars]][0].to(device),\n",
    "              idx_ann_outputhead = y,\n",
    "              du_per_gp = du_per_class,\n",
    "              scale_resizemaps = 1.0\n",
    "            )\n",
    "            \n",
    "            toret1 = np.sum(np.sum(explanationx1x2, axis=-1), axis=-1)\n",
    "            toret2 = np.sum(np.sum(explanationx1x2, axis=0), axis=0)\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(toret1, cmap=\"seismic\", vmin=np.min(toret1), vmax=np.max(toret1))\n",
    "                \n",
    "        \n",
    "    if(os.path.isfile(\"InterpGP/Kernel/{}/{}.jpg\".format(str_subfolder, n)) == False):\n",
    "        plt.savefig(\n",
    "                \"InterpGP/Kernel/{}/{}.jpg\".format(str_subfolder, n),\n",
    "                dpi=20, bbox_inches='tight', pad_inches=0, Q=80\n",
    "            )\n",
    "    plt.close()\n",
    "    #assert False\n",
    "#enable_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain by Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchofgp\n",
    "import torchofgp.kernel_explainers\n",
    "from torchofgp.kernel_explainers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_retval_inspectmodel = [inspect_kernel(gpmodel, ds_test, device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_retvals = open('InterpGP/retvals.pkl', 'wb')\n",
    "# pickle.dump(list_retval_inspectmodelval_inspectmodel, file_retvals, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_retvals = open('InterpGP/retvals.pkl', 'rb')\n",
    "list_retval_inspectmodel = pickle.load(file_retvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings ===\n",
    "n1 = 21\n",
    "n2 = 9064\n",
    "rgbsigma = 0.0\n",
    "#explain the similarity ===\n",
    "_, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "x1, y1 = list_x[n1], list_y[n1]\n",
    "x2 = ds_recurring[n2][0]\n",
    "explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "      gpmodel = gpmodel,\n",
    "      func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "      x1 = x1.to(device),\n",
    "      x2 = x2.to(device),\n",
    "      idx_ann_outputhead = y1,\n",
    "      du_per_gp = du_per_class,\n",
    "      scale_resizemaps = 1.0,\n",
    "      mode_upsample='nearest',\n",
    "      flag_aligcorners = None\n",
    ")\n",
    "explanationx1x2 = explanationx1x2 * ((explanationx1x2>0.0) + 0.0)\n",
    "print(\"shape of CAM-like output = {}\".format(explanationx1x2.shape))\n",
    "\n",
    "#explanationx1x2 = explanationx1x2/np.max(explanationx1x2) #TODO:check\n",
    "#explanationx1x2[explanationx1x2 < 0.1] = 0.0 #TODO:check\n",
    "\n",
    "#compute the heatmaps for img1 and img2 ===\n",
    "heatmap_1 = np.sum(np.sum(explanationx1x2, 3), 2)\n",
    "heatmap_2 = np.sum(np.sum(explanationx1x2, 0), 0)\n",
    "normalized_heatmap_1 = heatmap_1/np.max(heatmap_1)\n",
    "normalized_heatmap_2 = heatmap_2/np.max(heatmap_2)\n",
    "\n",
    "#assert False\n",
    "#make the pixel similarity matrix ====\n",
    "num_pix1 = explanationx1x2.shape[0] * explanationx1x2.shape[1]\n",
    "num_pix2 = explanationx1x2.shape[2] * explanationx1x2.shape[3]\n",
    "np_pixelmatrix = np.zeros((num_pix1+num_pix2 , num_pix1+num_pix2))\n",
    "\n",
    "#for pixels of image 1\n",
    "for idx_ij in range(num_pix1):\n",
    "    i, j = np.unravel_index(idx_ij, [explanationx1x2.shape[0], explanationx1x2.shape[1]])\n",
    "    #fill the similarities between i,j and image 2\n",
    "    for idx_kl in range(num_pix2):\n",
    "        k, l = np.unravel_index(idx_kl, [explanationx1x2.shape[2], explanationx1x2.shape[3]])\n",
    "        np_pixelmatrix[idx_ij, idx_kl] = explanationx1x2[i,j,k,l]\n",
    "    #fill the similarities between i,j and image 1 \n",
    "    for idx_i2j2 in range(num_pix1):\n",
    "        if(idx_i2j2 != idx_ij):\n",
    "            i2, j2 = np.unravel_index(\n",
    "                    idx_i2j2, [explanationx1x2.shape[0], explanationx1x2.shape[1]]\n",
    "                )\n",
    "            dist = np.array([i,j]) - np.array([i2,j2])\n",
    "            if(rgbsigma != 0.0):\n",
    "                rbf = np.exp(np.sum(-dist*dist) / (2.0*rgbsigma * rgbsigma))\n",
    "            else:\n",
    "                rbf = 0.0\n",
    "            np_pixelmatrix[idx_ij, idx_i2j2] = rbf\n",
    "            \n",
    "#for pixels of image 2\n",
    "for idx_kl in range(num_pix2):\n",
    "    k, l = np.unravel_index(idx_kl, [explanationx1x2.shape[2], explanationx1x2.shape[3]])\n",
    "    #fill the similarities between k,l and image 1\n",
    "    for idx_ij in range(num_pix1):\n",
    "        i, j = np.unravel_index(idx_ij, [explanationx1x2.shape[0], explanationx1x2.shape[1]])\n",
    "        np_pixelmatrix[idx_kl, idx_ij] = explanationx1x2[i,j,k,l]\n",
    "    #fill the similarities between k,l and image 2 \n",
    "    for idx_k2l2 in range(num_pix2):\n",
    "        if(idx_k2l2 != idx_kl):\n",
    "            k2, l2 = np.unravel_index(\n",
    "                    idx_k2l2, [explanationx1x2.shape[2], explanationx1x2.shape[3]]\n",
    "                )\n",
    "            dist = np.array([k,l]) - np.array([k2,l2])\n",
    "            if(rbf != 0.0):\n",
    "                rbf = np.exp(np.sum(-dist*dist) / (2.0*rgbsigma * rgbsigma))\n",
    "            else:\n",
    "                rbf = 0.0\n",
    "            np_pixelmatrix[idx_kl, idx_k2l2] = rbf\n",
    "            \n",
    "print(\"Computed the similarity matrix of shape {}\".format(np_pixelmatrix.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the histogram of two heatmaps ===\n",
    "plt.figure()\n",
    "plt.hist(heatmap_1.flatten(), bins=50)\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.hist(heatmap_2.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#run spectral clustering on pixel similarity matrix ====\n",
    "#settings ===\n",
    "n_clusters = 2\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "clustering = SpectralClustering(\n",
    "         affinity = \"precomputed\",\n",
    "         n_clusters= n_clusters,\n",
    "         random_state=1\n",
    ").fit(np_pixelmatrix+0.0)\n",
    "\n",
    "#show the clustering result\n",
    "plt.figure(figsize=((clustering.n_clusters+2)*10, 2*10))\n",
    "count_subplot = 1\n",
    "plt.subplot(2, clustering.n_clusters+2, count_subplot); count_subplot +=clustering.n_clusters+2;\n",
    "plt.imshow(tfm_denormalize(x1).cpu().numpy().transpose(1,2,0))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, clustering.n_clusters+2, count_subplot); count_subplot -= (clustering.n_clusters+1);\n",
    "plt.imshow(tfm_denormalize(x2).cpu().numpy().transpose(1,2,0))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.subplot(2, clustering.n_clusters+2, count_subplot); count_subplot +=clustering.n_clusters+2;\n",
    "plt.imshow(heatmap_1, cmap='seismic')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, clustering.n_clusters+2, count_subplot); count_subplot -= (clustering.n_clusters+1);\n",
    "plt.imshow(heatmap_2, cmap='seismic')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "for c in range(clustering.n_clusters):\n",
    "    idx_pixelsinc = np.where(np.array(clustering.labels_) == c)[0].tolist() #in rng pix1+pix2\n",
    "    \n",
    "    \n",
    "    #plot the cluster c for image 1\n",
    "    img1_inc = np.zeros((explanationx1x2.shape[0], explanationx1x2.shape[1]))\n",
    "    plt.subplot(\n",
    "        2, clustering.n_clusters+2,\n",
    "        count_subplot); count_subplot +=clustering.n_clusters+2;\n",
    "    idx_img1_inc = [idx for idx in idx_pixelsinc if(idx<num_pix1)]\n",
    "    if(idx_img1_inc != []):\n",
    "        np_img1_ij = np.array(np.unravel_index(idx_img1_inc, img1_inc.shape))\n",
    "        img1_inc[np_img1_ij[0,:] , np_img1_ij[1,:]] = 1\n",
    "    plt.imshow(img1_inc, vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    #plot the cluster c for image 2\n",
    "    img2_inc = np.zeros((explanationx1x2.shape[2], explanationx1x2.shape[3]))\n",
    "    plt.subplot(\n",
    "        2, clustering.n_clusters+2,\n",
    "        count_subplot); count_subplot -= (clustering.n_clusters+1);\n",
    "    idx_img2_inc = [idx-num_pix1 for idx in idx_pixelsinc if(idx>=num_pix1)]\n",
    "    if(idx_img2_inc != []):\n",
    "        np_img2_ij = np.array(np.unravel_index(idx_img2_inc, img2_inc.shape))\n",
    "        img2_inc[np_img2_ij[0,:] , np_img2_ij[1,:]] = 1\n",
    "    plt.imshow(img2_inc, vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explain the similarity itself =====\n",
    "m = 5\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "for n in range(len(ds_test)):\n",
    "    fname_n, _ = ds_test._ntoimage(n)\n",
    "    fname_n = os.path.relpath(fname_n, ds_rootdir)\n",
    "#     if(fname_n not in ds_split[\"fname_hoskys\"]):\n",
    "#         continue\n",
    "    #fields common between all models\n",
    "    _, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "    x, y = list_x[n], list_y[n]\n",
    "    \n",
    "    plt.figure(figsize=((m+1)*10, 3*10))\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    count_subplot = 1\n",
    "    str_subfolder = \"\"\n",
    "    for idx_model in range(len(list_retval_inspectmodel)):\n",
    "        list_predy, list_gty, list_uncertainty, list_similarities, _, _, output_g = \\\n",
    "                            list_retval_inspectmodel[idx_model]\n",
    "        np_argmax_list_predyn = np.argmax(output_g[n])\n",
    "        if(np_argmax_list_predyn == list_gty[n]):\n",
    "            str_subfolder = str_subfolder + \"True\"\n",
    "        else:\n",
    "            str_subfolder = str_subfolder + \"False\"\n",
    "        kn = list_similarities[n][np_argmax_list_predyn, :].flatten()\n",
    "        idx_similars = np.argsort(-kn).tolist()[0:m]\n",
    "        plt.ioff()\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        plt.title(\"gt-label = {}\\n predicted = {}\\n instance {}\".format(\n",
    "                                [\"dog\", \"wolf\"][y],\n",
    "                                [\"dog\", \"wolf\"][np_argmax_list_predyn],\n",
    "                                n\n",
    "                    ), fontsize=100\n",
    "                 )\n",
    "        \n",
    "        list_relevantinstances =[]\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(\n",
    "              tfm_denormalize(ds_recurring[idx_similars[count_similars]][0]).cpu().numpy().transpose(1,2,0),\n",
    "            )\n",
    "            plt.title(\"instnace {}\".format(idx_similars[count_similars]), fontsize=100)\n",
    "            list_relevantinstances.append(idx_similars[count_similars])\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        #plot rows 2 (explanations for x2)\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            x2 = ds_recurring[idx_similars[count_similars]][0].unsqueeze(0).to(device)\n",
    "            x1 = x.unsqueeze(0).to(device)\n",
    "            explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "              gpmodel = gpmodel,\n",
    "              func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "              x1 = x.to(device),\n",
    "              x2 = ds_recurring[idx_similars[count_similars]][0].to(device),\n",
    "              idx_ann_outputhead = y,\n",
    "              du_per_gp = du_per_class,\n",
    "              scale_resizemaps = 2.0\n",
    "            )\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            toret1 = np.sum(np.sum(explanationx1x2, axis=-1), axis=-1)\n",
    "            toret2 = np.sum(np.sum(explanationx1x2, axis=0), axis=0)\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(toret2, cmap=\"seismic\", vmin=np.min(toret2), vmax=np.max(toret2))\n",
    "        #plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        #plt.hist(kn.flatten(), bins=200)\n",
    "        \n",
    "        #plot rows 3 (explanations for the instance itself)\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            x2 = ds_recurring[idx_similars[count_similars]][0].unsqueeze(0).to(device)\n",
    "            x1 = x.unsqueeze(0).to(device)\n",
    "            explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "              gpmodel = gpmodel,\n",
    "              func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "              x1 = x.to(device),\n",
    "              x2 = ds_recurring[idx_similars[count_similars]][0].to(device),\n",
    "              idx_ann_outputhead = y,\n",
    "              du_per_gp = du_per_class,\n",
    "              scale_resizemaps = 2.0\n",
    "            )\n",
    "            \n",
    "            toret1 = np.sum(np.sum(explanationx1x2, axis=-1), axis=-1)\n",
    "            toret2 = np.sum(np.sum(explanationx1x2, axis=0), axis=0)\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(toret1, cmap=\"seismic\", vmin=np.min(toret1), vmax=np.max(toret1))\n",
    "                \n",
    "        \n",
    "    if(os.path.isfile(\"InterpGP/RegionBased/{}/{}.png\".format(str_subfolder, n)) == False):\n",
    "        plt.savefig(\n",
    "                \"InterpGP/RegionBased/{}/{}.png\".format(str_subfolder, n),\n",
    "                dpi=80, bbox_inches='tight', pad_inches=0, Q=100\n",
    "            )\n",
    "    plt.close()\n",
    "    assert False\n",
    "#enable_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depict the Prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_label_recurring = [ds_recurring.list_labelnames.index(ds_recurring._ntoimage(n)[1]+\"/\") for n in range(len(ds_recurring))]\n",
    "def inspect_kernel(module_input, dl_input, list_label_recurring):\n",
    "    np_lable_recurring = np.array(list_label_recurring)\n",
    "    dict_class_to_sumranks = {u:0.0 for u in range(module_input.Dv)}\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dl_input):\n",
    "            if((idx%10) == 0):\n",
    "                print(\"Finished {} out of {}\".format(\n",
    "                    idx, len(dl_input.dataset)/dl_input.batch_size\n",
    "                ), end='\\r')\n",
    "            x, y, n = data\n",
    "            output, uncertainty, output_similarities = \\\n",
    "                module_input.testingtime_forward(x.to(device), y, n)\n",
    "            output_similarities = output_similarities.detach().cpu().numpy() #[9xbatchsizex M]\n",
    "            for idx_inminibatch, idx_inds in enumerate(n):\n",
    "                xn_similarities = output_similarities[int(y[idx_inminibatch]),idx_inminibatch, :] #[M]\n",
    "                xn_similarities = xn_similarities.flatten()\n",
    "                xn_similarities = xn_similarities[np_lable_recurring == int(y[idx_inminibatch])]\n",
    "                dict_class_to_sumranks[int(y[idx_inminibatch])] += np.argsort(-xn_similarities)\n",
    "                \n",
    "    \n",
    "    return dict_class_to_sumranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_class_to_sumranks = inspect_kernel(gpmodel, dl_test, list_label_recurring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump( dict_class_to_sumranks, open(\"TrainingHistory/dict_class_to_sumranks.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_class_to_sumranks = pickle.load( open( \"TrainingHistory/dict_class_to_sumranks.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dict_class_to_sumranks.keys():\n",
    "    plt.figure()\n",
    "    plt.hist(dict_class_to_sumranks[k]/len(dict_class_to_sumranks[k].tolist()), bins=200)\n",
    "    plt.title(\"histogram of avg. ranks among the training instances \\n for class {}.\".format(k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import relatedwork\n",
    "import relatedwork.utils.transforms\n",
    "tfm_denormalize = relatedwork.utils.transforms.ImgnetDenormalize()\n",
    "#open the app to check whether the sample-compression scheme matches human's ====\n",
    "num_tocompare = 10\n",
    "input_class = int(input(\"Please input the class (between 0 and 9).\"))\n",
    "plt.figure()\n",
    "plt.hist(dict_class_to_sumranks[input_class]/\\\n",
    "         len(dict_class_to_sumranks[input_class].tolist()), bins=200)\n",
    "plt.show()\n",
    "input_threshold = float(input(\"Please enter a threshold based on the histogram.\"))\n",
    "#divide instances based on their ranks\n",
    "np_label_recurring = np.array(list_label_recurring)\n",
    "idx_k_inds = np.array(range(len(ds_recurring)))[np_label_recurring == input_class]\n",
    "\n",
    "idx_highrank_inlocal =  np.where(dict_class_to_sumranks[input_class] <\\\n",
    "                          (input_threshold*len(dict_class_to_sumranks[input_class].tolist())))[0]\n",
    "idx_highrank_inlocal = idx_highrank_inlocal.tolist()\n",
    "idx_highrank_inds = idx_k_inds[dict_class_to_sumranks[input_class] <\\\n",
    "                               input_threshold*len(dict_class_to_sumranks[input_class].tolist())]\n",
    "idx_lowrank_inds = np.array(list(set(range(len(ds_recurring))).difference(idx_highrank_inds)))\n",
    "assert(len(idx_highrank_inds.tolist())+len(idx_lowrank_inds.tolist()) == len(ds_recurring))\n",
    "print(\"{} percent of instances are below the threshold. exact num = {}\"\\\n",
    "      .format(100*len(idx_highrank_inds)/(np.sum(np_label_recurring==input_class)+0.0),\\\n",
    "              len(idx_highrank_inds.tolist())))\n",
    "score = 0\n",
    "for n in range(num_tocompare):\n",
    "    n_high = random.choice(idx_highrank_inds)\n",
    "    n_low = random.choice(idx_lowrank_inds)\n",
    "    img_high = ds_recurring[n_high][0]\n",
    "    img_low  = ds_recurring[n_low][0]\n",
    "    \n",
    "    list_toshow = [img_high, img_low]\n",
    "    flag_fillped = False\n",
    "    if(np.random.rand()<0.5):\n",
    "        list_toshow = [img_low, img_high]\n",
    "        flag_fillped = True\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(tfm_denormalize(list_toshow[0]).cpu().numpy().transpose(1,2,0))\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(tfm_denormalize(list_toshow[1]).cpu().numpy().transpose(1,2,0))\n",
    "    plt.show()\n",
    "    input_selection = int(input(\"Which image (left or right) seems a prototype?\"))\n",
    "    assert(input_selection in [0,1])\n",
    "    if(input_selection == 0):\n",
    "        if(flag_fillped == True):\n",
    "            score += 1\n",
    "    if(input_selection == 1):\n",
    "        if(flag_fillped == False):\n",
    "            score += 1\n",
    "print(\"score = {}\".format(score/(num_tocompare+0.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# np.max(dict_class_to_sumranks[input_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(dict_class_to_sumranks[input_class]/\\\n",
    "         len(dict_class_to_sumranks[input_class].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_onkernelspace(gpmodel, dl_input):\n",
    "    gpmodel.eval()\n",
    "    dict_n_to_kernelspace = {}\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dl_input):\n",
    "            if((idx%10) == 0):\n",
    "                print(\"Finished {} out of {}\".format(\n",
    "                    idx, len(dl_input.dataset)/dl_input.batch_size\n",
    "                ), end='\\r')\n",
    "            x, _, n = data\n",
    "            output_f1 = gpmodel.module_f1(x.to(device))\n",
    "            output_f1 = output_f1[:,:,0,0].detach().cpu().numpy()\n",
    "            \n",
    "            for idx_inminibatch, idx_inds in enumerate(n):\n",
    "                dict_n_to_kernelspace[idx_inds] = output_f1[idx_inminibatch,:].flatten().tolist()\n",
    "    \n",
    "    X_on_kernelspace = []\n",
    "    gpmodel.train()\n",
    "    return X_on_kernelspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_kernel(gpmodel, dl_recurring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((40000, 40000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = [\n",
    "1,\n",
    "2,\n",
    "5,\n",
    "6,\n",
    "7,\n",
    "8,\n",
    "9,\n",
    "11,\n",
    "14\n",
    "]\n",
    "a = np.array(a)\n",
    "b = [u+1 for u in range(len(a.tolist()))]\n",
    "b.reverse()\n",
    "print(np.sum(a*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2,3]\n",
    "l.reverse()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
