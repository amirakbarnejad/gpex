{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import loadcifar\n",
    "from loadcifar import *\n",
    "list_pathstoadd = [\n",
    "    \"../../\"\n",
    "]\n",
    "for path in list_pathstoadd:\n",
    "    if(path not in sys.path):\n",
    "        sys.path.append(path)\n",
    "#import generalGPmodule\n",
    "import localsrc_cifar10demo\n",
    "from localsrc_cifar10demo import *\n",
    "import gpex\n",
    "import resnetforcifar\n",
    "from resnetforcifar import *\n",
    "from gpex.kernelmappings.image import Resnet50BackboneKernelDivideAvgPool\n",
    "tfm_denormalize = loadcifar.ImgnetDenormalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings ====\n",
    "idx_trainingbatch = 1\n",
    "flag_enabledataaugmentation = True\n",
    "fname_gpmodel = os.path.join(\n",
    "    \"..\",\n",
    "    \"..\",\n",
    "    \"Material_PaperResults\",\n",
    "    \"Models\",\n",
    "    \"ExplainClassifier\",\n",
    "    \"cifar10.pt\"\n",
    ")\n",
    "\n",
    "#\"../../Material_PaperResults/Models/ExplainClassifier/cifar10.pt\"\n",
    "flag_loadalltraining = True\n",
    "int_mode_modulekernel = 16\n",
    "flag_train_memefficient, memefficeint_heads_in_compgraph = False, None\n",
    "du_per_class = 20\n",
    "int_exposedclass = None\n",
    "idx_split = 0\n",
    "dim_wideoutput = 1024\n",
    "flag_loadalltraining = False\n",
    "num_classes = 10\n",
    "batchsize = 10\n",
    "flag_efficient = True\n",
    "flag_detachcovpvn = True\n",
    "flag_controlvariate = True\n",
    "flag_setcovtoOne = False\n",
    "int_mode_controlvariate = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make datasets ====\n",
    "ds_rootdir = os.path.join(\n",
    "    \"..\",\n",
    "    \"..\",\n",
    "    \"Material_PaperResults\",\n",
    "    \"Datasets\",\n",
    "    \"Cifar10\"\n",
    ")\n",
    "\n",
    "ds_train = Cifar10Dataset(\n",
    "    rootdir = ds_rootdir,\n",
    "    fname_batchfile = \"data_batch_1\",\n",
    "    str_trainoreval = \"train\",\n",
    "    flag_enabledataaugmentation = flag_enabledataaugmentation,\n",
    "    flag_loadalltraining = flag_loadalltraining,\n",
    ")\n",
    "ds_inducing = Cifar10Dataset(\n",
    "    rootdir = ds_rootdir,\n",
    "    fname_batchfile = \"data_batch_1\",\n",
    "    str_trainoreval = \"eval\",\n",
    "    flag_loadalltraining = flag_loadalltraining,\n",
    ")\n",
    "ds_test = Cifar10Dataset(\n",
    "    rootdir = ds_rootdir,\n",
    "    fname_batchfile = \"test_batch\",\n",
    "    str_trainoreval = \"eval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=batchsize,\n",
    "                      shuffle=True, num_workers=0)\n",
    "dl_inducing = DataLoader(ds_inducing, batch_size=batchsize,\n",
    "                          shuffle=True, num_workers=0)\n",
    "dl_test = DataLoader(ds_test, batch_size=batchsize,\n",
    "                     shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_head is equal to 2048.\n",
      "<><><><><><><><><> finisehd creating module_tail <><><><><><><><>.\n",
      "Model was created in cuda:0.\n"
     ]
    }
   ],
   "source": [
    "model = MainModule(\n",
    "    dim_wideoutput=dim_wideoutput,\n",
    "    num_classes = num_classes,\n",
    "    device = device,\n",
    "    ds_inducing = ds_inducing,\n",
    "    dl_recurring = dl_inducing,\n",
    "    dl_nonrecurring = dl_train,\n",
    "    dl_test = dl_test,\n",
    "    batchsize = batchsize\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(\"Model was created in {}.\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controlvariate mode is set to 2\n",
      "x.shape = torch.Size([10, 3, 32, 32])\n",
      "    Dv was set to 10\n",
      "gpmodel was created on cuda:0\n"
     ]
    }
   ],
   "source": [
    "#model.n_subsampleminibatch = 2\n",
    "gpexmodule = gpex.GPEXModule(\n",
    "    module_rawmodule = model,\n",
    "    size_recurringdataset = len(ds_inducing),\n",
    "    device = device,\n",
    "    func_mainmodule_to_moduletobecomeGP = model.func_mainmodule_to_moduletobecomeGP, \n",
    "    func_feed_noise_minibatch = model.func_feed_noise_minibatch,\n",
    "    func_feed_inducing_minibatch = model.func_feed_inducing_minibatch,\n",
    "    func_feed_nonrecurring_minibatch = model.func_feed_nonrecurring_minibatch,\n",
    "    func_feed_test_minibatch = model.func_feed_test_minibatch,\n",
    "    func_get_indices_lastrecurringinstances = model.func_get_indices_lastrecurringinstances,\n",
    "    func_get_modulef1 = model.func_get_modulef1\n",
    ")\n",
    "gpexmodule.sigma2_GP = 1.0 #TODO:check\n",
    "gpexmodule.to(device)\n",
    "print(\"gpmodel was created on {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from checkpoint ====\n",
    "gpexmodule.load_state_dict(\n",
    "    torch.load(\n",
    "        fname_gpmodel\n",
    "     ),\n",
    "    strict = True\n",
    ")\n",
    "gpexmodule.train()\n",
    "gpexmodule.to(device)\n",
    "gpexmodule.renew_precomputed_XTX()\n",
    "print(\"gpmodel was loaded from checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check if g(.) and GP path match on test instances ====\n",
    "FLAG_RELU = False\n",
    "gpexmodule.renew_precomputed_XTX()\n",
    "list_outputgpoutputg = gpexmodule.checkequal_f1path_gpath_ontest(10)\n",
    "for n in range(len(list_outputgpoutputg)):\n",
    "    a = list_outputgpoutputg[n][0]\n",
    "    b = list_outputgpoutputg[n][1]\n",
    "    if(FLAG_RELU == True):\n",
    "        a = np_relu(a); b = np_relu(b)\n",
    "    \n",
    "    min_ab = min(np.min(a), np.min(b))\n",
    "    max_ab = max(np.max(a), np.max(b))\n",
    "    print(\"a-b in range [{} , {}]\".format(np.min(a-b), np.max(a-b)))\n",
    "    #print(np.round(a[0:5, 0:5], 2))\n",
    "    #print(np.round(b[0:5, 0:5], 2))\n",
    "    \n",
    "    #compute the class activations ====\n",
    "    gpexmodule.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            classactivation_a = gpexmodule.module_rawmodule.linear(torch.tensor(a).float().to(device))\n",
    "            classactivation_b = gpexmodule.module_rawmodule.linear(torch.tensor(b).float().to(device))\n",
    "            classactivation_a = classactivation_a.detach().cpu().numpy()\n",
    "            classactivation_b = classactivation_b.detach().cpu().numpy()\n",
    "            onehot_a = np.zeros((batchsize, 10))\n",
    "            onehot_a[list(range(batchsize)), np.argmax(classactivation_a, 1).tolist()] = 1\n",
    "            onehot_b = np.zeros((batchsize, 10))\n",
    "            onehot_b[list(range(batchsize)), np.argmax(classactivation_b, 1).tolist()] = 1\n",
    "\n",
    "            min_classactivations = min(np.min(classactivation_a), np.min(classactivation_b))\n",
    "            max_classactivations = max(np.max(classactivation_a), np.max(classactivation_b))\n",
    "        except:\n",
    "            onehot_a = np.zeros((batchsize, 10))\n",
    "            onehot_a[list(range(batchsize)), np.argmax(a, 1).tolist()] = 1\n",
    "            onehot_b = np.zeros((batchsize, 10))\n",
    "            onehot_b[list(range(batchsize)), np.argmax(b, 1).tolist()] = 1\n",
    "    gpexmodule.train()\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(1,6,1)\n",
    "    plt.imshow(np.round(a, 2), vmin=min_ab, vmax=max_ab, aspect=\"auto\"); plt.colorbar()\n",
    "    plt.subplot(1,6,2)\n",
    "    plt.imshow(np.round(b, 2), vmin=min_ab, vmax=max_ab, aspect=\"auto\"); plt.colorbar()\n",
    "    plt.subplot(1,6,3)\n",
    "    plt.imshow(np.round(a-b, 2), cmap=\"seismic\", vmin=-np.max(np.abs(a-b)),\\\n",
    "               vmax=np.max(np.abs(a-b)), aspect=\"auto\"); plt.colorbar()\n",
    "    plt.subplot(1,6,4)\n",
    "    plt.imshow(onehot_a, vmin=0,\n",
    "               vmax=1, aspect=\"auto\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1,6,5)\n",
    "    plt.imshow(onehot_b, vmin=0,\n",
    "               vmax=1, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1,6,6)\n",
    "    plt.imshow(np.abs(onehot_b - onehot_a), aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute GP diff ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpex.evaluation\n",
    "from gpex.evaluation import GPdiffANN_ontorchdl\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy import stats\n",
    "\n",
    "output, dict_gt = GPdiffANN_ontorchdl(\n",
    "    gpmodel = gpexmodule,\n",
    "    dl_input = dl_test,\n",
    "    input_device = device\n",
    ")\n",
    "\n",
    "np_gpann = np.array(\n",
    "    [output[k] for k in output.keys()]\n",
    ")\n",
    "np_gt = np.array([dict_gt[k] for k in output.keys()])\n",
    "\n",
    "numclasses = int(np_gpann.shape[1]/2)\n",
    "list_correlation = []\n",
    "for c in range(numclasses):\n",
    "    list_correlation.append(\n",
    "        stats.pearsonr(np_gpann[:, c], np_gpann[:, c+numclasses])[0]\n",
    "    )\n",
    "print(\"list correl = {}\".format(list_correlation))\n",
    "gp_labels = np.argmax(np_gpann[:, 0:numclasses], 1)\n",
    "ann_labels = np.argmax(np_gpann[:, numclasses::], 1)\n",
    "kappa_labels = cohen_kappa_score(gp_labels, ann_labels)\n",
    "disaggreement = np.sum(gp_labels != ann_labels)\n",
    "print(\"kappa between labels = {}\".format(kappa_labels))\n",
    "print(\"number of disagreements: {} out of {}\".format(disaggreement, np_gpann.shape[0]))\n",
    "acc_ann = np.sum(ann_labels == np_gt)/np_gpann.shape[0]\n",
    "acc_gp = np.sum(gp_labels == np_gt)/np_gpann.shape[0]\n",
    "print(\"Acuuray of ANN {}\".format(acc_ann))\n",
    "print(\"   Acuuracy of GP {}\".format(acc_gp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depict the Correlation Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "np_gp = np_gpann[:,0:num_classes]\n",
    "np_ann = np_gpann[:,num_classes::]\n",
    "np_gp_softmax = softmax(np_gp, 1)\n",
    "np_ann_softmax = softmax(np_ann, 1)\n",
    "\n",
    "list_disaggrement = (np.argmax(np_gp, 1) != np.argmax(np_ann, 1)).tolist()\n",
    "list_c = [0,0,0,0.2] #[[1,0,0,1] if(u==True) else [0,0,0,0.05] for u in list_disaggrement]\n",
    "count_plotted = 0\n",
    "for c in range(num_classes):\n",
    "    plt.ioff()\n",
    "    plt.figure()\n",
    "    plt.scatter(np_gp[:,c], np_ann[:,c], c=np.array(list_c), marker='o', facecolors='none')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.xlabel(\"GP output (head {})\".format(c+1), fontsize=18)\n",
    "    plt.ylabel(\"ANN output (head {})\".format(c+1), fontsize=18)\n",
    "    plt.title(\"Scatter for output head {}\".format(count_plotted+1), fontsize=20)\n",
    "    count_plotted += 1\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_kernel(model_input, ds_input, input_device):\n",
    "    model_input.eval()\n",
    "    toret = []\n",
    "    list_gty = []\n",
    "    list_uncertainty = []\n",
    "    list_similarities = []\n",
    "    list_x, list_y = [], []\n",
    "    list_output_g = []\n",
    "    for n in range(len(ds_input)):\n",
    "        print(\" instance {} from {}\".format(n, len(ds_input)), end='\\r')\n",
    "        x, y, _ = ds_input[n]\n",
    "        output, uncertainty, output_similarities = \\\n",
    "                model_input.testingtime_forward(x.unsqueeze(0).to(input_device), y, n)\n",
    "        #print(\"output_similaritites.shape = {}\".format(output_similarities.shape))\n",
    "        output = output[0]\n",
    "        toret.append(output.detach().cpu().numpy())\n",
    "        list_gty.append(y)\n",
    "        list_uncertainty.append(uncertainty)\n",
    "        list_similarities.append(output_similarities.detach().cpu().numpy())\n",
    "        list_x.append(x); list_y.append(y)\n",
    "        #feed the model to g(.) ====\n",
    "        output_g, _, _ = \\\n",
    "                model_input.module_rawmodule(x.unsqueeze(0).to(input_device), y, n)\n",
    "        list_output_g.append(output_g.detach().cpu().numpy().flatten().tolist())\n",
    "\n",
    "    print(\"\\n\")\n",
    "    toret = np.array(toret)\n",
    "    toret = toret[:,0,:]\n",
    "    output_g = np.array(list_output_g)\n",
    "    print(output_g.shape)\n",
    "    model_input.train()\n",
    "    return toret, list_gty, list_uncertainty, list_similarities, list_x, list_y, output_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_retval_inspectmodel = [inspect_kernel(gpmodel, ds_test, device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_recurring.label_names = ds_recurring.dict_labelnames[b'label_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two instances for the introduction of the paper ====\n",
    "#%%capture\n",
    "m = 10\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Computer Modern Roman\"],\n",
    "})\n",
    "matplotlib.rcParams['text.latex.preamble']=[r\"\\usepackage{amsmath}\"]\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "plt.figure(figsize=((m+2)*10, 2*10))\n",
    "count_subplot = 1\n",
    "for n in [2619, 48]:\n",
    "    print(\"n = {} ==========================\".format(n))\n",
    "#     fname_n, _ = ds_test._ntoimage(n)\n",
    "#     fname_n = os.path.relpath(fname_n, ds_rootdir)\n",
    "#     if(fname_n not in ds_split[\"fname_hoskys\"]):\n",
    "#         continue\n",
    "    #fields common between all models\n",
    "    _, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "    x, y = list_x[n], list_y[n]\n",
    "    \n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    str_subfolder = \"\"\n",
    "    idx_model = 0 #for idx_model in range(len(list_retval_inspectmodel)):\n",
    "    list_predy, list_gty, list_uncertainty, list_similarities, _, _, output_g = \\\n",
    "                        list_retval_inspectmodel[idx_model]\n",
    "    np_argmax_list_predyn = np.argmax(output_g[n])\n",
    "    if(np_argmax_list_predyn == list_gty[n]):\n",
    "        str_subfolder = str_subfolder + \"True\"\n",
    "    else:\n",
    "        str_subfolder = str_subfolder + \"False\"\n",
    "    kn = list_similarities[n][np_argmax_list_predyn, :].flatten()\n",
    "    idx_similars = np.argsort(-kn).tolist()[0:m]\n",
    "    plt.ioff()\n",
    "    plt.subplot(2, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "    plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "    print(\"{} =======> shown on {}\".format(n, count_subplot-1))\n",
    "    plt.axis('off')\n",
    "    print(\"===== showed.\")\n",
    "    if(n == 2619):\n",
    "        plt.title(\n",
    "                r'$\\boldsymbol{x}_{test}$',\\\n",
    "                fontsize=150\n",
    "        )\n",
    "\n",
    "    list_relevantinstances =[]\n",
    "    for count_similars in range(len(idx_similars)):\n",
    "        plt.subplot(2, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        print(\"    n, m  = {}, {} shown on {}\".format(n, count_similars, count_subplot-1))\n",
    "        plt.imshow(\n",
    "          tfm_denormalize(\n",
    "                  ds_recurring[idx_similars[count_similars]][0]\n",
    "              ).cpu().numpy().transpose(1,2,0),\n",
    "        )\n",
    "        plt.axis('off')\n",
    "        if(n == 2619):\n",
    "            plt.title(r\"$\\boldsymbol{x}_{i\"+str(count_similars+1)+\"}$\", fontsize=150)\n",
    "        list_relevantinstances.append(idx_similars[count_similars])\n",
    "            \n",
    "        \n",
    "                \n",
    "        \n",
    "if(True):#os.path.isfile(\"InterpGP/{}/{}.png\".format(str_subfolder, n)) == False):\n",
    "    plt.savefig(\n",
    "            \"InterpGP/ToPublish/twohorses.jpg\",\n",
    "            dpi=50, bbox_inches='tight', pad_inches=0, Q=100\n",
    "        )\n",
    "    print(\"================= saved.\")\n",
    "plt.close()\n",
    "#assert False\n",
    "#enable_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "m = 10\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "for n in range(len(ds_test)):\n",
    "    print(\"n = {} ==========================\".format(n))\n",
    "#     fname_n, _ = ds_test._ntoimage(n)\n",
    "#     fname_n = os.path.relpath(fname_n, ds_rootdir)\n",
    "#     if(fname_n not in ds_split[\"fname_hoskys\"]):\n",
    "#         continue\n",
    "    #fields common between all models\n",
    "    _, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "    x, y = list_x[n], list_y[n]\n",
    "    \n",
    "    plt.figure(figsize=((m+2)*10, 1*10))\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    count_subplot = 1\n",
    "    str_subfolder = \"\"\n",
    "    idx_model = 0 #for idx_model in range(len(list_retval_inspectmodel)):\n",
    "    list_predy, list_gty, list_uncertainty, list_similarities, _, _, output_g = \\\n",
    "                        list_retval_inspectmodel[idx_model]\n",
    "    np_argmax_list_predyn = np.argmax(output_g[n])\n",
    "    if(np_argmax_list_predyn == list_gty[n]):\n",
    "        str_subfolder = str_subfolder + \"True\"\n",
    "    else:\n",
    "        str_subfolder = str_subfolder + \"False\"\n",
    "    kn = list_similarities[n][np_argmax_list_predyn, :].flatten()\n",
    "    idx_similars = np.argsort(-kn).tolist()[0:m]\n",
    "    plt.ioff()\n",
    "    plt.subplot(1, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "    plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "    plt.axis('off')\n",
    "    print(\"===== showed.\")\n",
    "    plt.title(\"gt-label = {}\\n predicted = {}\\n instance {}\".format(\n",
    "                            ds_recurring.label_names[y],\n",
    "                            ds_recurring.label_names[np_argmax_list_predyn],\n",
    "                            n\n",
    "                ), fontsize=100\n",
    "             )\n",
    "\n",
    "    list_relevantinstances =[]\n",
    "    for count_similars in range(len(idx_similars)):\n",
    "        plt.subplot(1, 1*(m+2), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(\n",
    "          tfm_denormalize(\n",
    "                  ds_recurring[idx_similars[count_similars]][0]\n",
    "              ).cpu().numpy().transpose(1,2,0),\n",
    "        )\n",
    "        plt.axis('off')\n",
    "        plt.title(\"instnace {}\".format(idx_similars[count_similars]), fontsize=100)\n",
    "        list_relevantinstances.append(idx_similars[count_similars])\n",
    "            \n",
    "        \n",
    "                \n",
    "        \n",
    "    if(True):#os.path.isfile(\"InterpGP/{}/{}.png\".format(str_subfolder, n)) == False):\n",
    "        plt.savefig(\n",
    "                \"InterpGP/{}/{}/{}.jpg\".format(str_subfolder, y, n),\n",
    "                dpi=50, bbox_inches='tight', pad_inches=0, Q=80\n",
    "            )\n",
    "        print(\"================= saved.\")\n",
    "    plt.close()\n",
    "    #assert False\n",
    "#enable_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_recurring.label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToPublish Nearet Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_pilcompatible(x):\n",
    "    x = x - np.min(x)\n",
    "    x = x /np.max(x)\n",
    "    toret = (x*255.0).astype('uint8')\n",
    "    return toret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_airplane = [\n",
    "    3, 1078, 1596, 1726, 269, 27,\\\n",
    "    10, 406, 1744, 2792,\\\n",
    "    179, 180, 499, 791, 928, 1241,\\\n",
    "    1365, 2079,\\\n",
    "    352, 258, 281, 886, 2548,\\\n",
    "    189, 289, 1203, 1440, 1579, 2375, 2468,\\\n",
    "    2877, 2964,\\\n",
    "    257, 276, 602\n",
    "]\n",
    "selected_automobile = [\n",
    "    81, 114, 246,\\\n",
    "    9, 240, 645,\\\n",
    "    887, 865, 796, 1144\n",
    "]\n",
    "selected_bird = [\n",
    "    303, 86, 623, 1196, 1392, 1481,\\\n",
    "    391, 592, 603, 697, 843, 1382,\\\n",
    "    498, 123, 430\n",
    "]\n",
    "selected_cat = [\n",
    "    8, 432, 786, 950,\\\n",
    "    825, 1373, 1364\n",
    "]\n",
    "selected_deer = [\n",
    "    40, 373, 1315,\\\n",
    "    188, 802, 872, 1484,\\\n",
    "    94, 328, 465, 1207\n",
    "]\n",
    "selected_dog = [\n",
    "    502, 510, 549, 1329, 1337, 1438, 1598, 1694\n",
    "]\n",
    "selected_frog = [\n",
    "    9903, 9902, 9615, 9546, 9527, 9456, 9450,\\\n",
    "    9425, 9342, 8940\n",
    "]\n",
    "selected_horse = [\n",
    "    17, 99, 2619, 2470, 2297, 1615,\\\n",
    "    2626, 48, 2340,\\\n",
    "    203, 119, 2167, 1584,\\\n",
    "    57, 1602\n",
    "]\n",
    "selected_ship = [\n",
    "    2, 108, 233, 242,\\\n",
    "    79, 80, 185,\\\n",
    "    368, 403, 694\n",
    "]\n",
    "selected_truck = [\n",
    "    611, 940, 11, 14, 530, 585, 938, 213, 247, 538,\\\n",
    "    816, 38, 89, 259, 1104, 1707,\\\n",
    "    508, 517,\\\n",
    "    838, 922\n",
    "]\n",
    "selected_misclassified = [\n",
    "    287, 4355, 1969, 7892, 1858, 158, 1506, 1580, 3446, 8898, 6419\n",
    "]\n",
    "selected_all = selected_airplane +\\\n",
    "            selected_automobile + selected_bird +\\\n",
    "            selected_cat + selected_deer + selected_dog +\\\n",
    "            selected_frog + selected_horse + selected_ship+\\\n",
    "            selected_truck + selected_misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(selected_all))\n",
    "print(len(list(set(selected_all))))\n",
    "for u in selected_all:\n",
    "    if(selected_all.count(u) > 1):\n",
    "        print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_n = selected_all\n",
    "#%%capture\n",
    "m = 10\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "for n in range(len(ds_test)):\n",
    "    if(n not in selected_n):\n",
    "        continue\n",
    "    \n",
    "    path_tosave = \"InterpGP/ToPublish/Cifar10_NearesetNeighbours/\"\n",
    "    if(os.path.isdir(os.path.join(path_tosave, \"{}/\".format(n)))):\n",
    "        assert False\n",
    "    else:\n",
    "        os.mkdir(os.path.join(path_tosave, \"{}/\".format(n)))\n",
    "    path_instance_n = os.path.join(path_tosave, \"{}/\".format(n))\n",
    "    \n",
    "    print(\"n = {} ==========================\".format(n))\n",
    "#     fname_n, _ = ds_test._ntoimage(n)\n",
    "#     fname_n = os.path.relpath(fname_n, ds_rootdir)\n",
    "#     if(fname_n not in ds_split[\"fname_hoskys\"]):\n",
    "#         continue\n",
    "    #fields common between all models\n",
    "    _, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "    x, y = list_x[n], list_y[n]\n",
    "    \n",
    "    #plt.figure(figsize=((m+2)*10, 1*10))\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    count_subplot = 1\n",
    "    str_subfolder = \"\"\n",
    "    idx_model = 0 #for idx_model in range(len(list_retval_inspectmodel)):\n",
    "    list_predy, list_gty, list_uncertainty, list_similarities, _, _, output_g = \\\n",
    "                        list_retval_inspectmodel[idx_model]\n",
    "    np_argmax_list_predyn = np.argmax(output_g[n])\n",
    "    if(np_argmax_list_predyn == list_gty[n]):\n",
    "        str_subfolder = str_subfolder + \"True\"\n",
    "    else:\n",
    "        str_subfolder = str_subfolder + \"False\"\n",
    "    kn = list_similarities[n][np_argmax_list_predyn, :].flatten()\n",
    "    idx_similars = np.argsort(-kn).tolist()[0:m]\n",
    "    #plt.ioff()\n",
    "    #plt.subplot(1, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "    #plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "    pil_tosave = Image.fromarray(\n",
    "                    np_to_pilcompatible(\n",
    "                        tfm_denormalize(x).cpu().numpy().transpose(1,2,0)\n",
    "                    )\n",
    "    )\n",
    "    pil_tosave.save(\n",
    "        os.path.join(\n",
    "            path_instance_n, \"instance.png\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    #plt.axis('off')\n",
    "    print(\"===== showed.\")\n",
    "    \n",
    "\n",
    "    list_relevantinstances =[]\n",
    "    for count_similars in range(len(idx_similars)):\n",
    "#         plt.subplot(1, 1*(m+2), count_subplot); count_subplot+=1;\n",
    "#         plt.imshow(\n",
    "#           tfm_denormalize(\n",
    "#                   ds_recurring[idx_similars[count_similars]][0]\n",
    "#               ).cpu().numpy().transpose(1,2,0),\n",
    "#         )\n",
    "        pil_tosave = Image.fromarray(\n",
    "                    np_to_pilcompatible(\n",
    "                        tfm_denormalize(\n",
    "                              ds_recurring[idx_similars[count_similars]][0]\n",
    "                          ).cpu().numpy().transpose(1,2,0),\n",
    "                    )\n",
    "        )\n",
    "        pil_tosave.save(\n",
    "            os.path.join(\n",
    "                path_instance_n, \"{}.png\".format(count_similars+1)\n",
    "            )\n",
    "        )\n",
    "#         plt.axis('off')\n",
    "#         plt.title(\"instnace {}\".format(idx_similars[count_similars]), fontsize=100)\n",
    "        list_relevantinstances.append(idx_similars[count_similars])\n",
    "            \n",
    "        \n",
    "                \n",
    "        \n",
    "#     if(True):#os.path.isfile(\"InterpGP/{}/{}.png\".format(str_subfolder, n)) == False):\n",
    "#         plt.savefig(\n",
    "#                 \"InterpGP/{}/{}/{}.jpg\".format(str_subfolder, y, n),\n",
    "#                 dpi=50, bbox_inches='tight', pad_inches=0, Q=80\n",
    "#             )\n",
    "#         print(\"================= saved.\")\n",
    "#     plt.close()\n",
    "#     #assert False\n",
    "# #enable_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain Similarities CAM-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchofgp\n",
    "import torchofgp.kernel_explainers\n",
    "from torchofgp.kernel_explainers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_retval_inspectmodel = [inspect_kernel(gpmodel, ds_test, device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#explain the similarity itself =====\n",
    "m = 10\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "for n in range(len(ds_test)):\n",
    "    \n",
    "    #fields common between all models\n",
    "    _, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "    x, y = list_x[n], list_y[n]\n",
    "    \n",
    "    plt.figure(figsize=((m+1)*10, 3*10))\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    count_subplot = 1\n",
    "    str_subfolder = \"\"\n",
    "    for idx_model in range(len(list_retval_inspectmodel)):\n",
    "        list_predy, list_gty, list_uncertainty, list_similarities, _, _, output_g = \\\n",
    "                            list_retval_inspectmodel[idx_model]\n",
    "        np_argmax_list_predyn = np.argmax(output_g[n])\n",
    "        if(np_argmax_list_predyn == list_gty[n]):\n",
    "            str_subfolder = str_subfolder + \"True\"\n",
    "        else:\n",
    "            str_subfolder = str_subfolder + \"False\"\n",
    "        kn = list_similarities[n][np_argmax_list_predyn, :].flatten()\n",
    "        idx_similars = np.argsort(-kn).tolist()[0:m]\n",
    "        plt.ioff()\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        plt.title(\"gt-label = {}\\n predicted = {}\\n instance {}\".format(\n",
    "                                ds_recurring.label_names[y],\n",
    "                                ds_recurring.label_names[np_argmax_list_predyn],\n",
    "                                n\n",
    "                    ), fontsize=100\n",
    "                 )\n",
    "        plt.axis('off')\n",
    "        \n",
    "        list_relevantinstances =[]\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(\n",
    "              tfm_denormalize(ds_recurring[idx_similars[count_similars]][0]).cpu().numpy().transpose(1,2,0),\n",
    "            )\n",
    "            plt.axis('off')\n",
    "            plt.title(\"instnace {}\".format(idx_similars[count_similars]), fontsize=100)\n",
    "            list_relevantinstances.append(idx_similars[count_similars])\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        #plot rows 2 (explanations for x2)\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            x2 = ds_recurring[idx_similars[count_similars]][0].unsqueeze(0).to(device)\n",
    "            x1 = x.unsqueeze(0).to(device)\n",
    "            explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "              gpmodel = gpmodel,\n",
    "              func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "              x1 = x.to(device),\n",
    "              x2 = ds_recurring[idx_similars[count_similars]][0].to(device),\n",
    "              idx_ann_outputhead = y,\n",
    "              du_per_gp = du_per_class,\n",
    "              scale_resizemaps = 1.0\n",
    "            )\n",
    "            \n",
    "            toret1 = np.sum(np.sum(explanationx1x2, axis=-1), axis=-1)\n",
    "            toret2 = np.sum(np.sum(explanationx1x2, axis=0), axis=0)\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(toret2, cmap=\"seismic\", vmin=np.min(toret2), vmax=np.max(toret2))\n",
    "            plt.axis('off')\n",
    "        #plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        #plt.hist(kn.flatten(), bins=200)\n",
    "        \n",
    "        #plot rows 3 (explanations for the instance itself)\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            x2 = ds_recurring[idx_similars[count_similars]][0].unsqueeze(0).to(device)\n",
    "            x1 = x.unsqueeze(0).to(device)\n",
    "            explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "              gpmodel = gpmodel,\n",
    "              func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "              x1 = x.to(device),\n",
    "              x2 = ds_recurring[idx_similars[count_similars]][0].to(device),\n",
    "              idx_ann_outputhead = y,\n",
    "              du_per_gp = du_per_class,\n",
    "              scale_resizemaps = 1.0\n",
    "            )\n",
    "            \n",
    "            toret1 = np.sum(np.sum(explanationx1x2, axis=-1), axis=-1)\n",
    "            toret2 = np.sum(np.sum(explanationx1x2, axis=0), axis=0)\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(toret1, cmap=\"seismic\", vmin=np.min(toret1), vmax=np.max(toret1))\n",
    "                \n",
    "        \n",
    "    if(os.path.isfile(\"InterpGP/Kernel/{}/{}.jpg\".format(str_subfolder, n)) == False):\n",
    "        plt.savefig(\n",
    "                \"InterpGP/Kernel/{}/{}.jpg\".format(str_subfolder, n),\n",
    "                dpi=20, bbox_inches='tight', pad_inches=0, Q=80\n",
    "            )\n",
    "    plt.close()\n",
    "    #assert False\n",
    "#enable_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain by Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchofgp\n",
    "import torchofgp.kernel_explainers\n",
    "from torchofgp.kernel_explainers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_retval_inspectmodel = [inspect_kernel(gpmodel, ds_test, device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_retvals = open('InterpGP/retvals.pkl', 'wb')\n",
    "# pickle.dump(list_retval_inspectmodelval_inspectmodel, file_retvals, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_retvals = open('InterpGP/retvals.pkl', 'rb')\n",
    "list_retval_inspectmodel = pickle.load(file_retvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings ===\n",
    "n1 = 21\n",
    "n2 = 9064\n",
    "rgbsigma = 0.0\n",
    "#explain the similarity ===\n",
    "_, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "x1, y1 = list_x[n1], list_y[n1]\n",
    "x2 = ds_recurring[n2][0]\n",
    "explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "      gpmodel = gpmodel,\n",
    "      func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "      x1 = x1.to(device),\n",
    "      x2 = x2.to(device),\n",
    "      idx_ann_outputhead = y1,\n",
    "      du_per_gp = du_per_class,\n",
    "      scale_resizemaps = 1.0,\n",
    "      mode_upsample='nearest',\n",
    "      flag_aligcorners = None\n",
    ")\n",
    "explanationx1x2 = explanationx1x2 * ((explanationx1x2>0.0) + 0.0)\n",
    "print(\"shape of CAM-like output = {}\".format(explanationx1x2.shape))\n",
    "\n",
    "#explanationx1x2 = explanationx1x2/np.max(explanationx1x2) #TODO:check\n",
    "#explanationx1x2[explanationx1x2 < 0.1] = 0.0 #TODO:check\n",
    "\n",
    "#compute the heatmaps for img1 and img2 ===\n",
    "heatmap_1 = np.sum(np.sum(explanationx1x2, 3), 2)\n",
    "heatmap_2 = np.sum(np.sum(explanationx1x2, 0), 0)\n",
    "normalized_heatmap_1 = heatmap_1/np.max(heatmap_1)\n",
    "normalized_heatmap_2 = heatmap_2/np.max(heatmap_2)\n",
    "\n",
    "#assert False\n",
    "#make the pixel similarity matrix ====\n",
    "num_pix1 = explanationx1x2.shape[0] * explanationx1x2.shape[1]\n",
    "num_pix2 = explanationx1x2.shape[2] * explanationx1x2.shape[3]\n",
    "np_pixelmatrix = np.zeros((num_pix1+num_pix2 , num_pix1+num_pix2))\n",
    "\n",
    "#for pixels of image 1\n",
    "for idx_ij in range(num_pix1):\n",
    "    i, j = np.unravel_index(idx_ij, [explanationx1x2.shape[0], explanationx1x2.shape[1]])\n",
    "    #fill the similarities between i,j and image 2\n",
    "    for idx_kl in range(num_pix2):\n",
    "        k, l = np.unravel_index(idx_kl, [explanationx1x2.shape[2], explanationx1x2.shape[3]])\n",
    "        np_pixelmatrix[idx_ij, idx_kl] = explanationx1x2[i,j,k,l]\n",
    "    #fill the similarities between i,j and image 1 \n",
    "    for idx_i2j2 in range(num_pix1):\n",
    "        if(idx_i2j2 != idx_ij):\n",
    "            i2, j2 = np.unravel_index(\n",
    "                    idx_i2j2, [explanationx1x2.shape[0], explanationx1x2.shape[1]]\n",
    "                )\n",
    "            dist = np.array([i,j]) - np.array([i2,j2])\n",
    "            if(rgbsigma != 0.0):\n",
    "                rbf = np.exp(np.sum(-dist*dist) / (2.0*rgbsigma * rgbsigma))\n",
    "            else:\n",
    "                rbf = 0.0\n",
    "            np_pixelmatrix[idx_ij, idx_i2j2] = rbf\n",
    "            \n",
    "#for pixels of image 2\n",
    "for idx_kl in range(num_pix2):\n",
    "    k, l = np.unravel_index(idx_kl, [explanationx1x2.shape[2], explanationx1x2.shape[3]])\n",
    "    #fill the similarities between k,l and image 1\n",
    "    for idx_ij in range(num_pix1):\n",
    "        i, j = np.unravel_index(idx_ij, [explanationx1x2.shape[0], explanationx1x2.shape[1]])\n",
    "        np_pixelmatrix[idx_kl, idx_ij] = explanationx1x2[i,j,k,l]\n",
    "    #fill the similarities between k,l and image 2 \n",
    "    for idx_k2l2 in range(num_pix2):\n",
    "        if(idx_k2l2 != idx_kl):\n",
    "            k2, l2 = np.unravel_index(\n",
    "                    idx_k2l2, [explanationx1x2.shape[2], explanationx1x2.shape[3]]\n",
    "                )\n",
    "            dist = np.array([k,l]) - np.array([k2,l2])\n",
    "            if(rbf != 0.0):\n",
    "                rbf = np.exp(np.sum(-dist*dist) / (2.0*rgbsigma * rgbsigma))\n",
    "            else:\n",
    "                rbf = 0.0\n",
    "            np_pixelmatrix[idx_kl, idx_k2l2] = rbf\n",
    "            \n",
    "print(\"Computed the similarity matrix of shape {}\".format(np_pixelmatrix.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the histogram of two heatmaps ===\n",
    "plt.figure()\n",
    "plt.hist(heatmap_1.flatten(), bins=50)\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.hist(heatmap_2.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#run spectral clustering on pixel similarity matrix ====\n",
    "#settings ===\n",
    "n_clusters = 2\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "clustering = SpectralClustering(\n",
    "         affinity = \"precomputed\",\n",
    "         n_clusters= n_clusters,\n",
    "         random_state=1\n",
    ").fit(np_pixelmatrix+0.0)\n",
    "\n",
    "#show the clustering result\n",
    "plt.figure(figsize=((clustering.n_clusters+2)*10, 2*10))\n",
    "count_subplot = 1\n",
    "plt.subplot(2, clustering.n_clusters+2, count_subplot); count_subplot +=clustering.n_clusters+2;\n",
    "plt.imshow(tfm_denormalize(x1).cpu().numpy().transpose(1,2,0))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, clustering.n_clusters+2, count_subplot); count_subplot -= (clustering.n_clusters+1);\n",
    "plt.imshow(tfm_denormalize(x2).cpu().numpy().transpose(1,2,0))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.subplot(2, clustering.n_clusters+2, count_subplot); count_subplot +=clustering.n_clusters+2;\n",
    "plt.imshow(heatmap_1, cmap='seismic')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, clustering.n_clusters+2, count_subplot); count_subplot -= (clustering.n_clusters+1);\n",
    "plt.imshow(heatmap_2, cmap='seismic')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "for c in range(clustering.n_clusters):\n",
    "    idx_pixelsinc = np.where(np.array(clustering.labels_) == c)[0].tolist() #in rng pix1+pix2\n",
    "    \n",
    "    \n",
    "    #plot the cluster c for image 1\n",
    "    img1_inc = np.zeros((explanationx1x2.shape[0], explanationx1x2.shape[1]))\n",
    "    plt.subplot(\n",
    "        2, clustering.n_clusters+2,\n",
    "        count_subplot); count_subplot +=clustering.n_clusters+2;\n",
    "    idx_img1_inc = [idx for idx in idx_pixelsinc if(idx<num_pix1)]\n",
    "    if(idx_img1_inc != []):\n",
    "        np_img1_ij = np.array(np.unravel_index(idx_img1_inc, img1_inc.shape))\n",
    "        img1_inc[np_img1_ij[0,:] , np_img1_ij[1,:]] = 1\n",
    "    plt.imshow(img1_inc, vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    #plot the cluster c for image 2\n",
    "    img2_inc = np.zeros((explanationx1x2.shape[2], explanationx1x2.shape[3]))\n",
    "    plt.subplot(\n",
    "        2, clustering.n_clusters+2,\n",
    "        count_subplot); count_subplot -= (clustering.n_clusters+1);\n",
    "    idx_img2_inc = [idx-num_pix1 for idx in idx_pixelsinc if(idx>=num_pix1)]\n",
    "    if(idx_img2_inc != []):\n",
    "        np_img2_ij = np.array(np.unravel_index(idx_img2_inc, img2_inc.shape))\n",
    "        img2_inc[np_img2_ij[0,:] , np_img2_ij[1,:]] = 1\n",
    "    plt.imshow(img2_inc, vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explain the similarity itself =====\n",
    "m = 5\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "for n in range(len(ds_test)):\n",
    "    fname_n, _ = ds_test._ntoimage(n)\n",
    "    fname_n = os.path.relpath(fname_n, ds_rootdir)\n",
    "#     if(fname_n not in ds_split[\"fname_hoskys\"]):\n",
    "#         continue\n",
    "    #fields common between all models\n",
    "    _, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "    x, y = list_x[n], list_y[n]\n",
    "    \n",
    "    plt.figure(figsize=((m+1)*10, 3*10))\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    count_subplot = 1\n",
    "    str_subfolder = \"\"\n",
    "    for idx_model in range(len(list_retval_inspectmodel)):\n",
    "        list_predy, list_gty, list_uncertainty, list_similarities, _, _, output_g = \\\n",
    "                            list_retval_inspectmodel[idx_model]\n",
    "        np_argmax_list_predyn = np.argmax(output_g[n])\n",
    "        if(np_argmax_list_predyn == list_gty[n]):\n",
    "            str_subfolder = str_subfolder + \"True\"\n",
    "        else:\n",
    "            str_subfolder = str_subfolder + \"False\"\n",
    "        kn = list_similarities[n][np_argmax_list_predyn, :].flatten()\n",
    "        idx_similars = np.argsort(-kn).tolist()[0:m]\n",
    "        plt.ioff()\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        plt.title(\"gt-label = {}\\n predicted = {}\\n instance {}\".format(\n",
    "                                [\"dog\", \"wolf\"][y],\n",
    "                                [\"dog\", \"wolf\"][np_argmax_list_predyn],\n",
    "                                n\n",
    "                    ), fontsize=100\n",
    "                 )\n",
    "        \n",
    "        list_relevantinstances =[]\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(\n",
    "              tfm_denormalize(ds_recurring[idx_similars[count_similars]][0]).cpu().numpy().transpose(1,2,0),\n",
    "            )\n",
    "            plt.title(\"instnace {}\".format(idx_similars[count_similars]), fontsize=100)\n",
    "            list_relevantinstances.append(idx_similars[count_similars])\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        #plot rows 2 (explanations for x2)\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            x2 = ds_recurring[idx_similars[count_similars]][0].unsqueeze(0).to(device)\n",
    "            x1 = x.unsqueeze(0).to(device)\n",
    "            explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "              gpmodel = gpmodel,\n",
    "              func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "              x1 = x.to(device),\n",
    "              x2 = ds_recurring[idx_similars[count_similars]][0].to(device),\n",
    "              idx_ann_outputhead = y,\n",
    "              du_per_gp = du_per_class,\n",
    "              scale_resizemaps = 2.0\n",
    "            )\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            toret1 = np.sum(np.sum(explanationx1x2, axis=-1), axis=-1)\n",
    "            toret2 = np.sum(np.sum(explanationx1x2, axis=0), axis=0)\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(toret2, cmap=\"seismic\", vmin=np.min(toret2), vmax=np.max(toret2))\n",
    "        #plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        #plt.hist(kn.flatten(), bins=200)\n",
    "        \n",
    "        #plot rows 3 (explanations for the instance itself)\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            x2 = ds_recurring[idx_similars[count_similars]][0].unsqueeze(0).to(device)\n",
    "            x1 = x.unsqueeze(0).to(device)\n",
    "            explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "              gpmodel = gpmodel,\n",
    "              func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "              x1 = x.to(device),\n",
    "              x2 = ds_recurring[idx_similars[count_similars]][0].to(device),\n",
    "              idx_ann_outputhead = y,\n",
    "              du_per_gp = du_per_class,\n",
    "              scale_resizemaps = 2.0\n",
    "            )\n",
    "            \n",
    "            toret1 = np.sum(np.sum(explanationx1x2, axis=-1), axis=-1)\n",
    "            toret2 = np.sum(np.sum(explanationx1x2, axis=0), axis=0)\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(toret1, cmap=\"seismic\", vmin=np.min(toret1), vmax=np.max(toret1))\n",
    "                \n",
    "        \n",
    "    if(os.path.isfile(\"InterpGP/RegionBased/{}/{}.png\".format(str_subfolder, n)) == False):\n",
    "        plt.savefig(\n",
    "                \"InterpGP/RegionBased/{}/{}.png\".format(str_subfolder, n),\n",
    "                dpi=80, bbox_inches='tight', pad_inches=0, Q=100\n",
    "            )\n",
    "    plt.close()\n",
    "    assert False\n",
    "#enable_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depict the Prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_label_recurring = [ds_recurring.list_labelnames.index(ds_recurring._ntoimage(n)[1]+\"/\") for n in range(len(ds_recurring))]\n",
    "def inspect_kernel(module_input, dl_input, list_label_recurring):\n",
    "    np_lable_recurring = np.array(list_label_recurring)\n",
    "    dict_class_to_sumranks = {u:0.0 for u in range(module_input.Dv)}\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dl_input):\n",
    "            if((idx%10) == 0):\n",
    "                print(\"Finished {} out of {}\".format(\n",
    "                    idx, len(dl_input.dataset)/dl_input.batch_size\n",
    "                ), end='\\r')\n",
    "            x, y, n = data\n",
    "            output, uncertainty, output_similarities = \\\n",
    "                module_input.testingtime_forward(x.to(device), y, n)\n",
    "            output_similarities = output_similarities.detach().cpu().numpy() #[9xbatchsizex M]\n",
    "            for idx_inminibatch, idx_inds in enumerate(n):\n",
    "                xn_similarities = output_similarities[int(y[idx_inminibatch]),idx_inminibatch, :] #[M]\n",
    "                xn_similarities = xn_similarities.flatten()\n",
    "                xn_similarities = xn_similarities[np_lable_recurring == int(y[idx_inminibatch])]\n",
    "                dict_class_to_sumranks[int(y[idx_inminibatch])] += np.argsort(-xn_similarities)\n",
    "                \n",
    "    \n",
    "    return dict_class_to_sumranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_class_to_sumranks = inspect_kernel(gpmodel, dl_test, list_label_recurring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump( dict_class_to_sumranks, open(\"TrainingHistory/dict_class_to_sumranks.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_class_to_sumranks = pickle.load( open( \"TrainingHistory/dict_class_to_sumranks.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dict_class_to_sumranks.keys():\n",
    "    plt.figure()\n",
    "    plt.hist(dict_class_to_sumranks[k]/len(dict_class_to_sumranks[k].tolist()), bins=200)\n",
    "    plt.title(\"histogram of avg. ranks among the training instances \\n for class {}.\".format(k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import relatedwork\n",
    "import relatedwork.utils.transforms\n",
    "tfm_denormalize = relatedwork.utils.transforms.ImgnetDenormalize()\n",
    "#open the app to check whether the sample-compression scheme matches human's ====\n",
    "num_tocompare = 10\n",
    "input_class = int(input(\"Please input the class (between 0 and 9).\"))\n",
    "plt.figure()\n",
    "plt.hist(dict_class_to_sumranks[input_class]/\\\n",
    "         len(dict_class_to_sumranks[input_class].tolist()), bins=200)\n",
    "plt.show()\n",
    "input_threshold = float(input(\"Please enter a threshold based on the histogram.\"))\n",
    "#divide instances based on their ranks\n",
    "np_label_recurring = np.array(list_label_recurring)\n",
    "idx_k_inds = np.array(range(len(ds_recurring)))[np_label_recurring == input_class]\n",
    "\n",
    "idx_highrank_inlocal =  np.where(dict_class_to_sumranks[input_class] <\\\n",
    "                          (input_threshold*len(dict_class_to_sumranks[input_class].tolist())))[0]\n",
    "idx_highrank_inlocal = idx_highrank_inlocal.tolist()\n",
    "idx_highrank_inds = idx_k_inds[dict_class_to_sumranks[input_class] <\\\n",
    "                               input_threshold*len(dict_class_to_sumranks[input_class].tolist())]\n",
    "idx_lowrank_inds = np.array(list(set(range(len(ds_recurring))).difference(idx_highrank_inds)))\n",
    "assert(len(idx_highrank_inds.tolist())+len(idx_lowrank_inds.tolist()) == len(ds_recurring))\n",
    "print(\"{} percent of instances are below the threshold. exact num = {}\"\\\n",
    "      .format(100*len(idx_highrank_inds)/(np.sum(np_label_recurring==input_class)+0.0),\\\n",
    "              len(idx_highrank_inds.tolist())))\n",
    "score = 0\n",
    "for n in range(num_tocompare):\n",
    "    n_high = random.choice(idx_highrank_inds)\n",
    "    n_low = random.choice(idx_lowrank_inds)\n",
    "    img_high = ds_recurring[n_high][0]\n",
    "    img_low  = ds_recurring[n_low][0]\n",
    "    \n",
    "    list_toshow = [img_high, img_low]\n",
    "    flag_fillped = False\n",
    "    if(np.random.rand()<0.5):\n",
    "        list_toshow = [img_low, img_high]\n",
    "        flag_fillped = True\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(tfm_denormalize(list_toshow[0]).cpu().numpy().transpose(1,2,0))\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(tfm_denormalize(list_toshow[1]).cpu().numpy().transpose(1,2,0))\n",
    "    plt.show()\n",
    "    input_selection = int(input(\"Which image (left or right) seems a prototype?\"))\n",
    "    assert(input_selection in [0,1])\n",
    "    if(input_selection == 0):\n",
    "        if(flag_fillped == True):\n",
    "            score += 1\n",
    "    if(input_selection == 1):\n",
    "        if(flag_fillped == False):\n",
    "            score += 1\n",
    "print(\"score = {}\".format(score/(num_tocompare+0.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# np.max(dict_class_to_sumranks[input_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(dict_class_to_sumranks[input_class]/\\\n",
    "         len(dict_class_to_sumranks[input_class].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_onkernelspace(gpmodel, dl_input):\n",
    "    gpmodel.eval()\n",
    "    dict_n_to_kernelspace = {}\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dl_input):\n",
    "            if((idx%10) == 0):\n",
    "                print(\"Finished {} out of {}\".format(\n",
    "                    idx, len(dl_input.dataset)/dl_input.batch_size\n",
    "                ), end='\\r')\n",
    "            x, _, n = data\n",
    "            output_f1 = gpmodel.module_f1(x.to(device))\n",
    "            output_f1 = output_f1[:,:,0,0].detach().cpu().numpy()\n",
    "            \n",
    "            for idx_inminibatch, idx_inds in enumerate(n):\n",
    "                dict_n_to_kernelspace[idx_inds] = output_f1[idx_inminibatch,:].flatten().tolist()\n",
    "    \n",
    "    X_on_kernelspace = []\n",
    "    gpmodel.train()\n",
    "    return X_on_kernelspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_kernel(gpmodel, dl_recurring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((40000, 40000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = [\n",
    "1,\n",
    "2,\n",
    "5,\n",
    "6,\n",
    "7,\n",
    "8,\n",
    "9,\n",
    "11,\n",
    "14\n",
    "]\n",
    "a = np.array(a)\n",
    "b = [u+1 for u in range(len(a.tolist()))]\n",
    "b.reverse()\n",
    "print(np.sum(a*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2,3]\n",
    "l.reverse()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
