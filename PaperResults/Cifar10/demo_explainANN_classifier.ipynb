{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import loadcifar\n",
    "from loadcifar import *\n",
    "list_pathstoadd = [\n",
    "    \"../../\"\n",
    "]\n",
    "for path in list_pathstoadd:\n",
    "    if(path not in sys.path):\n",
    "        sys.path.append(path)\n",
    "#import generalGPmodule\n",
    "import localsrc_cifar10demo\n",
    "from localsrc_cifar10demo import *\n",
    "import gpex\n",
    "import resnetforcifar\n",
    "from resnetforcifar import *\n",
    "from gpex.kernelmappings.image import Resnet50BackboneKernelDivideAvgPool\n",
    "tfm_denormalize = loadcifar.ImgnetDenormalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings ====\n",
    "idx_trainingbatch = 1\n",
    "flag_enabledataaugmentation = True\n",
    "fname_gpmodel = \"TrainingHistory/Phase2/Oct25ExplainANNonBeluga/\"+\\\n",
    "                \"output_explainann_version1_afterepoch2.pt\"\n",
    "#\"TrainingHistory/Phase2/Dec02_Runs_On_narval/\"+\\\n",
    "#\"output_paramanalysis1_1024_200_version6_afterepoch2.pt\"\n",
    "flag_loadalltraining = True\n",
    "int_mode_modulekernel = 16\n",
    "flag_train_memefficient, memefficeint_heads_in_compgraph = False, None\n",
    "du_per_class = 20\n",
    "int_exposedclass = None\n",
    "idx_split = 0\n",
    "dim_wideoutput = 1024\n",
    "flag_loadalltraining = False\n",
    "num_classes = 10\n",
    "batchsize = 10\n",
    "flag_efficient = True\n",
    "flag_detachcovpvn = True\n",
    "flag_controlvariate = True\n",
    "flag_setcovtoOne = False\n",
    "int_mode_controlvariate = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make datasets ====\n",
    "ds_rootdir = os.path.join(\n",
    "    \"..\",\n",
    "    \"..\",\n",
    "    \"Material_PaperResults\",\n",
    "    \"Datasets\",\n",
    "    \"Cifar10\"\n",
    ")\n",
    "\n",
    "ds_train = Cifar10Dataset(\n",
    "    rootdir = ds_rootdir,\n",
    "    fname_batchfile = \"data_batch_1\",\n",
    "    str_trainoreval = \"train\",\n",
    "    flag_enabledataaugmentation = flag_enabledataaugmentation,\n",
    "    flag_loadalltraining = flag_loadalltraining,\n",
    ")\n",
    "ds_inducing = Cifar10Dataset(\n",
    "    rootdir = ds_rootdir,\n",
    "    fname_batchfile = \"data_batch_1\",\n",
    "    str_trainoreval = \"eval\",\n",
    "    flag_loadalltraining = flag_loadalltraining,\n",
    ")\n",
    "ds_test = Cifar10Dataset(\n",
    "    rootdir = ds_rootdir,\n",
    "    fname_batchfile = \"test_batch\",\n",
    "    str_trainoreval = \"eval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=batchsize,\n",
    "                      shuffle=True, num_workers=0)\n",
    "dl_inducing = DataLoader(ds_inducing, batch_size=batchsize,\n",
    "                          shuffle=True, num_workers=0)\n",
    "dl_test = DataLoader(ds_test, batch_size=batchsize,\n",
    "                     shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ClampAndTanh(torch.nn.Module):\n",
    "#     def __init_(self, minval=-1.0, maxval=1.0):\n",
    "#         self.minval, self.maxval = minval, maxval\n",
    "#         super(ClampAndTanh, self).__init__()\n",
    "        \n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         output = torch.nn.functional.tanh(\n",
    "#                         torch.clamp(\n",
    "#                             x, -1.0, 1.0\n",
    "#                         )\n",
    "#                     )\n",
    "#         return output\n",
    "    \n",
    "# def initweights_to_zero(m):\n",
    "#     if type(m) in {nn.Linear, nn.Conv2d, nn.Linear}:\n",
    "#         torch.nn.init.zeros_(m.weight)\n",
    "#         m.bias.data.fill_(np.random.randn()*0.1) #TODO:check\n",
    "    \n",
    "# class ModuleF1(torch.nn.Module):\n",
    "#     def __init__(self, module_caller):\n",
    "#         super(ModuleF1, self).__init__()\n",
    "#         #make internals ===\n",
    "#         if(int_mode_modulekernel == 1):\n",
    "#             #a list of resnet18s\n",
    "#             self.module = TinyResNet18List(\n",
    "#                 scale_macrokernel = 1,\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 2):\n",
    "#             #a list of resnet50s\n",
    "#             self.module = Resnet50List(\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 3):\n",
    "#             #a list of resnet18s eroded by 2\n",
    "#             self.module = TinyResNet18List(\n",
    "#                 scale_macrokernel = 2,\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 4):\n",
    "#             #a list of squeezenets\n",
    "#             self.module = SqueezeNetList(\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 5):\n",
    "#             #a list of resnet18s eroded by 4\n",
    "#             self.module = TinyResNet18List(\n",
    "#                 scale_macrokernel = 4,\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 6):\n",
    "#             #10 instances of MultiResnet18ListAndOneLayer ====\n",
    "#             self.module = MultiResnet18ListAndOneLayer(\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class,\n",
    "#                 num_backbones = num_backbones\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 7):\n",
    "#             self.module = MultiResnet50ListAndOneLayer(\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class,\n",
    "#                 num_backbones = num_backbones\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 9):\n",
    "#             self.module = PrimaryNetwork(\n",
    "#                     num_GPs=num_classes,\n",
    "#                     du_per_class = du_per_class\n",
    "#                 )\n",
    "#         elif(int_mode_modulekernel == 12):\n",
    "#             self.module = Resnet18BackboneKernel(\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 13):\n",
    "#             self.module = Resnet50BackboneKernel(\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 14):\n",
    "#             self.module = Resnet34BackboneKernel(\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 15):\n",
    "#             self.module = Resnet18BackboneKernelDivideAvgPool(\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 16):\n",
    "#             self.module = Resnet50BackboneKernelDivideAvgPool(\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 17):\n",
    "#             self.module = Resnet34BackboneKernelDivideAvgPool(\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 18):\n",
    "#             self.module = Resnet101BackboneKernelDivideAvgPool(\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 19):\n",
    "#             self.module = Resnet152BackboneKernelDivideAvgPool(\n",
    "#                 num_classes = num_classes,\n",
    "#                 du_per_class = du_per_class\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 21):\n",
    "#             self.module = Resnet50List(\n",
    "#                     num_classes = num_classes,\n",
    "#                     du_per_class = du_per_class,\n",
    "#                     scale_macrokernel = 2.0\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 22):\n",
    "#             self.module = Resnet34List(\n",
    "#                     num_classes = num_classes,\n",
    "#                     du_per_class = du_per_class,\n",
    "#                     scale_macrokernel = 2.0\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 23):\n",
    "#             self.module = Resnet101List(\n",
    "#                     num_classes = num_classes,\n",
    "#                     du_per_class = du_per_class,\n",
    "#                     scale_macrokernel = 2.0\n",
    "#             )\n",
    "#         elif(int_mode_modulekernel == 24):\n",
    "#             self.module = Resnet152List(\n",
    "#                     num_classes = num_classes,\n",
    "#                     du_per_class = du_per_class,\n",
    "#                     scale_macrokernel = 2.0\n",
    "#             )\n",
    "#         else:\n",
    "#             print(\"Unknown mode_modulekernel {}.\".format(int_mode_modulekernel))\n",
    "#             assert False\n",
    "#         print(\"<><><><><><><><><> finisehd creating module_tail <><><><><><><><>.\")\n",
    "    \n",
    "#     def set_rng_outputheads(self, rng_outputhead):\n",
    "#         self.module.set_rng_outputheads(rng_outputhead)\n",
    "    \n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         toret = self.module(x)\n",
    "#         toret = toret.unsqueeze(-1).unsqueeze(-1)\n",
    "#         return toret\n",
    "            \n",
    "# class MainModule(nn.Module):\n",
    "#     def __init__(self, num_classes, device, dl_recurring, dl_nonrecurring, dl_test, batchsize, dim_wideoutput):\n",
    "#         '''\n",
    "#         Inputs:\n",
    "#             - size_input: size of the input, e.g., [32 x 2000 x 7 x 7].\n",
    "#             - device: the device on which the GP fields are going to be created.\n",
    "#             - num_outputheads: an integer, number of output heads.\n",
    "#         '''\n",
    "#         super(MainModule, self).__init__()\n",
    "#         #grab args ===\n",
    "#         self.num_classes = num_classes\n",
    "#         self.device = device\n",
    "#         self.dl_recurring = dl_recurring\n",
    "#         self.dl_nonrecurring = dl_nonrecurring\n",
    "#         self.dl_test = dl_test\n",
    "#         self.batchsize = batchsize\n",
    "#         #self.iter_dl_recurring = iter(self.dl_recurring)\n",
    "#         #make internal module_tobecomeGP ===\n",
    "#         self.module_tobecomeGP = akresnetforcifar.AkResNet18(\n",
    "#             num_classes=num_classes,\n",
    "#             dim_wideoutput=dim_wideoutput\n",
    "#         )\n",
    "#         #make internals ===\n",
    "#         self.dic_dlname_to_iter = {\n",
    "#             \"dl_recurring\":iter(self.dl_recurring),\n",
    "#             \"dl_nonrecurring\":iter(self.dl_nonrecurring),\n",
    "#             \"dl_test\":iter(self.dl_test)\n",
    "#         }\n",
    "#         #make module f1 ===\n",
    "#         self.module_f1 = ModuleF1(self) #nn.Sequential(\n",
    "#         self._lastidx_recurring = []\n",
    "#         #internal field to subsample when feeding minbatch ====\n",
    "#         self.n_subsampleminibatch = None\n",
    "            \n",
    "#     def forward(self, x, y, n):\n",
    "#          return self.module_tobecomeGP(x), y, n\n",
    "        \n",
    "    \n",
    "#     def func_get_modulef1(self):\n",
    "#         return self.module_f1\n",
    "    \n",
    "#     def func_mainmodule_to_moduletobecomeGP(self, module_input):\n",
    "#         return module_input.module_tobecomeGP\n",
    "    \n",
    "    \n",
    "#     def _func_feed_minibatch(self, dl_input, str_dlname, flag_addnoisetoX = False):\n",
    "#         #print(\"reached here 1\")\n",
    "#         if(False):#iter_dl is None):\n",
    "#             pass #x, y, n = next(iter(dl_input))\n",
    "#         else:\n",
    "#             try:\n",
    "#                 x, y, n = next(self.dic_dlname_to_iter[str_dlname])\n",
    "#             except (StopIteration):\n",
    "#                 self.dic_dlname_to_iter[str_dlname] = iter(dl_input)\n",
    "#                 x, y, n = next(self.dic_dlname_to_iter[str_dlname])\n",
    "                \n",
    "        \n",
    "#         if(flag_addnoisetoX == True):\n",
    "#             idx_permutex = np.random.permutation([u for u in range(list(x.size())[0])]).tolist()\n",
    "#             idx_permutex = torch.LongTensor(idx_permutex)\n",
    "#             x_perumted = x[idx_permutex, :, :, :]\n",
    "            \n",
    "#             rand_w = torch.rand((list(x.size())[0])).float().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) #[Nx1x1x1]\n",
    "#             rand_w = rand_w*(rng_randw[1]-rng_randw[0]) + rng_randw[0] #in [rng[0] , rng[1]]\n",
    "#             rand_w = rand_w.detach()\n",
    "#             #print(\"rand_w.shape = {}\".format(rand_w.shape))\n",
    "#             x = rand_w*x + (1.0-rand_w)*x_perumted  #[N x 3 x 224 x 224]\n",
    "#             x = x + 0.1*torch.randn_like(x).float()\n",
    "        \n",
    "#         #print(\"reached here 2\")\n",
    "# #         if(dl_input.dataset == ds_recurring):\n",
    "# #             self._lastidx_recurring = n #TODO:move this operation inside the dl.\n",
    "#         #print(\"reached here 3\")\n",
    "#         #print(\"x.shape = {}\".format(x.shape))\n",
    "#         #print(\"y.shape = {}\".format(y.shape))\n",
    "        \n",
    "#         if(self.n_subsampleminibatch is None):\n",
    "#             pass\n",
    "#         else:\n",
    "#             x = x[0:self.n_subsampleminibatch, :,:,:]\n",
    "#             y = y[0:self.n_subsampleminibatch]\n",
    "#             n = n[0:self.n_subsampleminibatch]\n",
    "        \n",
    "#         if(dl_input.dataset == ds_recurring):\n",
    "#             self._lastidx_recurring = n\n",
    "#             if(len(n) != list(x.size())[0]):\n",
    "#                 assert False\n",
    "        \n",
    "#         output, _, _ = self.forward(x.to(self.device), y, n)\n",
    "        \n",
    "#         #print(\"reached here 4\")\n",
    "#         return output, y, n\n",
    "    \n",
    "#     def func_feed_recurring_minibatch(self):\n",
    "#         output, y, n = self._func_feed_minibatch(\n",
    "#                             self.dl_recurring,\n",
    "#                             str_dlname = \"dl_recurring\"\n",
    "#                          )\n",
    "#         return output, y, n\n",
    "    \n",
    "#     def func_feed_noise_minibatch(self):\n",
    "#         output, y, n = self._func_feed_minibatch(\n",
    "#                         self.dl_nonrecurring,\n",
    "#                         str_dlname = \"dl_nonrecurring\",\n",
    "#                         flag_addnoisetoX=True\n",
    "#                       )\n",
    "#         return output, y, n\n",
    "    \n",
    "#     def func_feed_nonrecurring_minibatch(self):\n",
    "#         output, y, n = self._func_feed_minibatch(\n",
    "#                                 self.dl_nonrecurring,\n",
    "#                                 str_dlname = \"dl_nonrecurring\",\n",
    "#                                 flag_addnoisetoX=False\n",
    "#                               )\n",
    "#         return output, y, n\n",
    "    \n",
    "#     def func_feed_test_minibatch(self):\n",
    "#         output, y, n = self._func_feed_minibatch(\n",
    "#                                     self.dl_test,\n",
    "#                                     str_dlname = \"dl_test\"\n",
    "#                                 )\n",
    "#         return output, y, n\n",
    "    \n",
    "#     def func_get_indices_lastrecurringinstances(self):\n",
    "#         return self._lastidx_recurring.cpu().numpy().tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MainModule(\n",
    "    dim_wideoutput=dim_wideoutput,\n",
    "    num_classes = num_classes,\n",
    "    device = device,\n",
    "    ds_inducing = ds_inducing,\n",
    "    dl_recurring = dl_inducing,\n",
    "    dl_nonrecurring = dl_train,\n",
    "    dl_test = dl_test,\n",
    "    batchsize = batchsize\n",
    "  )\n",
    "\n",
    "model.to(device)\n",
    "print(\"Model was created in {}.\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.n_subsampleminibatch = 2\n",
    "gpexmodule = gpex.GPEXModule(\n",
    "    module_rawmodule = model,\n",
    "    size_recurringdataset = len(ds_inducing),\n",
    "    device = device,\n",
    "    func_mainmodule_to_moduletobecomeGP = model.func_mainmodule_to_moduletobecomeGP, \n",
    "    func_feed_noise_minibatch = model.func_feed_noise_minibatch,\n",
    "    func_feed_recurring_minibatch = model.func_feed_recurring_minibatch,\n",
    "    func_feed_nonrecurring_minibatch = model.func_feed_nonrecurring_minibatch,\n",
    "    func_feed_test_minibatch = model.func_feed_test_minibatch,\n",
    "    func_get_indices_lastrecurringinstances = model.func_get_indices_lastrecurringinstances,\n",
    "    func_get_modulef1 = model.func_get_modulef1\n",
    ")\n",
    "gpexmodule.sigma2_GP = 1.0 #TODO:check\n",
    "gpexmodule.to(device)\n",
    "print(\"gpmodel was created on {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model from checkpoint ====\n",
    "gpmodel.load_state_dict(\n",
    "    torch.load(\n",
    "        fname_gpmodel\n",
    "     ),\n",
    "    strict = True\n",
    ")\n",
    "gpmodel.train()\n",
    "gpmodel.to(device)\n",
    "gpmodel.renew_precomputed_XTX()\n",
    "print(\"gpmodel was loaded from checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_gpmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_kernel(model_input, ds_input, input_device, idx_begin, idx_end, tbegin=0.0, tend=1.0):\n",
    "    model_input.eval()\n",
    "    t_stepsize = 0.01\n",
    "    x_begin, _, _ = ds_input[idx_begin]\n",
    "    x_end, _, _ = ds_input[idx_end]\n",
    "    interval_toinspect = [\n",
    "        x_begin+((tend-tbegin)*t+tbegin)*(x_end-x_begin)\\\n",
    "        for t in np.arange(0.0, 1.0, t_stepsize)\n",
    "    ]\n",
    "    #compute the t's for x_begin and x_end ====\n",
    "    alpha, beta = tbegin, tend\n",
    "    t1 = (-alpha)/(beta-alpha)\n",
    "    t2 = (1.0-alpha)/(beta-alpha)\n",
    "    interval_t = [t for t in np.arange(0.0, 1.0, t_stepsize)]\n",
    "    idx_beforet1 = np.where(np.array(interval_t)>t1)[0][0]-1\n",
    "    idx_t1 = idx_beforet1 +\\\n",
    "            (interval_t[idx_beforet1+1]-t1)/(interval_t[1]-interval_t[0])\n",
    "    idx_beforet2 = np.where(np.array(interval_t)>t2)[0][0]-1\n",
    "    idx_t2 = idx_beforet2 +\\\n",
    "            (interval_t[idx_beforet2+1]-t2)/(interval_t[1]-interval_t[0])\n",
    "    \n",
    "    output_gp, output_nn = [], []\n",
    "    output_uncertainty = []\n",
    "    for idx_x, x in enumerate(interval_toinspect):\n",
    "        print(\" instance {} from {}\".format(idx_x, len(interval_toinspect)), end='\\r')\n",
    "        #feed to GP ======\n",
    "        output, uncertainty, output_similarities = \\\n",
    "                model_input.testingtime_forward(x.unsqueeze(0).to(input_device), 0, 0)\n",
    "        output = output[0]\n",
    "        output_gp.append(output.squeeze().detach().cpu().numpy())\n",
    "        output_uncertainty.append(uncertainty.flatten())\n",
    "        #feed to NN ===\n",
    "        netout, _, _ = model_input.module_rawmodule(x.unsqueeze(0).to(input_device), 0, 0)\n",
    "        output_nn.append(netout.squeeze().detach().cpu().numpy())\n",
    "    print(\"\\n\")\n",
    "    output_gp = np.array(output_gp)\n",
    "    output_nn = np.array(output_nn)\n",
    "    output_uncertainty = np.array(output_uncertainty)\n",
    "    model_input.train()\n",
    "    return output_gp, output_nn, output_uncertainty, idx_t1, idx_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpmodel.module_f1.module.set_rng_outputheads(rng_outputhead = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gpmodel.module_f1.module._rng_outputheads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_gp, output_nn, output_uncertainty, t1, t2 = inspect_kernel(\n",
    "        model_input = gpmodel,\n",
    "        ds_input = ds_test,\n",
    "        input_device = device,\n",
    "        idx_begin = 10,\n",
    "        idx_end = 1100,\n",
    "        tbegin = -5.0,\n",
    "        tend = 5.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_inspectkernel(output_gp, output_nn, output_uncertainty, t1, t2):\n",
    "    if(int_exposedclass is None):\n",
    "        num_classes = output_gp.shape[1]\n",
    "        for c in range(num_classes):\n",
    "            fig, ax1 = plt.subplots()\n",
    "            ln1 = ax1.plot(range(output_gp.shape[0]), output_gp[:,c], label=\"GP-mean\", c='r')\n",
    "            ln2 = ax1.plot(range(output_gp.shape[0]), output_nn[:,c], label=\"NN-mean\", c='b')\n",
    "\n",
    "            ax2 = ax1.twinx()\n",
    "            ln3 = ax2.plot(range(output_gp.shape[0]), 1.0/output_uncertainty[:,c],\\\n",
    "                           label=\"1.0/GP-uncertainty\", c='g')\n",
    "            ax2.axvline(x=t1, color='k', linestyle='--')\n",
    "            ax2.axvline(x=t2, color='k', linestyle='--')\n",
    "            lns = ln1+ln2+ln3\n",
    "            plt.legend(lns, [u.get_label() for u in lns], loc=0)\n",
    "            plt.title(\"output {}\".format(c))\n",
    "            plt.show()\n",
    "    else:\n",
    "        #the output index is not None ===\n",
    "        print(output_nn.shape)\n",
    "        fig, ax1 = plt.subplots()\n",
    "        ln1 = ax1.plot(range(output_gp.shape[0]), output_gp, label=\"GP-mean\", c='r')\n",
    "        ln2 = ax1.plot(range(output_gp.shape[0]), output_nn, label=\"NN-mean\", c='b')\n",
    "        ax2 = ax1.twinx()\n",
    "        ln3 = ax2.plot(range(output_gp.shape[0]), 1.0/output_uncertainty,\\\n",
    "                       label=\"1.0/GP-uncertainty\", c='g')\n",
    "        ax2.axvline(x=t1, color='k', linestyle='--')\n",
    "        ax2.axvline(x=t2, color='k', linestyle='--')\n",
    "        lns = ln1+ln2+ln3\n",
    "        plt.legend(lns, [u.get_label() for u in lns], loc=0)\n",
    "        plt.title(\"output {}\".format(int_exposedclass))\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(range(output_gp.shape[0]), output_gp, label=\"GP-mean\", c='r')\n",
    "        plt.axvline(x=t1, color='k', linestyle='--')\n",
    "        plt.axvline(x=t2, color='k', linestyle='--')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_inspectkernel(output_gp, output_nn, output_uncertainty, t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_input, ds_input, input_device):\n",
    "    model_input.eval()\n",
    "    with torch.no_grad():\n",
    "        toret = []\n",
    "        list_gty = []\n",
    "        for n in range(len(ds_input)):\n",
    "            if(True):#try:\n",
    "                print(\" instance {} from {}\".format(n, len(ds_input)), end='\\r')\n",
    "                x, y, _ = ds_input[n]\n",
    "                output, _, _ = model_input.testingtime_forward(\n",
    "                                  x.unsqueeze(0).to(input_device), y, n\n",
    "                               )\n",
    "                #TODO:check output = output.clamp(min=-clampval_netout, max=clampval_netout)\n",
    "                toret.append(output[0].detach().cpu().numpy())\n",
    "                list_gty.append(y)\n",
    "            #except:\n",
    "            #    print(\"An exception occured for instnace {}\".format(n))\n",
    "        print(\"\\n\")\n",
    "        toret = np.array(toret)\n",
    "        toret = toret[:,0,:]\n",
    "    model_input.train()\n",
    "    return toret, list_gty\n",
    "\n",
    "def evaluate_g(model_input, ds_input, input_device):\n",
    "    model_input.eval()\n",
    "    with torch.no_grad():\n",
    "        toret = []\n",
    "        list_gty = []\n",
    "        for n in range(len(ds_input)):\n",
    "            print(\" instance {} from {}\".format(n, len(ds_input)), end='\\r')\n",
    "            x, y, n = ds_input[n]\n",
    "            output, _, _ = model_input(x.unsqueeze(0).to(input_device), y, n)\n",
    "            toret.append(output[0,:].detach().cpu().numpy())\n",
    "            list_gty.append(y)\n",
    "        print(\"\\n\")\n",
    "        toret = np.array(toret) #[N x 10]\n",
    "        #toret = toret[:,0,:]\n",
    "    model_input.train()\n",
    "    return toret, list_gty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate g(.) i.e. the bypass network ======\n",
    "#model.to(device)\n",
    "predy, list_gty = evaluate_g(gpmodel.module_rawmodule, ds_test, device)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np_confmatrix = confusion_matrix(np.argmax(predy,1), list_gty)\n",
    "print(np_confmatrix)\n",
    "print(\"\\n\\n\\n\")\n",
    "print(\"accuracy of g(.) = {}\".format(np.sum(np_confmatrix*np.eye(9))/np.sum(np_confmatrix) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the GP path ===\n",
    "gpmodel.renew_precomputed_XTX()\n",
    "predy, list_gty = evaluate_model(gpmodel, ds_test, device)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "np_confmatrix = confusion_matrix(np.argmax(predy[:,:,0,0],1), list_gty)\n",
    "print(np_confmatrix)\n",
    "print(\"\\n\\n\\n\") \n",
    "print(\"accuracy = {}\".format(np.sum(np_confmatrix*np.eye(9))/np.sum(np_confmatrix) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check if g(.) and GP path match on test instances ====\n",
    "FLAG_RELU = False\n",
    "gpmodel.renew_precomputed_XTX()\n",
    "list_outputgpoutputg = gpmodel.checkequal_f1path_gpath_ontest(10)\n",
    "for n in range(len(list_outputgpoutputg)):\n",
    "    a = list_outputgpoutputg[n][0]\n",
    "    b = list_outputgpoutputg[n][1]\n",
    "    if(FLAG_RELU == True):\n",
    "        a = np_relu(a); b = np_relu(b)\n",
    "    \n",
    "    min_ab = min(np.min(a), np.min(b))\n",
    "    max_ab = max(np.max(a), np.max(b))\n",
    "    print(\"a-b in range [{} , {}]\".format(np.min(a-b), np.max(a-b)))\n",
    "    #print(np.round(a[0:5, 0:5], 2))\n",
    "    #print(np.round(b[0:5, 0:5], 2))\n",
    "    \n",
    "    #compute the class activations ====\n",
    "    gpmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            classactivation_a = gpmodel.module_rawmodule.linear(torch.tensor(a).float().to(device))\n",
    "            classactivation_b = gpmodel.module_rawmodule.linear(torch.tensor(b).float().to(device))\n",
    "            classactivation_a = classactivation_a.detach().cpu().numpy()\n",
    "            classactivation_b = classactivation_b.detach().cpu().numpy()\n",
    "            onehot_a = np.zeros((batchsize, 10))\n",
    "            onehot_a[list(range(batchsize)), np.argmax(classactivation_a, 1).tolist()] = 1\n",
    "            onehot_b = np.zeros((batchsize, 10))\n",
    "            onehot_b[list(range(batchsize)), np.argmax(classactivation_b, 1).tolist()] = 1\n",
    "\n",
    "            min_classactivations = min(np.min(classactivation_a), np.min(classactivation_b))\n",
    "            max_classactivations = max(np.max(classactivation_a), np.max(classactivation_b))\n",
    "        except:\n",
    "            onehot_a = np.zeros((batchsize, 10))\n",
    "            onehot_a[list(range(batchsize)), np.argmax(a, 1).tolist()] = 1\n",
    "            onehot_b = np.zeros((batchsize, 10))\n",
    "            onehot_b[list(range(batchsize)), np.argmax(b, 1).tolist()] = 1\n",
    "    gpmodel.train()\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(1,6,1)\n",
    "    plt.imshow(np.round(a, 2), vmin=min_ab, vmax=max_ab, aspect=\"auto\"); plt.colorbar()\n",
    "    plt.subplot(1,6,2)\n",
    "    plt.imshow(np.round(b, 2), vmin=min_ab, vmax=max_ab, aspect=\"auto\"); plt.colorbar()\n",
    "    plt.subplot(1,6,3)\n",
    "    plt.imshow(np.round(a-b, 2), cmap=\"seismic\", vmin=-np.max(np.abs(a-b)),\\\n",
    "               vmax=np.max(np.abs(a-b)), aspect=\"auto\"); plt.colorbar()\n",
    "    plt.subplot(1,6,4)\n",
    "    plt.imshow(onehot_a, vmin=0,\n",
    "               vmax=1, aspect=\"auto\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1,6,5)\n",
    "    plt.imshow(onehot_b, vmin=0,\n",
    "               vmax=1, aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.subplot(1,6,6)\n",
    "    plt.imshow(np.abs(onehot_b - onehot_a), aspect=\"auto\")\n",
    "    plt.colorbar()\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToPublish Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if g(.) and GP path match on test instances ====\n",
    "FLAG_RELU = False\n",
    "gpmodel.renew_precomputed_XTX()\n",
    "list_outputgpoutputg = gpmodel.checkequal_f1path_gpath_ontest(10)\n",
    "for n in range(len(list_outputgpoutputg)):\n",
    "    a = list_outputgpoutputg[n][0]\n",
    "    b = list_outputgpoutputg[n][1]\n",
    "    if(FLAG_RELU == True):\n",
    "        a = np_relu(a); b = np_relu(b)\n",
    "    \n",
    "    min_ab = min(np.min(a), np.min(b))\n",
    "    max_ab = max(np.max(a), np.max(b))\n",
    "    print(\"a-b in range [{} , {}]\".format(np.min(a-b), np.max(a-b)))\n",
    "    #print(np.round(a[0:5, 0:5], 2))\n",
    "    #print(np.round(b[0:5, 0:5], 2))\n",
    "    \n",
    "    #compute the class activations ====\n",
    "    gpmodel.eval()\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            classactivation_a = gpmodel.module_rawmodule.linear(torch.tensor(a).float().to(device))\n",
    "            classactivation_b = gpmodel.module_rawmodule.linear(torch.tensor(b).float().to(device))\n",
    "            classactivation_a = classactivation_a.detach().cpu().numpy()\n",
    "            classactivation_b = classactivation_b.detach().cpu().numpy()\n",
    "            onehot_a = np.zeros((batchsize, 10))\n",
    "            onehot_a[list(range(batchsize)), np.argmax(classactivation_a, 1).tolist()] = 1\n",
    "            onehot_b = np.zeros((batchsize, 10))\n",
    "            onehot_b[list(range(batchsize)), np.argmax(classactivation_b, 1).tolist()] = 1\n",
    "\n",
    "            min_classactivations = min(np.min(classactivation_a), np.min(classactivation_b))\n",
    "            max_classactivations = max(np.max(classactivation_a), np.max(classactivation_b))\n",
    "        except:\n",
    "            onehot_a = np.zeros((batchsize, 10))\n",
    "            onehot_a[list(range(batchsize)), np.argmax(a, 1).tolist()] = 1\n",
    "            onehot_b = np.zeros((batchsize, 10))\n",
    "            onehot_b[list(range(batchsize)), np.argmax(b, 1).tolist()] = 1\n",
    "    gpmodel.train()\n",
    "    \n",
    "    \n",
    "    #spcifcy the disagreements rows ======\n",
    "    onehot_a = np.argmax(a, 1)\n",
    "    onehot_b = np.argmax(b, 1)\n",
    "    idx_disagreement = np.where(np.array(onehot_a) != np.array(onehot_b))[0].flatten().tolist()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(np.round(a, 2), vmin=min_ab, vmax=max_ab, aspect=\"auto\"); plt.colorbar()\n",
    "    for row in idx_disagreement:\n",
    "        rect = patches.Rectangle(\n",
    "                (0-0.2, row-0.5), 10-0.5, 1,\n",
    "                linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "        print(\"<><><><><><><><> {}\".format(idx_disagreement))\n",
    "    \n",
    "    plt.title(\"GPs output \\n (batchsize x D)\", font = 'Formata', fontsize = 22)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(np.round(b, 2), vmin=min_ab, vmax=max_ab, aspect=\"auto\"); plt.colorbar()\n",
    "    for row in idx_disagreement:\n",
    "        rect = patches.Rectangle(\n",
    "                (0-0.2, row-0.5), 10-0.5, 1,\n",
    "                linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "        print(\"<><><><><><><><> {}\".format(idx_disagreement))\n",
    "    plt.title(\"ANN output \\n (batchsize x D)\", font = 'Formata', fontsize = 22)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(\n",
    "        \"InterpGP/ToPublish/Tables/Cifar10/{}.png\".format(time.time()),\n",
    "        dpi=100, bbox_inches='tight', pad_inches=0, Q=100\n",
    "    )\n",
    "    #plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_a = np.argmax(a, 1)\n",
    "onehot_b = np.argmax(b, 1)\n",
    "idx_disagreement = np.where(np.array(onehot_a) != np.array(onehot_b))\n",
    "print(onehot_a)\n",
    "print(onehot_b)\n",
    "print(idx_disagreement[0].flatten().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute GP diff ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import projutils.evaluation\n",
    "from projutils.evaluation import GPdiffANN_ontorchdl\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy import stats\n",
    "\n",
    "output, dict_gt = GPdiffANN_ontorchdl(\n",
    "    gpmodel = gpmodel,\n",
    "    dl_input = dl_test,\n",
    "    input_device = device\n",
    ")\n",
    "\n",
    "np_gpann = np.array(\n",
    "    [output[k] for k in output.keys()]\n",
    ")\n",
    "np_gt = np.array([dict_gt[k] for k in output.keys()])\n",
    "\n",
    "numclasses = int(np_gpann.shape[1]/2)\n",
    "list_correlation = []\n",
    "for c in range(numclasses):\n",
    "    list_correlation.append(\n",
    "        stats.pearsonr(np_gpann[:, c], np_gpann[:, c+numclasses])[0]\n",
    "    )\n",
    "print(\"list correl = {}\".format(list_correlation))\n",
    "gp_labels = np.argmax(np_gpann[:, 0:numclasses], 1)\n",
    "ann_labels = np.argmax(np_gpann[:, numclasses::], 1)\n",
    "kappa_labels = cohen_kappa_score(gp_labels, ann_labels)\n",
    "disaggreement = np.sum(gp_labels != ann_labels)\n",
    "print(\"kappa between labels = {}\".format(kappa_labels))\n",
    "print(\"number of disagreements: {} out of {}\".format(disaggreement, np_gpann.shape[0]))\n",
    "acc_ann = np.sum(ann_labels == np_gt)/np_gpann.shape[0]\n",
    "acc_gp = np.sum(gp_labels == np_gt)/np_gpann.shape[0]\n",
    "print(\"Acuuray of ANN {}\".format(acc_ann))\n",
    "print(\"   Acuuracy of GP {}\".format(acc_gp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depict the Correlation Scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "np_gp = np_gpann[:,0:num_classes]\n",
    "np_ann = np_gpann[:,num_classes::]\n",
    "np_gp_softmax = softmax(np_gp, 1)\n",
    "np_ann_softmax = softmax(np_ann, 1)\n",
    "\n",
    "list_disaggrement = (np.argmax(np_gp, 1) != np.argmax(np_ann, 1)).tolist()\n",
    "list_c = [0,0,0,0.2] #[[1,0,0,1] if(u==True) else [0,0,0,0.05] for u in list_disaggrement]\n",
    "count_plotted = 0\n",
    "for c in range(num_classes):\n",
    "    plt.ioff()\n",
    "    plt.figure()\n",
    "    plt.scatter(np_gp[:,c], np_ann[:,c], c=np.array(list_c), marker='o', facecolors='none')\n",
    "    plt.axis(\"equal\")\n",
    "    plt.xlabel(\"GP output (head {})\".format(c+1), fontsize=22, font = 'Formata')\n",
    "    plt.ylabel(\"ANN output (head {})\".format(c+1), fontsize=22, font = 'Formata')\n",
    "    plt.savefig(\n",
    "        \"InterpGP/Correlation_Scatters/Classifier/{}.png\".format(count_plotted),\n",
    "        dpi=100, bbox_inches='tight', pad_inches=0, Q=100\n",
    "    )\n",
    "    count_plotted += 1\n",
    "    plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_kernel(model_input, ds_input, input_device):\n",
    "    model_input.eval()\n",
    "    toret = []\n",
    "    list_gty = []\n",
    "    list_uncertainty = []\n",
    "    list_similarities = []\n",
    "    list_x, list_y = [], []\n",
    "    list_output_g = []\n",
    "    for n in range(len(ds_input)):\n",
    "        print(\" instance {} from {}\".format(n, len(ds_input)), end='\\r')\n",
    "        x, y, _ = ds_input[n]\n",
    "        output, uncertainty, output_similarities = \\\n",
    "                model_input.testingtime_forward(x.unsqueeze(0).to(input_device), y, n)\n",
    "        #print(\"output_similaritites.shape = {}\".format(output_similarities.shape))\n",
    "        output = output[0]\n",
    "        toret.append(output.detach().cpu().numpy())\n",
    "        list_gty.append(y)\n",
    "        list_uncertainty.append(uncertainty)\n",
    "        list_similarities.append(output_similarities.detach().cpu().numpy())\n",
    "        list_x.append(x); list_y.append(y)\n",
    "        #feed the model to g(.) ====\n",
    "        output_g, _, _ = \\\n",
    "                model_input.module_rawmodule(x.unsqueeze(0).to(input_device), y, n)\n",
    "        list_output_g.append(output_g.detach().cpu().numpy().flatten().tolist())\n",
    "\n",
    "    print(\"\\n\")\n",
    "    toret = np.array(toret)\n",
    "    toret = toret[:,0,:]\n",
    "    output_g = np.array(list_output_g)\n",
    "    print(output_g.shape)\n",
    "    model_input.train()\n",
    "    return toret, list_gty, list_uncertainty, list_similarities, list_x, list_y, output_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_retval_inspectmodel = [inspect_kernel(gpmodel, ds_test, device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_recurring.label_names = ds_recurring.dict_labelnames[b'label_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the two instances for the introduction of the paper ====\n",
    "#%%capture\n",
    "m = 10\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Computer Modern Roman\"],\n",
    "})\n",
    "matplotlib.rcParams['text.latex.preamble']=[r\"\\usepackage{amsmath}\"]\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "plt.figure(figsize=((m+2)*10, 2*10))\n",
    "count_subplot = 1\n",
    "for n in [2619, 48]:\n",
    "    print(\"n = {} ==========================\".format(n))\n",
    "#     fname_n, _ = ds_test._ntoimage(n)\n",
    "#     fname_n = os.path.relpath(fname_n, ds_rootdir)\n",
    "#     if(fname_n not in ds_split[\"fname_hoskys\"]):\n",
    "#         continue\n",
    "    #fields common between all models\n",
    "    _, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "    x, y = list_x[n], list_y[n]\n",
    "    \n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    str_subfolder = \"\"\n",
    "    idx_model = 0 #for idx_model in range(len(list_retval_inspectmodel)):\n",
    "    list_predy, list_gty, list_uncertainty, list_similarities, _, _, output_g = \\\n",
    "                        list_retval_inspectmodel[idx_model]\n",
    "    np_argmax_list_predyn = np.argmax(output_g[n])\n",
    "    if(np_argmax_list_predyn == list_gty[n]):\n",
    "        str_subfolder = str_subfolder + \"True\"\n",
    "    else:\n",
    "        str_subfolder = str_subfolder + \"False\"\n",
    "    kn = list_similarities[n][np_argmax_list_predyn, :].flatten()\n",
    "    idx_similars = np.argsort(-kn).tolist()[0:m]\n",
    "    plt.ioff()\n",
    "    plt.subplot(2, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "    plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "    print(\"{} =======> shown on {}\".format(n, count_subplot-1))\n",
    "    plt.axis('off')\n",
    "    print(\"===== showed.\")\n",
    "    if(n == 2619):\n",
    "        plt.title(\n",
    "                r'$\\boldsymbol{x}_{test}$',\\\n",
    "                fontsize=150\n",
    "        )\n",
    "\n",
    "    list_relevantinstances =[]\n",
    "    for count_similars in range(len(idx_similars)):\n",
    "        plt.subplot(2, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        print(\"    n, m  = {}, {} shown on {}\".format(n, count_similars, count_subplot-1))\n",
    "        plt.imshow(\n",
    "          tfm_denormalize(\n",
    "                  ds_recurring[idx_similars[count_similars]][0]\n",
    "              ).cpu().numpy().transpose(1,2,0),\n",
    "        )\n",
    "        plt.axis('off')\n",
    "        if(n == 2619):\n",
    "            plt.title(r\"$\\boldsymbol{x}_{i\"+str(count_similars+1)+\"}$\", fontsize=150)\n",
    "        list_relevantinstances.append(idx_similars[count_similars])\n",
    "            \n",
    "        \n",
    "                \n",
    "        \n",
    "if(True):#os.path.isfile(\"InterpGP/{}/{}.png\".format(str_subfolder, n)) == False):\n",
    "    plt.savefig(\n",
    "            \"InterpGP/ToPublish/twohorses.jpg\",\n",
    "            dpi=50, bbox_inches='tight', pad_inches=0, Q=100\n",
    "        )\n",
    "    print(\"================= saved.\")\n",
    "plt.close()\n",
    "#assert False\n",
    "#enable_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_subplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "m = 10\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "for n in range(len(ds_test)):\n",
    "    print(\"n = {} ==========================\".format(n))\n",
    "#     fname_n, _ = ds_test._ntoimage(n)\n",
    "#     fname_n = os.path.relpath(fname_n, ds_rootdir)\n",
    "#     if(fname_n not in ds_split[\"fname_hoskys\"]):\n",
    "#         continue\n",
    "    #fields common between all models\n",
    "    _, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "    x, y = list_x[n], list_y[n]\n",
    "    \n",
    "    plt.figure(figsize=((m+2)*10, 1*10))\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    count_subplot = 1\n",
    "    str_subfolder = \"\"\n",
    "    idx_model = 0 #for idx_model in range(len(list_retval_inspectmodel)):\n",
    "    list_predy, list_gty, list_uncertainty, list_similarities, _, _, output_g = \\\n",
    "                        list_retval_inspectmodel[idx_model]\n",
    "    np_argmax_list_predyn = np.argmax(output_g[n])\n",
    "    if(np_argmax_list_predyn == list_gty[n]):\n",
    "        str_subfolder = str_subfolder + \"True\"\n",
    "    else:\n",
    "        str_subfolder = str_subfolder + \"False\"\n",
    "    kn = list_similarities[n][np_argmax_list_predyn, :].flatten()\n",
    "    idx_similars = np.argsort(-kn).tolist()[0:m]\n",
    "    plt.ioff()\n",
    "    plt.subplot(1, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "    plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "    plt.axis('off')\n",
    "    print(\"===== showed.\")\n",
    "    plt.title(\"gt-label = {}\\n predicted = {}\\n instance {}\".format(\n",
    "                            ds_recurring.label_names[y],\n",
    "                            ds_recurring.label_names[np_argmax_list_predyn],\n",
    "                            n\n",
    "                ), fontsize=100\n",
    "             )\n",
    "\n",
    "    list_relevantinstances =[]\n",
    "    for count_similars in range(len(idx_similars)):\n",
    "        plt.subplot(1, 1*(m+2), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(\n",
    "          tfm_denormalize(\n",
    "                  ds_recurring[idx_similars[count_similars]][0]\n",
    "              ).cpu().numpy().transpose(1,2,0),\n",
    "        )\n",
    "        plt.axis('off')\n",
    "        plt.title(\"instnace {}\".format(idx_similars[count_similars]), fontsize=100)\n",
    "        list_relevantinstances.append(idx_similars[count_similars])\n",
    "            \n",
    "        \n",
    "                \n",
    "        \n",
    "    if(True):#os.path.isfile(\"InterpGP/{}/{}.png\".format(str_subfolder, n)) == False):\n",
    "        plt.savefig(\n",
    "                \"InterpGP/{}/{}/{}.jpg\".format(str_subfolder, y, n),\n",
    "                dpi=50, bbox_inches='tight', pad_inches=0, Q=80\n",
    "            )\n",
    "        print(\"================= saved.\")\n",
    "    plt.close()\n",
    "    #assert False\n",
    "#enable_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_recurring.label_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToPublish Nearet Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_pilcompatible(x):\n",
    "    x = x - np.min(x)\n",
    "    x = x /np.max(x)\n",
    "    toret = (x*255.0).astype('uint8')\n",
    "    return toret "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_airplane = [\n",
    "    3, 1078, 1596, 1726, 269, 27,\\\n",
    "    10, 406, 1744, 2792,\\\n",
    "    179, 180, 499, 791, 928, 1241,\\\n",
    "    1365, 2079,\\\n",
    "    352, 258, 281, 886, 2548,\\\n",
    "    189, 289, 1203, 1440, 1579, 2375, 2468,\\\n",
    "    2877, 2964,\\\n",
    "    257, 276, 602\n",
    "]\n",
    "selected_automobile = [\n",
    "    81, 114, 246,\\\n",
    "    9, 240, 645,\\\n",
    "    887, 865, 796, 1144\n",
    "]\n",
    "selected_bird = [\n",
    "    303, 86, 623, 1196, 1392, 1481,\\\n",
    "    391, 592, 603, 697, 843, 1382,\\\n",
    "    498, 123, 430\n",
    "]\n",
    "selected_cat = [\n",
    "    8, 432, 786, 950,\\\n",
    "    825, 1373, 1364\n",
    "]\n",
    "selected_deer = [\n",
    "    40, 373, 1315,\\\n",
    "    188, 802, 872, 1484,\\\n",
    "    94, 328, 465, 1207\n",
    "]\n",
    "selected_dog = [\n",
    "    502, 510, 549, 1329, 1337, 1438, 1598, 1694\n",
    "]\n",
    "selected_frog = [\n",
    "    9903, 9902, 9615, 9546, 9527, 9456, 9450,\\\n",
    "    9425, 9342, 8940\n",
    "]\n",
    "selected_horse = [\n",
    "    17, 99, 2619, 2470, 2297, 1615,\\\n",
    "    2626, 48, 2340,\\\n",
    "    203, 119, 2167, 1584,\\\n",
    "    57, 1602\n",
    "]\n",
    "selected_ship = [\n",
    "    2, 108, 233, 242,\\\n",
    "    79, 80, 185,\\\n",
    "    368, 403, 694\n",
    "]\n",
    "selected_truck = [\n",
    "    611, 940, 11, 14, 530, 585, 938, 213, 247, 538,\\\n",
    "    816, 38, 89, 259, 1104, 1707,\\\n",
    "    508, 517,\\\n",
    "    838, 922\n",
    "]\n",
    "selected_misclassified = [\n",
    "    287, 4355, 1969, 7892, 1858, 158, 1506, 1580, 3446, 8898, 6419\n",
    "]\n",
    "selected_all = selected_airplane +\\\n",
    "            selected_automobile + selected_bird +\\\n",
    "            selected_cat + selected_deer + selected_dog +\\\n",
    "            selected_frog + selected_horse + selected_ship+\\\n",
    "            selected_truck + selected_misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(selected_all))\n",
    "print(len(list(set(selected_all))))\n",
    "for u in selected_all:\n",
    "    if(selected_all.count(u) > 1):\n",
    "        print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_n = selected_all\n",
    "#%%capture\n",
    "m = 10\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "for n in range(len(ds_test)):\n",
    "    if(n not in selected_n):\n",
    "        continue\n",
    "    \n",
    "    path_tosave = \"InterpGP/ToPublish/Cifar10_NearesetNeighbours/\"\n",
    "    if(os.path.isdir(os.path.join(path_tosave, \"{}/\".format(n)))):\n",
    "        assert False\n",
    "    else:\n",
    "        os.mkdir(os.path.join(path_tosave, \"{}/\".format(n)))\n",
    "    path_instance_n = os.path.join(path_tosave, \"{}/\".format(n))\n",
    "    \n",
    "    print(\"n = {} ==========================\".format(n))\n",
    "#     fname_n, _ = ds_test._ntoimage(n)\n",
    "#     fname_n = os.path.relpath(fname_n, ds_rootdir)\n",
    "#     if(fname_n not in ds_split[\"fname_hoskys\"]):\n",
    "#         continue\n",
    "    #fields common between all models\n",
    "    _, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "    x, y = list_x[n], list_y[n]\n",
    "    \n",
    "    #plt.figure(figsize=((m+2)*10, 1*10))\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    count_subplot = 1\n",
    "    str_subfolder = \"\"\n",
    "    idx_model = 0 #for idx_model in range(len(list_retval_inspectmodel)):\n",
    "    list_predy, list_gty, list_uncertainty, list_similarities, _, _, output_g = \\\n",
    "                        list_retval_inspectmodel[idx_model]\n",
    "    np_argmax_list_predyn = np.argmax(output_g[n])\n",
    "    if(np_argmax_list_predyn == list_gty[n]):\n",
    "        str_subfolder = str_subfolder + \"True\"\n",
    "    else:\n",
    "        str_subfolder = str_subfolder + \"False\"\n",
    "    kn = list_similarities[n][np_argmax_list_predyn, :].flatten()\n",
    "    idx_similars = np.argsort(-kn).tolist()[0:m]\n",
    "    #plt.ioff()\n",
    "    #plt.subplot(1, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "    #plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "    pil_tosave = Image.fromarray(\n",
    "                    np_to_pilcompatible(\n",
    "                        tfm_denormalize(x).cpu().numpy().transpose(1,2,0)\n",
    "                    )\n",
    "    )\n",
    "    pil_tosave.save(\n",
    "        os.path.join(\n",
    "            path_instance_n, \"instance.png\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    #plt.axis('off')\n",
    "    print(\"===== showed.\")\n",
    "    \n",
    "\n",
    "    list_relevantinstances =[]\n",
    "    for count_similars in range(len(idx_similars)):\n",
    "#         plt.subplot(1, 1*(m+2), count_subplot); count_subplot+=1;\n",
    "#         plt.imshow(\n",
    "#           tfm_denormalize(\n",
    "#                   ds_recurring[idx_similars[count_similars]][0]\n",
    "#               ).cpu().numpy().transpose(1,2,0),\n",
    "#         )\n",
    "        pil_tosave = Image.fromarray(\n",
    "                    np_to_pilcompatible(\n",
    "                        tfm_denormalize(\n",
    "                              ds_recurring[idx_similars[count_similars]][0]\n",
    "                          ).cpu().numpy().transpose(1,2,0),\n",
    "                    )\n",
    "        )\n",
    "        pil_tosave.save(\n",
    "            os.path.join(\n",
    "                path_instance_n, \"{}.png\".format(count_similars+1)\n",
    "            )\n",
    "        )\n",
    "#         plt.axis('off')\n",
    "#         plt.title(\"instnace {}\".format(idx_similars[count_similars]), fontsize=100)\n",
    "        list_relevantinstances.append(idx_similars[count_similars])\n",
    "            \n",
    "        \n",
    "                \n",
    "        \n",
    "#     if(True):#os.path.isfile(\"InterpGP/{}/{}.png\".format(str_subfolder, n)) == False):\n",
    "#         plt.savefig(\n",
    "#                 \"InterpGP/{}/{}/{}.jpg\".format(str_subfolder, y, n),\n",
    "#                 dpi=50, bbox_inches='tight', pad_inches=0, Q=80\n",
    "#             )\n",
    "#         print(\"================= saved.\")\n",
    "#     plt.close()\n",
    "#     #assert False\n",
    "# #enable_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain Similarities CAM-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchofgp\n",
    "import torchofgp.kernel_explainers\n",
    "from torchofgp.kernel_explainers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_retval_inspectmodel = [inspect_kernel(gpmodel, ds_test, device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#explain the similarity itself =====\n",
    "m = 10\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "for n in range(len(ds_test)):\n",
    "    \n",
    "    #fields common between all models\n",
    "    _, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "    x, y = list_x[n], list_y[n]\n",
    "    \n",
    "    plt.figure(figsize=((m+1)*10, 3*10))\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    count_subplot = 1\n",
    "    str_subfolder = \"\"\n",
    "    for idx_model in range(len(list_retval_inspectmodel)):\n",
    "        list_predy, list_gty, list_uncertainty, list_similarities, _, _, output_g = \\\n",
    "                            list_retval_inspectmodel[idx_model]\n",
    "        np_argmax_list_predyn = np.argmax(output_g[n])\n",
    "        if(np_argmax_list_predyn == list_gty[n]):\n",
    "            str_subfolder = str_subfolder + \"True\"\n",
    "        else:\n",
    "            str_subfolder = str_subfolder + \"False\"\n",
    "        kn = list_similarities[n][np_argmax_list_predyn, :].flatten()\n",
    "        idx_similars = np.argsort(-kn).tolist()[0:m]\n",
    "        plt.ioff()\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        plt.title(\"gt-label = {}\\n predicted = {}\\n instance {}\".format(\n",
    "                                ds_recurring.label_names[y],\n",
    "                                ds_recurring.label_names[np_argmax_list_predyn],\n",
    "                                n\n",
    "                    ), fontsize=100\n",
    "                 )\n",
    "        plt.axis('off')\n",
    "        \n",
    "        list_relevantinstances =[]\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(\n",
    "              tfm_denormalize(ds_recurring[idx_similars[count_similars]][0]).cpu().numpy().transpose(1,2,0),\n",
    "            )\n",
    "            plt.axis('off')\n",
    "            plt.title(\"instnace {}\".format(idx_similars[count_similars]), fontsize=100)\n",
    "            list_relevantinstances.append(idx_similars[count_similars])\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        #plot rows 2 (explanations for x2)\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            x2 = ds_recurring[idx_similars[count_similars]][0].unsqueeze(0).to(device)\n",
    "            x1 = x.unsqueeze(0).to(device)\n",
    "            explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "              gpmodel = gpmodel,\n",
    "              func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "              x1 = x.to(device),\n",
    "              x2 = ds_recurring[idx_similars[count_similars]][0].to(device),\n",
    "              idx_ann_outputhead = y,\n",
    "              du_per_gp = du_per_class,\n",
    "              scale_resizemaps = 1.0\n",
    "            )\n",
    "            \n",
    "            toret1 = np.sum(np.sum(explanationx1x2, axis=-1), axis=-1)\n",
    "            toret2 = np.sum(np.sum(explanationx1x2, axis=0), axis=0)\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(toret2, cmap=\"seismic\", vmin=np.min(toret2), vmax=np.max(toret2))\n",
    "            plt.axis('off')\n",
    "        #plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        #plt.hist(kn.flatten(), bins=200)\n",
    "        \n",
    "        #plot rows 3 (explanations for the instance itself)\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        plt.axis('off')\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            x2 = ds_recurring[idx_similars[count_similars]][0].unsqueeze(0).to(device)\n",
    "            x1 = x.unsqueeze(0).to(device)\n",
    "            explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "              gpmodel = gpmodel,\n",
    "              func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "              x1 = x.to(device),\n",
    "              x2 = ds_recurring[idx_similars[count_similars]][0].to(device),\n",
    "              idx_ann_outputhead = y,\n",
    "              du_per_gp = du_per_class,\n",
    "              scale_resizemaps = 1.0\n",
    "            )\n",
    "            \n",
    "            toret1 = np.sum(np.sum(explanationx1x2, axis=-1), axis=-1)\n",
    "            toret2 = np.sum(np.sum(explanationx1x2, axis=0), axis=0)\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(toret1, cmap=\"seismic\", vmin=np.min(toret1), vmax=np.max(toret1))\n",
    "                \n",
    "        \n",
    "    if(os.path.isfile(\"InterpGP/Kernel/{}/{}.jpg\".format(str_subfolder, n)) == False):\n",
    "        plt.savefig(\n",
    "                \"InterpGP/Kernel/{}/{}.jpg\".format(str_subfolder, n),\n",
    "                dpi=20, bbox_inches='tight', pad_inches=0, Q=80\n",
    "            )\n",
    "    plt.close()\n",
    "    #assert False\n",
    "#enable_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain by Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchofgp\n",
    "import torchofgp.kernel_explainers\n",
    "from torchofgp.kernel_explainers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_retval_inspectmodel = [inspect_kernel(gpmodel, ds_test, device)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_retvals = open('InterpGP/retvals.pkl', 'wb')\n",
    "# pickle.dump(list_retval_inspectmodelval_inspectmodel, file_retvals, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_retvals = open('InterpGP/retvals.pkl', 'rb')\n",
    "list_retval_inspectmodel = pickle.load(file_retvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings ===\n",
    "n1 = 21\n",
    "n2 = 9064\n",
    "rgbsigma = 0.0\n",
    "#explain the similarity ===\n",
    "_, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "x1, y1 = list_x[n1], list_y[n1]\n",
    "x2 = ds_recurring[n2][0]\n",
    "explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "      gpmodel = gpmodel,\n",
    "      func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "      x1 = x1.to(device),\n",
    "      x2 = x2.to(device),\n",
    "      idx_ann_outputhead = y1,\n",
    "      du_per_gp = du_per_class,\n",
    "      scale_resizemaps = 1.0,\n",
    "      mode_upsample='nearest',\n",
    "      flag_aligcorners = None\n",
    ")\n",
    "explanationx1x2 = explanationx1x2 * ((explanationx1x2>0.0) + 0.0)\n",
    "print(\"shape of CAM-like output = {}\".format(explanationx1x2.shape))\n",
    "\n",
    "#explanationx1x2 = explanationx1x2/np.max(explanationx1x2) #TODO:check\n",
    "#explanationx1x2[explanationx1x2 < 0.1] = 0.0 #TODO:check\n",
    "\n",
    "#compute the heatmaps for img1 and img2 ===\n",
    "heatmap_1 = np.sum(np.sum(explanationx1x2, 3), 2)\n",
    "heatmap_2 = np.sum(np.sum(explanationx1x2, 0), 0)\n",
    "normalized_heatmap_1 = heatmap_1/np.max(heatmap_1)\n",
    "normalized_heatmap_2 = heatmap_2/np.max(heatmap_2)\n",
    "\n",
    "#assert False\n",
    "#make the pixel similarity matrix ====\n",
    "num_pix1 = explanationx1x2.shape[0] * explanationx1x2.shape[1]\n",
    "num_pix2 = explanationx1x2.shape[2] * explanationx1x2.shape[3]\n",
    "np_pixelmatrix = np.zeros((num_pix1+num_pix2 , num_pix1+num_pix2))\n",
    "\n",
    "#for pixels of image 1\n",
    "for idx_ij in range(num_pix1):\n",
    "    i, j = np.unravel_index(idx_ij, [explanationx1x2.shape[0], explanationx1x2.shape[1]])\n",
    "    #fill the similarities between i,j and image 2\n",
    "    for idx_kl in range(num_pix2):\n",
    "        k, l = np.unravel_index(idx_kl, [explanationx1x2.shape[2], explanationx1x2.shape[3]])\n",
    "        np_pixelmatrix[idx_ij, idx_kl] = explanationx1x2[i,j,k,l]\n",
    "    #fill the similarities between i,j and image 1 \n",
    "    for idx_i2j2 in range(num_pix1):\n",
    "        if(idx_i2j2 != idx_ij):\n",
    "            i2, j2 = np.unravel_index(\n",
    "                    idx_i2j2, [explanationx1x2.shape[0], explanationx1x2.shape[1]]\n",
    "                )\n",
    "            dist = np.array([i,j]) - np.array([i2,j2])\n",
    "            if(rgbsigma != 0.0):\n",
    "                rbf = np.exp(np.sum(-dist*dist) / (2.0*rgbsigma * rgbsigma))\n",
    "            else:\n",
    "                rbf = 0.0\n",
    "            np_pixelmatrix[idx_ij, idx_i2j2] = rbf\n",
    "            \n",
    "#for pixels of image 2\n",
    "for idx_kl in range(num_pix2):\n",
    "    k, l = np.unravel_index(idx_kl, [explanationx1x2.shape[2], explanationx1x2.shape[3]])\n",
    "    #fill the similarities between k,l and image 1\n",
    "    for idx_ij in range(num_pix1):\n",
    "        i, j = np.unravel_index(idx_ij, [explanationx1x2.shape[0], explanationx1x2.shape[1]])\n",
    "        np_pixelmatrix[idx_kl, idx_ij] = explanationx1x2[i,j,k,l]\n",
    "    #fill the similarities between k,l and image 2 \n",
    "    for idx_k2l2 in range(num_pix2):\n",
    "        if(idx_k2l2 != idx_kl):\n",
    "            k2, l2 = np.unravel_index(\n",
    "                    idx_k2l2, [explanationx1x2.shape[2], explanationx1x2.shape[3]]\n",
    "                )\n",
    "            dist = np.array([k,l]) - np.array([k2,l2])\n",
    "            if(rbf != 0.0):\n",
    "                rbf = np.exp(np.sum(-dist*dist) / (2.0*rgbsigma * rgbsigma))\n",
    "            else:\n",
    "                rbf = 0.0\n",
    "            np_pixelmatrix[idx_kl, idx_k2l2] = rbf\n",
    "            \n",
    "print(\"Computed the similarity matrix of shape {}\".format(np_pixelmatrix.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the histogram of two heatmaps ===\n",
    "plt.figure()\n",
    "plt.hist(heatmap_1.flatten(), bins=50)\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.hist(heatmap_2.flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#run spectral clustering on pixel similarity matrix ====\n",
    "#settings ===\n",
    "n_clusters = 2\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "clustering = SpectralClustering(\n",
    "         affinity = \"precomputed\",\n",
    "         n_clusters= n_clusters,\n",
    "         random_state=1\n",
    ").fit(np_pixelmatrix+0.0)\n",
    "\n",
    "#show the clustering result\n",
    "plt.figure(figsize=((clustering.n_clusters+2)*10, 2*10))\n",
    "count_subplot = 1\n",
    "plt.subplot(2, clustering.n_clusters+2, count_subplot); count_subplot +=clustering.n_clusters+2;\n",
    "plt.imshow(tfm_denormalize(x1).cpu().numpy().transpose(1,2,0))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, clustering.n_clusters+2, count_subplot); count_subplot -= (clustering.n_clusters+1);\n",
    "plt.imshow(tfm_denormalize(x2).cpu().numpy().transpose(1,2,0))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "plt.subplot(2, clustering.n_clusters+2, count_subplot); count_subplot +=clustering.n_clusters+2;\n",
    "plt.imshow(heatmap_1, cmap='seismic')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, clustering.n_clusters+2, count_subplot); count_subplot -= (clustering.n_clusters+1);\n",
    "plt.imshow(heatmap_2, cmap='seismic')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "for c in range(clustering.n_clusters):\n",
    "    idx_pixelsinc = np.where(np.array(clustering.labels_) == c)[0].tolist() #in rng pix1+pix2\n",
    "    \n",
    "    \n",
    "    #plot the cluster c for image 1\n",
    "    img1_inc = np.zeros((explanationx1x2.shape[0], explanationx1x2.shape[1]))\n",
    "    plt.subplot(\n",
    "        2, clustering.n_clusters+2,\n",
    "        count_subplot); count_subplot +=clustering.n_clusters+2;\n",
    "    idx_img1_inc = [idx for idx in idx_pixelsinc if(idx<num_pix1)]\n",
    "    if(idx_img1_inc != []):\n",
    "        np_img1_ij = np.array(np.unravel_index(idx_img1_inc, img1_inc.shape))\n",
    "        img1_inc[np_img1_ij[0,:] , np_img1_ij[1,:]] = 1\n",
    "    plt.imshow(img1_inc, vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    #plot the cluster c for image 2\n",
    "    img2_inc = np.zeros((explanationx1x2.shape[2], explanationx1x2.shape[3]))\n",
    "    plt.subplot(\n",
    "        2, clustering.n_clusters+2,\n",
    "        count_subplot); count_subplot -= (clustering.n_clusters+1);\n",
    "    idx_img2_inc = [idx-num_pix1 for idx in idx_pixelsinc if(idx>=num_pix1)]\n",
    "    if(idx_img2_inc != []):\n",
    "        np_img2_ij = np.array(np.unravel_index(idx_img2_inc, img2_inc.shape))\n",
    "        img2_inc[np_img2_ij[0,:] , np_img2_ij[1,:]] = 1\n",
    "    plt.imshow(img2_inc, vmin=0, vmax=1)\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explain the similarity itself =====\n",
    "m = 5\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"once\")\n",
    "for n in range(len(ds_test)):\n",
    "    fname_n, _ = ds_test._ntoimage(n)\n",
    "    fname_n = os.path.relpath(fname_n, ds_rootdir)\n",
    "#     if(fname_n not in ds_split[\"fname_hoskys\"]):\n",
    "#         continue\n",
    "    #fields common between all models\n",
    "    _, list_gty, _, _, list_x, list_y, _ = list_retval_inspectmodel[0]\n",
    "    x, y = list_x[n], list_y[n]\n",
    "    \n",
    "    plt.figure(figsize=((m+1)*10, 3*10))\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    count_subplot = 1\n",
    "    str_subfolder = \"\"\n",
    "    for idx_model in range(len(list_retval_inspectmodel)):\n",
    "        list_predy, list_gty, list_uncertainty, list_similarities, _, _, output_g = \\\n",
    "                            list_retval_inspectmodel[idx_model]\n",
    "        np_argmax_list_predyn = np.argmax(output_g[n])\n",
    "        if(np_argmax_list_predyn == list_gty[n]):\n",
    "            str_subfolder = str_subfolder + \"True\"\n",
    "        else:\n",
    "            str_subfolder = str_subfolder + \"False\"\n",
    "        kn = list_similarities[n][np_argmax_list_predyn, :].flatten()\n",
    "        idx_similars = np.argsort(-kn).tolist()[0:m]\n",
    "        plt.ioff()\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        plt.title(\"gt-label = {}\\n predicted = {}\\n instance {}\".format(\n",
    "                                [\"dog\", \"wolf\"][y],\n",
    "                                [\"dog\", \"wolf\"][np_argmax_list_predyn],\n",
    "                                n\n",
    "                    ), fontsize=100\n",
    "                 )\n",
    "        \n",
    "        list_relevantinstances =[]\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(\n",
    "              tfm_denormalize(ds_recurring[idx_similars[count_similars]][0]).cpu().numpy().transpose(1,2,0),\n",
    "            )\n",
    "            plt.title(\"instnace {}\".format(idx_similars[count_similars]), fontsize=100)\n",
    "            list_relevantinstances.append(idx_similars[count_similars])\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        #plot rows 2 (explanations for x2)\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            x2 = ds_recurring[idx_similars[count_similars]][0].unsqueeze(0).to(device)\n",
    "            x1 = x.unsqueeze(0).to(device)\n",
    "            explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "              gpmodel = gpmodel,\n",
    "              func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "              x1 = x.to(device),\n",
    "              x2 = ds_recurring[idx_similars[count_similars]][0].to(device),\n",
    "              idx_ann_outputhead = y,\n",
    "              du_per_gp = du_per_class,\n",
    "              scale_resizemaps = 2.0\n",
    "            )\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            toret1 = np.sum(np.sum(explanationx1x2, axis=-1), axis=-1)\n",
    "            toret2 = np.sum(np.sum(explanationx1x2, axis=0), axis=0)\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(toret2, cmap=\"seismic\", vmin=np.min(toret2), vmax=np.max(toret2))\n",
    "        #plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        #plt.hist(kn.flatten(), bins=200)\n",
    "        \n",
    "        #plot rows 3 (explanations for the instance itself)\n",
    "        plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "        plt.imshow(tfm_denormalize(x).cpu().numpy().transpose(1,2,0))\n",
    "        for count_similars in range(len(idx_similars)):\n",
    "            x2 = ds_recurring[idx_similars[count_similars]][0].unsqueeze(0).to(device)\n",
    "            x1 = x.unsqueeze(0).to(device)\n",
    "            explanationx1x2 = explainkern_imgimg_CAMlike(\n",
    "              gpmodel = gpmodel,\n",
    "              func_forward_beforeavgpool = gpmodel.module_f1.module.forward_untilbeforeavgpooling,\n",
    "              x1 = x.to(device),\n",
    "              x2 = ds_recurring[idx_similars[count_similars]][0].to(device),\n",
    "              idx_ann_outputhead = y,\n",
    "              du_per_gp = du_per_class,\n",
    "              scale_resizemaps = 2.0\n",
    "            )\n",
    "            \n",
    "            toret1 = np.sum(np.sum(explanationx1x2, axis=-1), axis=-1)\n",
    "            toret2 = np.sum(np.sum(explanationx1x2, axis=0), axis=0)\n",
    "            plt.subplot(3, 1*(m+1), count_subplot); count_subplot+=1;\n",
    "            plt.imshow(toret1, cmap=\"seismic\", vmin=np.min(toret1), vmax=np.max(toret1))\n",
    "                \n",
    "        \n",
    "    if(os.path.isfile(\"InterpGP/RegionBased/{}/{}.png\".format(str_subfolder, n)) == False):\n",
    "        plt.savefig(\n",
    "                \"InterpGP/RegionBased/{}/{}.png\".format(str_subfolder, n),\n",
    "                dpi=80, bbox_inches='tight', pad_inches=0, Q=100\n",
    "            )\n",
    "    plt.close()\n",
    "    assert False\n",
    "#enable_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depict the Prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_label_recurring = [ds_recurring.list_labelnames.index(ds_recurring._ntoimage(n)[1]+\"/\") for n in range(len(ds_recurring))]\n",
    "def inspect_kernel(module_input, dl_input, list_label_recurring):\n",
    "    np_lable_recurring = np.array(list_label_recurring)\n",
    "    dict_class_to_sumranks = {u:0.0 for u in range(module_input.Dv)}\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dl_input):\n",
    "            if((idx%10) == 0):\n",
    "                print(\"Finished {} out of {}\".format(\n",
    "                    idx, len(dl_input.dataset)/dl_input.batch_size\n",
    "                ), end='\\r')\n",
    "            x, y, n = data\n",
    "            output, uncertainty, output_similarities = \\\n",
    "                module_input.testingtime_forward(x.to(device), y, n)\n",
    "            output_similarities = output_similarities.detach().cpu().numpy() #[9xbatchsizex M]\n",
    "            for idx_inminibatch, idx_inds in enumerate(n):\n",
    "                xn_similarities = output_similarities[int(y[idx_inminibatch]),idx_inminibatch, :] #[M]\n",
    "                xn_similarities = xn_similarities.flatten()\n",
    "                xn_similarities = xn_similarities[np_lable_recurring == int(y[idx_inminibatch])]\n",
    "                dict_class_to_sumranks[int(y[idx_inminibatch])] += np.argsort(-xn_similarities)\n",
    "                \n",
    "    \n",
    "    return dict_class_to_sumranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_class_to_sumranks = inspect_kernel(gpmodel, dl_test, list_label_recurring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump( dict_class_to_sumranks, open(\"TrainingHistory/dict_class_to_sumranks.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_class_to_sumranks = pickle.load( open( \"TrainingHistory/dict_class_to_sumranks.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in dict_class_to_sumranks.keys():\n",
    "    plt.figure()\n",
    "    plt.hist(dict_class_to_sumranks[k]/len(dict_class_to_sumranks[k].tolist()), bins=200)\n",
    "    plt.title(\"histogram of avg. ranks among the training instances \\n for class {}.\".format(k))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import relatedwork\n",
    "import relatedwork.utils.transforms\n",
    "tfm_denormalize = relatedwork.utils.transforms.ImgnetDenormalize()\n",
    "#open the app to check whether the sample-compression scheme matches human's ====\n",
    "num_tocompare = 10\n",
    "input_class = int(input(\"Please input the class (between 0 and 9).\"))\n",
    "plt.figure()\n",
    "plt.hist(dict_class_to_sumranks[input_class]/\\\n",
    "         len(dict_class_to_sumranks[input_class].tolist()), bins=200)\n",
    "plt.show()\n",
    "input_threshold = float(input(\"Please enter a threshold based on the histogram.\"))\n",
    "#divide instances based on their ranks\n",
    "np_label_recurring = np.array(list_label_recurring)\n",
    "idx_k_inds = np.array(range(len(ds_recurring)))[np_label_recurring == input_class]\n",
    "\n",
    "idx_highrank_inlocal =  np.where(dict_class_to_sumranks[input_class] <\\\n",
    "                          (input_threshold*len(dict_class_to_sumranks[input_class].tolist())))[0]\n",
    "idx_highrank_inlocal = idx_highrank_inlocal.tolist()\n",
    "idx_highrank_inds = idx_k_inds[dict_class_to_sumranks[input_class] <\\\n",
    "                               input_threshold*len(dict_class_to_sumranks[input_class].tolist())]\n",
    "idx_lowrank_inds = np.array(list(set(range(len(ds_recurring))).difference(idx_highrank_inds)))\n",
    "assert(len(idx_highrank_inds.tolist())+len(idx_lowrank_inds.tolist()) == len(ds_recurring))\n",
    "print(\"{} percent of instances are below the threshold. exact num = {}\"\\\n",
    "      .format(100*len(idx_highrank_inds)/(np.sum(np_label_recurring==input_class)+0.0),\\\n",
    "              len(idx_highrank_inds.tolist())))\n",
    "score = 0\n",
    "for n in range(num_tocompare):\n",
    "    n_high = random.choice(idx_highrank_inds)\n",
    "    n_low = random.choice(idx_lowrank_inds)\n",
    "    img_high = ds_recurring[n_high][0]\n",
    "    img_low  = ds_recurring[n_low][0]\n",
    "    \n",
    "    list_toshow = [img_high, img_low]\n",
    "    flag_fillped = False\n",
    "    if(np.random.rand()<0.5):\n",
    "        list_toshow = [img_low, img_high]\n",
    "        flag_fillped = True\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(tfm_denormalize(list_toshow[0]).cpu().numpy().transpose(1,2,0))\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(tfm_denormalize(list_toshow[1]).cpu().numpy().transpose(1,2,0))\n",
    "    plt.show()\n",
    "    input_selection = int(input(\"Which image (left or right) seems a prototype?\"))\n",
    "    assert(input_selection in [0,1])\n",
    "    if(input_selection == 0):\n",
    "        if(flag_fillped == True):\n",
    "            score += 1\n",
    "    if(input_selection == 1):\n",
    "        if(flag_fillped == False):\n",
    "            score += 1\n",
    "print(\"score = {}\".format(score/(num_tocompare+0.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# np.max(dict_class_to_sumranks[input_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(dict_class_to_sumranks[input_class]/\\\n",
    "         len(dict_class_to_sumranks[input_class].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_onkernelspace(gpmodel, dl_input):\n",
    "    gpmodel.eval()\n",
    "    dict_n_to_kernelspace = {}\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(dl_input):\n",
    "            if((idx%10) == 0):\n",
    "                print(\"Finished {} out of {}\".format(\n",
    "                    idx, len(dl_input.dataset)/dl_input.batch_size\n",
    "                ), end='\\r')\n",
    "            x, _, n = data\n",
    "            output_f1 = gpmodel.module_f1(x.to(device))\n",
    "            output_f1 = output_f1[:,:,0,0].detach().cpu().numpy()\n",
    "            \n",
    "            for idx_inminibatch, idx_inds in enumerate(n):\n",
    "                dict_n_to_kernelspace[idx_inds] = output_f1[idx_inminibatch,:].flatten().tolist()\n",
    "    \n",
    "    X_on_kernelspace = []\n",
    "    gpmodel.train()\n",
    "    return X_on_kernelspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_kernel(gpmodel, dl_recurring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.zeros((40000, 40000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = [\n",
    "1,\n",
    "2,\n",
    "5,\n",
    "6,\n",
    "7,\n",
    "8,\n",
    "9,\n",
    "11,\n",
    "14\n",
    "]\n",
    "a = np.array(a)\n",
    "b = [u+1 for u in range(len(a.tolist()))]\n",
    "b.reverse()\n",
    "print(np.sum(a*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1,2,3]\n",
    "l.reverse()\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
