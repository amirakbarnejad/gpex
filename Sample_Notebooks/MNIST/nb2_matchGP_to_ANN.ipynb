{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd292d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49597476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import types\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "list_pathstoadd = [\n",
    "       \"../../\"\n",
    "]\n",
    "for path in list_pathstoadd:\n",
    "    if(path not in sys.path):\n",
    "        sys.path.append(path)\n",
    "#import generalGPmodule\n",
    "import gpex\n",
    "import gpex.kernelmappings.image\n",
    "import skimage\n",
    "from skimage import io\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#import torchofgp.tgp as tgp\n",
    "from gpex.kernelmappings.image import Resnet50BackboneKernelDivideAvgPool\n",
    "#from hpresnet18 import PrimaryNetwork\n",
    "#import relatedwork\n",
    "#from relatedwork.utils.generativemodels import ResidualEncoder\n",
    "import akresnetformnist\n",
    "from akresnetformnist import *\n",
    "import loadmnist_fromsplit\n",
    "from loadmnist_fromsplit import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f62ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings ================\n",
    "#fname_split = args.fname_split #\"split_0.pkl\"\n",
    "#tune for each run ==\n",
    "#numepochs_1, numepochs_2 = args.numepochs_1, args.numepochs_2 #10000, 10 #---overrideint(sys.argv[1]), int(sys.argv[2])\n",
    "#countupdateU_afterupdateGP = 10 #---override int(sys.argv[3]) #should be 100\n",
    "#idx_trainingbatch = 1\n",
    "#flag_enabledataaugmentation = True\n",
    "\n",
    "#str_versionname = \"nbrun\" #---overrid\"explainann_\"+str(sys.argv[-1])\n",
    "#int_mode_modulekernel, num_backbones = 16, None\n",
    "du_per_class = 20\n",
    "rng_randw = [-1.0, 2.0]\n",
    "#flag_train_memefficient, memefficeint_heads_in_compgraph = False, None\n",
    "dim_wideoutput = 1024\n",
    "#stepsize_gradupdate = 1\n",
    "#tune for each dataset\n",
    "batchsize = 8\n",
    "num_classes = 10\n",
    "#general settings ===\n",
    "#idx_split = 0\n",
    "#int_exposedclass = None\n",
    "\n",
    "#flag_efficient = True\n",
    "#flag_detachcovpvn = True\n",
    "#flag_controlvariate = True\n",
    "#flag_setcovtoOne = False\n",
    "#int_mode_controlvariate = 2\n",
    "#val_clipgradient = None #set to None to avoid any gradient clipping.\n",
    "#fname_phase1output = None#\"W0-1_relatively_fit/4872\" #\"model_0.96_outputwidth_1024\"\n",
    "#mode_ann = 1\n",
    "rootpath_ds = \"./NonGit/MNIST/\"\n",
    "#\"./cifar-10-batches-py\"\n",
    "#if(mode_ann==1):\n",
    "#fname_phase0output = args.fname_phase0output\n",
    "#\"Phase0_DifferentSplits/checkpoint/split_0/ckpt_200_88.22.pth\"\n",
    "#\"model_0.9538_outputwidth_1024\"\n",
    "#elif(mode_ann==2):\n",
    "#    assert False\n",
    "#    fname_phase0output = \"model_0.9538_outputwidth_1024\"\n",
    "#\"model_0.9814024796693774_outputwidth_1024_gpmodelinitialized\"\n",
    "#assert((fname_phase1output is None) or (fname_phase0output is None))\n",
    "#assert(int_mode_modulekernel in [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17])\n",
    "# if(num_backbones is not None):\n",
    "#     assert(int_mode_modulekernel in [6,7])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbda5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e209a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Training set:\")\n",
    "ds_train = loadmnist_fromsplit.MNISTDatasetFromSplit(\n",
    "    rootdir = rootpath_ds,\n",
    "    fname_split = \"split_0.pkl\",\n",
    "    str_trainortest = \"train\",\n",
    "    flag_enabledataaugmentation = False,\n",
    "    rootpath_splits = \"./Splits/\"\n",
    ")\n",
    "print(\"Recurring:\")\n",
    "ds_recurring = loadmnist_fromsplit.MNISTDatasetFromSplit(\n",
    "    rootdir = rootpath_ds,\n",
    "    fname_split = 'split_0.pkl',\n",
    "    str_trainortest = \"train\",\n",
    "    flag_enabledataaugmentation = False,\n",
    "    rootpath_splits = \"./Splits/\"\n",
    ")\n",
    "print(\"Test:\")\n",
    "ds_test = loadmnist_fromsplit.MNISTDatasetFromSplit(\n",
    "    rootdir = rootpath_ds,\n",
    "    fname_split = 'split_0.pkl',\n",
    "    str_trainortest = \"test\",\n",
    "    flag_enabledataaugmentation = False,\n",
    "    rootpath_splits = \"./Splits/\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc22bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(ds_train, batch_size=batchsize,\n",
    "                      shuffle=True, num_workers=2)\n",
    "dl_recurring = DataLoader(ds_recurring, batch_size=batchsize,\n",
    "                          shuffle=False, num_workers=2)\n",
    "dl_test = DataLoader(ds_test, batch_size=batchsize,\n",
    "                     shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85afc1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClampAndTanh(torch.nn.Module):\n",
    "    def __init_(self, minval=-1.0, maxval=1.0):\n",
    "        self.minval, self.maxval = minval, maxval\n",
    "        super(ClampAndTanh, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = torch.nn.functional.tanh(\n",
    "                        torch.clamp(\n",
    "                            x, -1.0, 1.0\n",
    "                        )\n",
    "                    )\n",
    "        return output\n",
    "    \n",
    "def initweights_to_zero(m):\n",
    "    if type(m) in {nn.Linear, nn.Conv2d, nn.Linear}:\n",
    "        torch.nn.init.zeros_(m.weight)\n",
    "        m.bias.data.fill_(np.random.randn()*0.1) #TODO:check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleF1(torch.nn.Module):\n",
    "    def __init__(self, module_caller):\n",
    "        super(ModuleF1, self).__init__()\n",
    "        #make internals ===\n",
    "        self.module = Resnet50BackboneKernelDivideAvgPool(\n",
    "            num_classes = num_classes,\n",
    "            du_per_class = du_per_class\n",
    "        )\n",
    "    \n",
    "    def set_rng_outputheads(self, rng_outputhead):\n",
    "        self.module.set_rng_outputheads(rng_outputhead)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        toret = self.module(x)\n",
    "        toret = toret.unsqueeze(-1).unsqueeze(-1)\n",
    "        return toret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfceda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainModule(nn.Module):\n",
    "    def __init__(self, num_classes, device, dl_recurring, dl_nonrecurring, dl_test, batchsize, dim_wideoutput):\n",
    "        '''\n",
    "        Inputs:\n",
    "            - size_input: size of the input, e.g., [32 x 2000 x 7 x 7].\n",
    "            - device: the device on which the GP fields are going to be created.\n",
    "            - num_outputheads: an integer, number of output heads.\n",
    "        '''\n",
    "        super(MainModule, self).__init__()\n",
    "        #grab args ===\n",
    "        self.num_classes = num_classes\n",
    "        self.device = device\n",
    "        self.dl_recurring = dl_recurring\n",
    "        self.dl_nonrecurring = dl_nonrecurring\n",
    "        self.dl_test = dl_test\n",
    "        self.batchsize = batchsize\n",
    "        #self.iter_dl_recurring = iter(self.dl_recurring)\n",
    "        #make internal module_tobecomeGP ===\n",
    "        self.module_tobecomeGP = AkResNet18(\n",
    "            num_classes=num_classes,\n",
    "            dim_wideoutput=dim_wideoutput\n",
    "        )\n",
    "        #make internals ===\n",
    "        self.dic_dlname_to_iter = {\n",
    "            \"dl_recurring\":iter(self.dl_recurring),\n",
    "            \"dl_nonrecurring\":iter(self.dl_nonrecurring),\n",
    "            \"dl_test\":iter(self.dl_test)\n",
    "        }\n",
    "        #make module f1 ===\n",
    "        self.module_f1 = ModuleF1(self) #nn.Sequential(\n",
    "        self._lastidx_recurring = []\n",
    "        #internal field to subsample when feeding minbatch ====\n",
    "        self.n_subsampleminibatch = None\n",
    "            \n",
    "    def forward(self, x, y, n):\n",
    "         return self.module_tobecomeGP(x), y, n\n",
    "        \n",
    "    \n",
    "    def func_get_modulef1(self):\n",
    "        return self.module_f1\n",
    "    \n",
    "    def func_mainmodule_to_moduletobecomeGP(self, module_input):\n",
    "        return module_input.module_tobecomeGP\n",
    "    \n",
    "    \n",
    "    def _func_feed_minibatch(self, dl_input, str_dlname, flag_addnoisetoX = False):\n",
    "        #print(\"reached here 1\")\n",
    "        if(False):#iter_dl is None):\n",
    "            pass #x, y, n = next(iter(dl_input))\n",
    "        else:\n",
    "            try:\n",
    "                x, y, n = next(self.dic_dlname_to_iter[str_dlname])\n",
    "            except (StopIteration):\n",
    "                self.dic_dlname_to_iter[str_dlname] = iter(dl_input)\n",
    "                x, y, n = next(self.dic_dlname_to_iter[str_dlname])\n",
    "                \n",
    "        \n",
    "        if(flag_addnoisetoX == True):\n",
    "            idx_permutex = np.random.permutation([u for u in range(list(x.size())[0])]).tolist()\n",
    "            idx_permutex = torch.LongTensor(idx_permutex)\n",
    "            x_perumted = x[idx_permutex, :, :, :]\n",
    "            \n",
    "            rand_w = torch.rand((list(x.size())[0]))                        .float().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1) #[Nx1x1x1]\n",
    "            rand_w = rand_w*(rng_randw[1]-rng_randw[0]) + rng_randw[0] #in [rng[0] , rng[1]]\n",
    "            rand_w = rand_w.detach()\n",
    "            #print(\"rand_w.shape = {}\".format(rand_w.shape))\n",
    "            x = rand_w*x + (1.0-rand_w)*x_perumted  #[N x 3 x 224 x 224]\n",
    "            x = x + 0.1*torch.randn_like(x).float()\n",
    "        \n",
    "        if(self.n_subsampleminibatch is None):\n",
    "            pass\n",
    "        else:\n",
    "            x = x[0:self.n_subsampleminibatch, :,:,:]\n",
    "            y = y[0:self.n_subsampleminibatch]\n",
    "            n = n[0:self.n_subsampleminibatch]\n",
    "        \n",
    "        if(dl_input.dataset == ds_recurring):\n",
    "            self._lastidx_recurring = n\n",
    "            if(len(n) != list(x.size())[0]):\n",
    "                assert False\n",
    "        \n",
    "        output, _, _ = self.forward(x.to(self.device), y, n)\n",
    "        \n",
    "        return output, y, n\n",
    "    \n",
    "    def func_feed_recurring_minibatch(self):\n",
    "        output, y, n = self._func_feed_minibatch(\n",
    "                            self.dl_recurring,\n",
    "                            str_dlname = \"dl_recurring\"\n",
    "                         )\n",
    "        return output, y, n\n",
    "    \n",
    "    def func_feed_noise_minibatch(self):\n",
    "        output, y, n = self._func_feed_minibatch(\n",
    "                        self.dl_nonrecurring,\n",
    "                        str_dlname = \"dl_nonrecurring\",\n",
    "                        flag_addnoisetoX=True\n",
    "                      )\n",
    "        return output, y, n\n",
    "    \n",
    "    def func_feed_nonrecurring_minibatch(self):\n",
    "        output, y, n = self._func_feed_minibatch(\n",
    "                                self.dl_nonrecurring,\n",
    "                                str_dlname = \"dl_nonrecurring\",\n",
    "                                flag_addnoisetoX=False\n",
    "                              )\n",
    "        return output, y, n\n",
    "    \n",
    "    def func_feed_test_minibatch(self):\n",
    "        output, y, n = self._func_feed_minibatch(\n",
    "                                    self.dl_test,\n",
    "                                    str_dlname = \"dl_test\"\n",
    "                                )\n",
    "        return output, y, n\n",
    "    \n",
    "    def func_get_indices_lastrecurringinstances(self):\n",
    "        return self._lastidx_recurring.cpu().numpy().tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c89054",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MainModule(\n",
    "    dim_wideoutput=dim_wideoutput,\n",
    "    num_classes = num_classes,\n",
    "    device = device,\n",
    "    dl_recurring = dl_recurring,\n",
    "    dl_nonrecurring = dl_train,\n",
    "    dl_test = dl_test,\n",
    "    batchsize = batchsize\n",
    ")\n",
    "state_dict = torch.load(\n",
    "        \"NonGit/checkpoint_train_split0.pth\"\n",
    ")\n",
    "model.module_tobecomeGP.load_state_dict(\n",
    "       state_dict,\n",
    "       strict = True\n",
    "    )\n",
    "print(\"classifier was loaded from checkpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ad8d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's compute the accuracy of the classifier on the test set.\n",
    "def evaluate_classifier(model_input, ds_input, input_device):\n",
    "    model_input.eval()\n",
    "    with torch.no_grad():\n",
    "        toret = []\n",
    "        list_gty = []\n",
    "        for n in range(len(ds_input)):\n",
    "            print(\" instance {} from {}\".format(n, len(ds_input)), end='\\r')\n",
    "            x, y, _ = ds_input[n]\n",
    "            output = model_input(x.unsqueeze(0).to(input_device))\n",
    "            toret.append(output[0,:].detach().cpu().numpy())\n",
    "            list_gty.append(y)\n",
    "        print(\"\\n\")\n",
    "        toret = np.array(toret) #[N x 10]\n",
    "        #toret = toret[:,0,:]\n",
    "    model_input.train()\n",
    "    return toret, list_gty\n",
    "predy, list_gty = evaluate_classifier(model.module_tobecomeGP, ds_test, device)\n",
    "np_confmatrix = confusion_matrix(np.argmax(predy,1), list_gty)\n",
    "acc = np.sum(np_confmatrix*np.eye(10))/np.sum(np_confmatrix)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ea1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "#model.n_subsampleminibatch = 50\n",
    "gpmodel = gpex.GPEXModule(\n",
    "    module_rawmodule = model,\n",
    "    size_recurringdataset = len(ds_recurring),\n",
    "    device = device,\n",
    "    func_mainmodule_to_moduletobecomeGP = model.func_mainmodule_to_moduletobecomeGP, \n",
    "    func_feed_noise_minibatch = model.func_feed_noise_minibatch,\n",
    "    func_feed_inducing_minibatch  = model.func_feed_recurring_minibatch,\n",
    "    func_feed_nonrecurring_minibatch = model.func_feed_nonrecurring_minibatch,\n",
    "    func_feed_test_minibatch = model.func_feed_test_minibatch,\n",
    "    func_get_indices_lastrecurringinstances = model.func_get_indices_lastrecurringinstances,\n",
    "    func_get_modulef1 = model.func_get_modulef1,\n",
    "    flag_efficient = True,\n",
    "    flag_detachcovpvn = True,\n",
    "    flag_controlvariate = True,\n",
    "    flag_setcovtoOne = False,\n",
    "    int_mode_controlvariate = 2,\n",
    "    flag_train_memefficient = False,\n",
    "    memefficeint_heads_in_compgraph = None\n",
    "  )\n",
    "#model.n_subsampleminibatch = 50\n",
    "gpmodel.sigma2_GP = 1.0 #TODO:check\n",
    "gpmodel.train()\n",
    "gpmodel.to(device)\n",
    "print(\"gpmodel was created on {}\".format(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c7573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the gpmodel\n",
    "gpmodel.init_UV()\n",
    "print(\"Initialized U&V from module f1(.).\")\n",
    "gpmodel.train()\n",
    "initweights_to_zero(gpmodel.module_f1)\n",
    "print(\"Weights were initialized to zero.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e690521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_params_gpwithoutuncertainty = set(list(gpmodel.parameters())) - set(gpmodel.func_mainmodule_to_moduletobecomeGP(model).parameters()) \n",
    "# optimizer_straight = torch.optim.Adam(\n",
    "#         list(set_params_gpwithoutuncertainty), lr=0.0001, amsgrad=True\n",
    "#     )\n",
    "# optimizer_g = torch.optim.Adam(\n",
    "#         list(gpmodel.module_tobecomeGP.parameters())+\\\n",
    "#         [], lr=0.00001, amsgrad=True\n",
    "#     )#TODO:check\n",
    "# optimizer_GPY = torch.optim.Adam([gpmodel.GP_Y], lr=0.00001) #TODO:check\n",
    "#set_modelparams = gpmodel.module_f1.parameters()\n",
    "optimizer_modelparams = torch.optim.Adam(\n",
    "                 gpmodel.module_f1.parameters(),\n",
    "                lr=0.0001, amsgrad=False\n",
    "            )\n",
    "# scheduler_modelparams = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "#     optimizer_modelparams, T_max=200\n",
    "# )\n",
    "# list(model.module_gp.module_fullyconvpart.parameters())+\\\n",
    "#                 [model.module_gp.GP_Y]+\\\n",
    "#                 list(model.module_fullyconnected.parameters())\n",
    "# criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b0621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #set path_output_dump ===\n",
    "# if(flag_isrunning_slurm == False):\n",
    "#     path_output_dump = \"NonGit/{}\".format(args.fname_split[0:-4])\n",
    "# else:\n",
    "#      path_output_dump = \"/home/ah8uofa/scratch/Rebuttal_Neurips2023/{}/{}\".format(\n",
    "#            os.getcwd().split('/')[-1],\n",
    "#          args.fname_split[0:-4]\n",
    "#      )\n",
    "# print(\"path_output_dump is set to {}.\".format(path_output_dump))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61710152",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%capture\n",
    "if('history_Qvn_loss' not in globals()):\n",
    "    history_modelparamsloss = []\n",
    "    #history_Qvn_loss, history_Qvhatm_loss, history_modelparamsloss = [], [], []\n",
    "    #history_Qvn_loss_celoss, history_Qvn_loss_klterm = [], []\n",
    "    #history_Qvhatm_loss_dot_term1, history_Qvhatm_loss_dot_kl = [], []\n",
    "    #history_celoss = []\n",
    "    #history_straightceloss = []\n",
    "    batchcount = 0\n",
    "    #count_updatevarpars = 0\n",
    "    #flag_GPXinitialized = False\n",
    "    \n",
    "    #torch.save(\n",
    "    #    gpmodel.state_dict(),\n",
    "    #    \"{}/CheckpointAvoidNan/gpmodel_phase2_w{}.pt\".format(path_output_dump, dim_wideoutput)\n",
    "    #)\n",
    "    #backup_optimizer_straight = copy.deepcopy(optimizer_straight)\n",
    "    #backup_optimizer_g = copy.deepcopy(optimizer_g)\n",
    "    #backup_optimizer_GPY = copy.deepcopy(optimizer_GPY)\n",
    "    #backup_optimizer_modelparams = copy.deepcopy(optimizer_modelparams)\n",
    "    #count_updatestomodelparams = 0\n",
    "    \n",
    "#count_dlrecurringvisited = 0\n",
    "#count_backward_qvn = 1\n",
    "#count_backward_qvhatm = 1\n",
    "#count_backward_modelparams = 1\n",
    "\n",
    "#zero-fill all gradients ===\n",
    "#optimizer_straight.zero_grad()\n",
    "#optimizer_g.zero_grad()\n",
    "#optimizer_GPY.zero_grad()\n",
    "#optimizer_modelparams.zero_grad()\n",
    "\n",
    "\n",
    "t_beforeepoch_1 = time.time()\n",
    "for epoch in range(2):\n",
    "    \n",
    "    for idx, data in enumerate(dl_train):\n",
    "        batchcount += 1\n",
    "        if((batchcount%100) == 0):\n",
    "            print(\"batchcount = {}\".format(batchcount))\n",
    "        \n",
    "        if(((batchcount%1000) == 0) or (batchcount==0)):    \n",
    "            gpmodel.renew_precomputed_XTX()\n",
    "            print(\"XTX renewed (to avoid propagating numerical error.)\")\n",
    "        \n",
    "        \n",
    "        #if(True):\n",
    "        #    if((count_updatevarpars%1) == 0):#TODO:change\n",
    "        #count_backward_modelparams += 1\n",
    "        #model.n_subsampleminibatch = 15\n",
    "        optimizer_modelparams.zero_grad()\n",
    "\n",
    "        cost_modelparams = gpmodel.getcost_GPmatchNN()\n",
    "        if(gpmodel.flag_svdfailed == True):\n",
    "            pass #in this case the svd has failed.\n",
    "            assert False #TODO:delete\n",
    "        else:\n",
    "\n",
    "\n",
    "            #netout_modelparams, y_modelparams, _ = output_modelparams\n",
    "\n",
    "            #cost_modelparams = costmodelparams_term1 #/(costmodelparams_term1.detach())\n",
    "            #if(flag_train_memefficient == False):\n",
    "            #cost_modelparams = cost_modelparams/(stepsize_gradupdate+0.0)\n",
    "            cost_modelparams.backward()\n",
    "            optimizer_modelparams.step()\n",
    "\n",
    "            #print(\"    performed the backprop 3.\")\n",
    "            #if((count_backward_modelparams%stepsize_gradupdate) == 0):\n",
    "                #pass\n",
    "                #optimizer_modelparams.step()\n",
    "                #optimizer_modelparams.zero_grad()\n",
    "                #print(\"   >>> backprop3.step.\")\n",
    "\n",
    "            #count_updatestomodelparams += 1\n",
    "            history_modelparamsloss.append(cost_modelparams.detach().cpu().numpy().tolist())\n",
    "            #history_celoss.append(celoss_modelprams.detach().cpu().numpy().tolist())\n",
    "\n",
    "\n",
    "\n",
    "            #update GPX ====\n",
    "            gpmodel.update_U()\n",
    "            #print(\"         performed the update to GPX.\")\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7ff070",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(history_modelparamsloss)), history_modelparamsloss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cd1100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if g(.) and GP path match on test instances ====\n",
    "gpmodel.renew_precomputed_XTX()\n",
    "list_outputgpoutputg = gpmodel.checkequal_GPout_ANNout_ontest(10)\n",
    "for n in range(len(list_outputgpoutputg)):\n",
    "    a = list_outputgpoutputg[n][0]\n",
    "    b = list_outputgpoutputg[n][1]\n",
    "\n",
    "\n",
    "    min_ab = min(np.min(a), np.min(b))\n",
    "    max_ab = max(np.max(a), np.max(b))\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(np.round(a, 2), vmin=min_ab, vmax=max_ab, aspect=\"auto\"); plt.colorbar()\n",
    "    plt.title(\"GPs output\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(np.round(b, 2), vmin=min_ab, vmax=max_ab, aspect=\"auto\"); plt.colorbar()\n",
    "    plt.title(\"ANN output\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676ac233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model_input, ds_input, input_device):\n",
    "#     model_input.eval()\n",
    "#     with torch.no_grad():\n",
    "#         toret = []\n",
    "#         list_gty = []\n",
    "#         for n in range(len(ds_input)):\n",
    "#             print(\" instance {} from {}\".format(n, len(ds_input)), end='\\r')\n",
    "#             x, y, _ = ds_input[n]\n",
    "#             output = model_input(x.unsqueeze(0).to(input_device))\n",
    "#             toret.append(output[0,:].detach().cpu().numpy())\n",
    "#             list_gty.append(y)\n",
    "#         print(\"\\n\")\n",
    "#         toret = np.array(toret) #[N x 10]\n",
    "#         #toret = toret[:,0,:]\n",
    "#     model_input.train()\n",
    "#     return toret, list_gty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df577566",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(flag_isrunning_jupyterNB):\n",
    "    predy, list_gty = evaluate_model(model.module_tobecomeGP, ds_test, device)\n",
    "    np_confmatrix = confusion_matrix(np.argmax(predy,1), list_gty)\n",
    "    #print(np_confmatrix)\n",
    "    #print(\"\\n\\n\\n\")\n",
    "    acc = np.sum(np_confmatrix*np.eye(10))/np.sum(np_confmatrix)\n",
    "    print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ed2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dlrecurringvisited = 0\n",
    "count_backward_qvn = 1\n",
    "count_backward_qvhatm = 1\n",
    "count_backward_modelparams = 1\n",
    "\n",
    "#zero-fill all gradients ===\n",
    "optimizer_straight.zero_grad()\n",
    "optimizer_g.zero_grad()\n",
    "optimizer_GPY.zero_grad()\n",
    "optimizer_modelparams.zero_grad()\n",
    "\n",
    "\n",
    "t_beforeepoch_2 = time.time()\n",
    "for epoch in range(2):\n",
    "    \n",
    "    if(epoch>0):\n",
    "        torch.save(\n",
    "            gpmodel.state_dict(),\n",
    "            \"{}/round2_epoch_{}.pt\".format(path_output_dump, epoch)\n",
    "        )\n",
    "    \n",
    "    for idx, data in enumerate(dl_train):\n",
    "        batchcount += 1\n",
    "        if((batchcount%100) == 0):\n",
    "            print(\"batchcount = {}\".format(batchcount))\n",
    "        if(((batchcount%1000) == 0) or (batchcount==0)):    \n",
    "            gpmodel.renew_precomputed_XTX()\n",
    "            print(\"XTX renewed (to avoid propagating numerical error.)\")\n",
    "        \n",
    "        \n",
    "        if(True):\n",
    "            if((count_updatevarpars%1) == 0):#TODO:change\n",
    "                count_backward_modelparams += 1\n",
    "                #model.n_subsampleminibatch = 15\n",
    "                \n",
    "                costmodelparams_term1 = gpmodel.getcost_GPmatchNN()\n",
    "                if(gpmodel.flag_svdfailed == True):\n",
    "                    pass #in this case the svd has failed.\n",
    "                else:\n",
    "                    \n",
    "                    \n",
    "                    #netout_modelparams, y_modelparams, _ = output_modelparams\n",
    "                    if(int_exposedclass is None):\n",
    "                        pass#celoss_modelprams = criterion(netout_modelparams, y_modelparams.to(device))\n",
    "                    else:\n",
    "                        pass#celoss_modelprams = torch.Tensor([0.0])\n",
    "                    cost_modelparams = costmodelparams_term1 #/(costmodelparams_term1.detach())\n",
    "                    if(flag_train_memefficient == False):\n",
    "                        cost_modelparams = cost_modelparams/(stepsize_gradupdate+0.0)\n",
    "                    else:\n",
    "                        cost_modelparams = (memefficeint_heads_in_compgraph+0.0)*cost_modelparams/(stepsize_gradupdate+0.0)\n",
    "                    cost_modelparams.backward()\n",
    "                    #print(\"    performed the backprop 3.\")\n",
    "                    if((count_backward_modelparams%stepsize_gradupdate) == 0):\n",
    "                        optimizer_modelparams.step()\n",
    "                        optimizer_modelparams.zero_grad()\n",
    "                        #print(\"   >>> backprop3.step.\")\n",
    "                    \n",
    "                    count_updatestomodelparams += 1\n",
    "                    history_modelparamsloss.append(costmodelparams_term1.detach().cpu().numpy().tolist())\n",
    "                    #history_celoss.append(celoss_modelprams.detach().cpu().numpy().tolist())\n",
    "                    \n",
    "                        \n",
    "                    \n",
    "                    #update GPX ====\n",
    "                    for _ in range(10):\n",
    "                        gpmodel.update_U()\n",
    "                    #print(\"         performed the update to GPX.\")\n",
    "                    \n",
    "                    #update scheduler of model_params ====\n",
    "                    if((count_updatestomodelparams % int(len(ds_train)/(batchsize*1.0))) == 0):\n",
    "                        if(count_updatestomodelparams > 0):\n",
    "                            #check if the model has to be reverted (the nan issue) ===\n",
    "                            flag_nanoccured = np.isnan(cost_modelparams.detach().cpu().numpy().tolist())\n",
    "                            if(flag_nanoccured == False):\n",
    "                                #update the backup of model and optim\n",
    "                                torch.save(\n",
    "                                    gpmodel.state_dict(),\n",
    "                                    \"{}/CheckpointAvoidNan/gpmodel_phase2_w{}.pt\"\\\n",
    "                                        .format(path_output_dump, dim_wideoutput)\n",
    "                                )\n",
    "                                backup_optimizer_straight = copy.deepcopy(optimizer_straight)\n",
    "                                backup_optimizer_g = copy.deepcopy(optimizer_g)\n",
    "                                backup_optimizer_GPY = copy.deepcopy(optimizer_GPY)\n",
    "                                backup_optimizer_modelparams = copy.deepcopy(optimizer_modelparams)\n",
    "                            else:\n",
    "                                #revert-back the model and the optimizer \n",
    "                                gpmodel.load_state_dict(\n",
    "                                        torch.load(\n",
    "                                           \"{}/CheckpointAvoidNan/gpmodel_phase2_w{}.pt\".format(\n",
    "                                               path_output_dump,\n",
    "                                               dim_wideoutput\n",
    "                                           )\n",
    "                                         ),\n",
    "                                        strict = True\n",
    "                                     )\n",
    "                                optimizer_straight = copy.deepcopy(backup_optimizer_straight)\n",
    "                                optimizer_g = copy.deepcopy(backup_optimizer_g)\n",
    "                                optimizer_GPY = copy.deepcopy(backup_optimizer_GPY)\n",
    "                                optimizer_modelparams = copy.deepcopy(backup_optimizer_modelparams)\n",
    "                                optimizer_straight.zero_grad()\n",
    "                                optimizer_g.zero_grad()\n",
    "                                optimizer_GPY.zero_grad()\n",
    "                                optimizer_modelparams.zero_grad()\n",
    "                                count_backward_qvn = 1\n",
    "                                count_backward_qvhatm = 1\n",
    "                                count_backward_modelparams = 1\n",
    "                                #optimizer_GPmatchNN.zero_grad()\n",
    "                                print(\">>>>>>>>>>>>>>>>>> NAN OCCURED <<<<<<<<<<<<<<<<<<<.\")\n",
    "                            \n",
    "                        '''\n",
    "                        torch.save(\n",
    "                            gpmodel.state_dict(),\n",
    "                            \"TrainingHistory/Phase2/{}.pt\".format(count_updatestomodelparams)\n",
    "                        )\n",
    "                        '''\n",
    "                               \n",
    "    \n",
    "    flag_GPXinitialized = True        \n",
    "            \n",
    "t_afterepoch_2 = time.time()\n",
    "print(\"Round 2 took {} seconds.\".format(t_afterepoch_2 - t_beforeepoch_2))\n",
    "\n",
    "if(epoch>0):\n",
    "    torch.save(\n",
    "        gpmodel.state_dict(),\n",
    "        \"{}/round2_epoch_{}.pt\".format(path_output_dump, epoch)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f6fce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5457ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c399822",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
